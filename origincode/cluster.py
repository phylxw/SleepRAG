import os
import json
import re
import time
import numpy as np
import torch
import google.generativeai as genai
import matplotlib.pyplot as plt 
from typing import List, Dict
from sentence_transformers import SentenceTransformer
from sklearn.cluster import AgglomerativeClustering, KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.manifold import TSNE 
from sklearn.decomposition import PCA # ğŸ”¥ é‡æ–°åŠ å› PCA
from transformers import AutoModelForCausalLM, AutoTokenizer
import umap


# ================= é…ç½®åŒºåŸŸ =================

# 1. æ ¸å¿ƒå¼€å…³: é€‰æ‹©èµ·åå­—çš„æ¨¡å‹æ¥æº
MODEL_SOURCE = "huggingface" 

# [HuggingFace é…ç½®]
HF_MODEL_NAME = "Qwen/Qwen3-4B-Instruct-2507" 

# [Gemini é…ç½®]
GEMINI_MODEL_NAME = "gemini-2.5-flash"
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")

# 2. æ–‡ä»¶é…ç½®
INPUT_FILE = "MATH-lighteval_memory_freq_20251218_150403.jsonl" 
OUTPUT_FILE = "AMATH-lighteval_auto_clustered_result.jsonl"
SUMMARY_OUTPUT_FILE = "AMATH-lighteval_cluster_summary.jsonl"
PLOT_FILE = "AMATH-lighteval_cluster_distribution.png"
# å¯è§†åŒ–å›¾ç‰‡è¾“å‡ºè·¯å¾„
VIS_PLOT_FILE = "AMATH-lighteval_visualization.png"

# 3. èšç±»ç®—æ³•è®¾ç½® (å†³å®šæ€ä¹ˆâ€œåˆ†â€ç±»)
# é€‰é¡¹: 'agglomerative' (è‡ªåŠ¨å‘ç°ç±»åˆ«æ•°) æˆ– 'kmeans' (æŒ‡å®šç±»åˆ«æ•°)
CLUSTERING_METHOD = "agglomerative" 

# [Agglomerative å‚æ•°]
DISTANCE_THRESHOLD = 1.0  

# [K-Means å‚æ•°]
KMEANS_N_CLUSTERS = 10    

# 4. å¯è§†åŒ–é™ç»´ç®—æ³•è®¾ç½® (å†³å®šæ€ä¹ˆâ€œç”»â€å›¾)
# é€‰é¡¹: 'tsne' (æœ€å¸¸ç”¨ï¼Œæ•ˆæœå¥½), 'pca' (æœ€å¿«ï¼Œçº¿æ€§), 'umap' (å¹³è¡¡ï¼Œéœ€å®‰è£…umap-learn)
VISUALIZATION_METHOD = "tsne"

# 5. æ•°æ®é¢„å¤„ç†ä¸é«˜çº§å‚æ•° (ğŸ”¥ æ–°å¢ï¼šè§£å†³èšç±»â€œç³Šæˆä¸€å›¢â€çš„ä¼˜åŒ–é¡¹)
# -------------------------------------------------------------
# æ˜¯å¦åœ¨èšç±»å’Œç”»å›¾å‰ï¼Œå…ˆå¯¹ Embedding è¿›è¡Œ PCA é™ç»´å»å™ªï¼Ÿ
# æ¨è: Trueã€‚é€šå¸¸ Sentence Embedding ç»´åº¦å¾ˆé«˜(1024ç»´)ï¼Œç›´æ¥èšç±»æ•ˆæœä¸å¥½ã€‚
# é™ç»´åˆ° 50 ç»´å·¦å³é€šå¸¸èƒ½å»é™¤å™ªéŸ³ï¼Œæ˜¾è‘—æ”¹å–„ t-SNE çš„åˆ†ç¦»æ•ˆæœã€‚
ENABLE_PCA_PREPROCESS = True
PCA_PREPROCESS_DIMS = 50 

# t-SNE å›°æƒ‘åº¦ (Perplexity): 
# æ§åˆ¶ t-SNE å…³æ³¨å±€éƒ¨è¿˜æ˜¯å…¨å±€ã€‚æ•°æ®ç‚¹å¤šæ—¶(>1000)å»ºè®®è°ƒå¤§ (30-50)ï¼Œå°‘æ—¶è°ƒå° (5-20)ã€‚
# è°ƒæ•´è¿™ä¸ªå‚æ•°å¾€å¾€èƒ½æŠŠ"ç³Šæˆä¸€å›¢"çš„æ•°æ®æ‹‰å¼€ã€‚
TSNE_PERPLEXITY = 40
# -------------------------------------------------------------

# [Embedding æ¨¡å‹]
EMBEDDING_MODEL = "BAAI/bge-large-en-v1.5" 
# ===========================================

GLOBAL_MODEL = None
GLOBAL_TOKENIZER = None

# =============== 0. å·¥å…·å‡½æ•° ===============
def clean_special_chars(text: str) -> str:
    if not isinstance(text, str): return text
    return text.replace('\u2028', ' ').replace('\u2029', ' ')

def normalize_text(x: str) -> str:
    x = x.lower()
    x = re.sub(r"\d+(\.\d+)?", " <num> ", x) 
    x = re.sub(r"\s+", " ", x).strip()
    return x

def import_torch_and_check_gpu():
    try: return torch.cuda.is_available()
    except: return False

# =============== 1. LLM åˆå§‹åŒ–ä¸è°ƒç”¨ ===============

def init_llm():
    global GLOBAL_MODEL, GLOBAL_TOKENIZER
    
    if MODEL_SOURCE == "gemini":
        if GEMINI_API_KEY:
            genai.configure(api_key=GEMINI_API_KEY)
            print(f"ğŸ¤– [Init] Gemini API ({GEMINI_MODEL_NAME}) å·²é…ç½®")
        else:
            print("âš ï¸ [Init] æœªæ£€æµ‹åˆ° GEMINI_API_KEYï¼ŒGemini æ¨¡å¼å¯èƒ½æ— æ³•å·¥ä½œ")
            
    elif MODEL_SOURCE == "huggingface":
        print(f"ğŸ“¥ [Init] æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹: {HF_MODEL_NAME} ...")
        try:
            GLOBAL_TOKENIZER = AutoTokenizer.from_pretrained(HF_MODEL_NAME, trust_remote_code=True)
            GLOBAL_MODEL = AutoModelForCausalLM.from_pretrained(
                HF_MODEL_NAME,
                device_map="auto",
                torch_dtype="auto",
                trust_remote_code=True
            ).eval()
            print("âœ… [Init] æœ¬åœ°æ¨¡å‹åŠ è½½å®Œæˆï¼")
        except Exception as e:
            print(f"âŒ [Init] æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ’¡ æç¤º: è¯·ç¡®ä¿å·²é€šè¿‡ `huggingface-cli login` ç™»å½•æˆ–æ£€æŸ¥ç½‘ç»œ")

def call_llm(prompt: str) -> str:
    if MODEL_SOURCE == "gemini":
        if not GEMINI_API_KEY: return "Skipped (No Key)"
        model = genai.GenerativeModel(GEMINI_MODEL_NAME) 
        try:
            print("  ğŸ¤– [Gemini] æ­£åœ¨æ€è€ƒ...", end="", flush=True)
            resp = model.generate_content(prompt)
            print(" å®Œæˆ!")
            return clean_special_chars(resp.text.strip())
        except Exception as e:
            print(f"\nâŒ [Gemini Error]: {e}")
            time.sleep(1)
            return "Unknown Topic"

    elif MODEL_SOURCE == "huggingface":
        if GLOBAL_MODEL is None:
            return "Skipped (Model Not Loaded)"
        try:
            print("  ğŸš€ [Local] æ­£åœ¨æ¨ç†...", end="", flush=True)
            messages = [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt}
            ]
            text = GLOBAL_TOKENIZER.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            model_inputs = GLOBAL_TOKENIZER([text], return_tensors="pt").to(GLOBAL_MODEL.device)

            with torch.no_grad():
                generated_ids = GLOBAL_MODEL.generate(model_inputs.input_ids, max_new_tokens=50, do_sample=False)
            
            generated_ids = [
                output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
            ]
            response = GLOBAL_TOKENIZER.batch_decode(generated_ids, skip_special_tokens=True)[0]
            print(" å®Œæˆ!")
            return clean_special_chars(response.strip())
        except Exception as e:
            print(f"\nâŒ [Local Error]: {e}")
            return "Unknown Topic"
    return "Unknown Config"

# =============== 2. åŸºç¡€ IO ===============
def load_questions(jsonl_path: str):
    print(f"ğŸ“¥ æ­£åœ¨åŠ è½½æ–‡ä»¶: {jsonl_path}...")
    if not os.path.exists(jsonl_path):
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {jsonl_path}")
        return [], []

    ids, questions = [], []
    with open(jsonl_path, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip(): continue
            try:
                obj = json.loads(line)
            except json.JSONDecodeError: continue
            
            content = obj.get("contents", "")
            if "Question:" in content:
                q_part = content.split("Solution:")[0].replace("Question:", "").strip()
            else:
                q_part = content
            
            ids.append(str(obj["memory_id"]))
            questions.append(clean_special_chars(q_part))
            
    print(f"âœ… åŠ è½½å®Œæˆï¼Œå…± {len(questions)} æ¡æ•°æ®")
    return ids, questions

# =============== 3. ä¸»æµç¨‹ï¼šembedding + è‡ªåŠ¨èšç±» ===============
def build_embeddings(questions: List[str], model_name: str) -> np.ndarray:
    print(f"ğŸš€ æ­£åœ¨è®¡ç®— Embeddings ({model_name})...")
    device = "cuda" if import_torch_and_check_gpu() else "cpu"
    print(f"   >>> ä½¿ç”¨è®¾å¤‡: {device}")
    
    model = SentenceTransformer(model_name, device=device)
    q_norm = [normalize_text(q) for q in questions]
    emb = model.encode(q_norm, batch_size=32, show_progress_bar=True, normalize_embeddings=True)
    return np.asarray(emb)

def preprocess_embeddings_pca(embeddings: np.ndarray, n_components: int) -> np.ndarray:
    """
    ğŸ”¥ æ–°å¢é¢„å¤„ç†å‡½æ•°: ä½¿ç”¨ PCA é™ç»´å»å™ª
    """
    print(f"ğŸ§¹ æ­£åœ¨æ‰§è¡Œ PCA é¢„å¤„ç† (é™ç»´: {embeddings.shape[1]} -> {n_components})...")
    if embeddings.shape[0] < n_components:
        print(f"âš ï¸ æ ·æœ¬æ•° ({embeddings.shape[0]}) å°‘äºç›®æ ‡ç»´åº¦ ({n_components})ï¼Œè·³è¿‡ PCA é¢„å¤„ç†ã€‚")
        return embeddings
    
    pca = PCA(n_components=n_components)
    reduced = pca.fit_transform(embeddings)
    
    # æ‰“å°ä¿ç•™çš„æ–¹å·®æ¯”ä¾‹ï¼Œè®©ç”¨æˆ·çŸ¥é“æŸå¤±äº†å¤šå°‘ä¿¡æ¯
    explained_variance = np.sum(pca.explained_variance_ratio_)
    print(f"   >>> ä¿ç•™æ–¹å·®æ¯”ä¾‹: {explained_variance:.2%}")
    return reduced

def cluster_questions_auto(embeddings: np.ndarray) -> np.ndarray:
    if CLUSTERING_METHOD == "kmeans":
        print(f"ğŸ¤– æ­£åœ¨æ‰§è¡Œ K-Means èšç±» (N_Clusters={KMEANS_N_CLUSTERS})...")
        model = KMeans(n_clusters=KMEANS_N_CLUSTERS, random_state=42, n_init='auto')
        labels = model.fit_predict(embeddings)
        print(f"âœ¨ K-Means èšç±»å®Œæˆï¼å…±ç”Ÿæˆ {KMEANS_N_CLUSTERS} ä¸ªç±»åˆ«ã€‚")
        return labels
        
    elif CLUSTERING_METHOD == "agglomerative":
        print(f"ğŸ¤– æ­£åœ¨æ‰§è¡Œå±‚æ¬¡èšç±» Agglomerative (Distance Threshold={DISTANCE_THRESHOLD})...")
        model = AgglomerativeClustering(
            n_clusters=None, 
            distance_threshold=DISTANCE_THRESHOLD,
            metric='euclidean', 
            linkage='ward'
        )
        labels = model.fit_predict(embeddings)
        n_clusters_found = len(set(labels))
        print(f"âœ¨ å±‚æ¬¡èšç±»å®Œæˆï¼æ¨¡å‹è‡ªåŠ¨å‘ç°äº† {n_clusters_found} ä¸ªé¢˜å‹ç±»åˆ«ã€‚")
        return labels
    
    else:
        raise ValueError(f"æœªçŸ¥çš„èšç±»æ–¹æ³•: {CLUSTERING_METHOD}")

# =============== 4. ç»Ÿè®¡ç»˜å›¾ & å…³é”®è¯ & é™ç»´å¯è§†åŒ– ===============

def plot_cluster_stats(labels: np.ndarray, save_path: str):
    print(f"\nğŸ“Š æ­£åœ¨ç”Ÿæˆç»Ÿè®¡å›¾è¡¨...")
    unique_labels, counts = np.unique(labels, return_counts=True)
    
    singleton_mask = counts == 1
    num_singletons = np.sum(singleton_mask)
    
    valid_mask = ~singleton_mask
    valid_labels = unique_labels[valid_mask]
    valid_counts = counts[valid_mask]
    
    print(f"   - æ€»èšç±»æ•°: {len(unique_labels)}")
    print(f"   - å­¤ç«‹èšç±»æ•° (Size=1): {num_singletons}")
    print(f"   - æœ‰æ•ˆèšç±»æ•° (Size>1): {len(valid_labels)}")
    
    if len(valid_counts) == 0:
        print("   âš ï¸ æ²¡æœ‰åŒ…å«å¤šä¸ªé—®é¢˜çš„èšç±»ï¼Œè·³è¿‡ç»˜å›¾ã€‚")
        return

    sorted_indices = np.argsort(valid_counts)[::-1]
    sorted_plot_labels = valid_labels[sorted_indices]
    sorted_plot_counts = valid_counts[sorted_indices]
    
    plt.figure(figsize=(12, 6))
    x_ticks = [str(lbl) for lbl in sorted_plot_labels]
    plt.bar(x_ticks, sorted_plot_counts, color='steelblue', edgecolor='black', alpha=0.8)
    plt.xlabel('Cluster ID', fontsize=12)
    plt.ylabel('Number of Questions', fontsize=12)
    plt.title(f'Cluster Size Distribution (Descending)\nMethod: {CLUSTERING_METHOD}', fontsize=14)
    if len(x_ticks) > 30: plt.xticks(rotation=90, fontsize=8)
    else: plt.xticks(rotation=0)
    plt.grid(axis='y', linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig(save_path)
    print(f"ğŸ–¼ï¸ å›¾è¡¨å·²ä¿å­˜è‡³: {save_path}")

def plot_dimensionality_reduction(embeddings: np.ndarray, labels: np.ndarray, method: str, save_path: str):
    """
    ğŸ”¥ ç»Ÿä¸€çš„é™ç»´å¯è§†åŒ–å‡½æ•°ï¼Œæ”¯æŒ t-SNE, PCA, UMAP
    """
    print(f"\nğŸ¨ æ­£åœ¨ç”Ÿæˆ {method.upper()} èšç±»åˆ†å¸ƒå›¾...")
    if embeddings.shape[0] < 2:
        print("âš ï¸ æ•°æ®ç‚¹å¤ªå°‘ï¼Œè·³è¿‡å¯è§†åŒ–ã€‚")
        return

    reducer = None
    
    # --- 1. é€‰æ‹©ç®—æ³• ---
    if method == "tsne":
        n_samples = embeddings.shape[0]
        # å…è®¸ç”¨æˆ·é€šè¿‡å…¨å±€å‚æ•° TSNE_PERPLEXITY è°ƒæ•´
        perplexity_val = min(TSNE_PERPLEXITY, n_samples - 1) if n_samples > 1 else 1
        print(f"   >>> è¿è¡Œ t-SNE (perplexity={perplexity_val})...")
        
        reducer = TSNE(
            n_components=2, 
            perplexity=perplexity_val, 
            random_state=42, 
            init='pca', 
            learning_rate='auto'
        )
        
    elif method == "pca":
        print(f"   >>> è¿è¡Œ PCA (Linear)...")
        reducer = PCA(n_components=2)
        
    elif method == "umap":
        if umap is None:
            print("âŒ æœªæ£€æµ‹åˆ° UMAP åº“ã€‚è¯·è¿è¡Œ `pip install umap-learn` å®‰è£…ã€‚")
            print("   (å°†è‡ªåŠ¨å›é€€åˆ° t-SNE)")
            return plot_dimensionality_reduction(embeddings, labels, "tsne", save_path)
        print(f"   >>> è¿è¡Œ UMAP...")
        reducer = umap.UMAP(n_components=2, random_state=42)
        
    else:
        print(f"âŒ æœªçŸ¥çš„å¯è§†åŒ–æ–¹æ³•: {method}")
        return

    # --- 2. é™ç»´ ---
    reduced_emb = reducer.fit_transform(embeddings)

    # --- 3. ç»˜å›¾ ---
    plt.figure(figsize=(12, 10))
    scatter = plt.scatter(
        reduced_emb[:, 0], 
        reduced_emb[:, 1], 
        c=labels, 
        cmap='nipy_spectral', 
        s=15, 
        alpha=0.6,
        edgecolor='none'
    )
    
    plt.colorbar(scatter, label='Cluster ID')
    plt.title(f'{method.upper()} Visualization\n(Cluster: {CLUSTERING_METHOD}, Preprocess: {ENABLE_PCA_PREPROCESS})', fontsize=15)
    plt.xlabel(f'{method.upper()} Dimension 1')
    plt.ylabel(f'{method.upper()} Dimension 2')
    plt.grid(True, linestyle='--', alpha=0.3)
    plt.tight_layout()
    
    plt.savefig(save_path, dpi=300)
    print(f"ğŸ–¼ï¸ å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜è‡³: {save_path}")

def tfidf_keywords_per_cluster(questions, cluster_labels, max_features=5000, top_k=10):
    print("ğŸ” æå–å…³é”®è¯...")
    q_norm = [normalize_text(q) for q in questions]
    vectorizer = TfidfVectorizer(max_df=0.9, min_df=3, max_features=max_features, stop_words="english")
    X = vectorizer.fit_transform(q_norm)
    vocab = np.array(vectorizer.get_feature_names_out())

    cluster_keywords = {}
    for cid in np.unique(cluster_labels):
        idx = np.where(cluster_labels == cid)[0]
        if len(idx) == 0: continue
        tfidf_mean = np.asarray(X[idx].mean(axis=0)).ravel()
        top_idx = tfidf_mean.argsort()[::-1][:top_k]
        cluster_keywords[cid] = vocab[top_idx].tolist()
    return cluster_keywords

def llm_label_cluster(cid, questions, cluster_labels, cluster_keywords, max_examples=5):
    idx = np.where(cluster_labels == cid)[0]
    examples_idx = np.random.choice(idx, min(len(idx), max_examples), replace=False)
    examples = [questions[i] for i in examples_idx]
    kw = ", ".join(cluster_keywords.get(cid, []))

    prompt = f"""You are a Math Education Expert. 
I have automatically grouped similar math problems together.
Keywords: [{kw}]
Examples:
{chr(10).join(f"- {q}" for q in examples)}

Task: Provide a **very short category name** (3-6 words) for this specific math problem type.
Output ONLY the category name.
"""
    label = call_llm(prompt)
    return label.replace("\n", "").replace('"', "").strip()

# =============== Main ===============
def cluster():
    init_llm()

    ids, questions = load_questions(INPUT_FILE)
    if not ids: return

    embeddings = build_embeddings(questions, model_name=EMBEDDING_MODEL)
    
    # ğŸ”¥ 1. é¢„å¤„ç† (æ–°å¢æ­¥éª¤ï¼šé™ç»´å»å™ª)
    if ENABLE_PCA_PREPROCESS:
        embeddings = preprocess_embeddings_pca(embeddings, n_components=PCA_PREPROCESS_DIMS)

    # 2. èšç±»
    labels = cluster_questions_auto(embeddings)

    # 3. ç”»å›¾
    plot_cluster_stats(labels, save_path=PLOT_FILE)
    
    # 4. å¯è§†åŒ– (t-SNE/PCA/UMAP)
    plot_dimensionality_reduction(embeddings, labels, method=VISUALIZATION_METHOD, save_path=VIS_PLOT_FILE)

    # 5. åˆ†æå…³é”®è¯ä¸ä¿å­˜
    keywords_map = tfidf_keywords_per_cluster(questions, labels)
    
    print("\n" + "="*20 + " èšç±»ç»“æœåˆ†æ " + "="*20)
    cluster_labels_text = {}
    
    unique, counts = np.unique(labels, return_counts=True)
    sorted_clusters = sorted(zip(unique, counts), key=lambda x: x[1], reverse=True)
    
    print(f"ğŸ“Š æ€»å…±å‘ç° {len(sorted_clusters)} ä¸ªèšç±»ã€‚")
    print("   (ä»…å±•ç¤ºå¹¶å‘½ååŒ…å«é¢˜ç›®æœ€å¤šçš„å‰ 10 ä¸ªèšç±»)\n")

    for cid, count in sorted_clusters[:10]:
        print(f"\nğŸ·ï¸ åˆ†æ Cluster {cid} (åŒ…å« {count} é¢˜)...")
        label_text = llm_label_cluster(cid, questions, labels, keywords_map)
        cluster_labels_text[cid] = label_text
        print(f"   >>> é¢˜å‹: {label_text}")
        print(f"   >>> å…³é”®è¯: {keywords_map.get(cid, [])}")
        if MODEL_SOURCE == "gemini": time.sleep(2)

    print(f"\nğŸ’¾ ä¿å­˜è¯¦ç»†ç»“æœåˆ° {OUTPUT_FILE}...")
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        for qid, q, cid in zip(ids, questions, labels):
            obj = {
                "id": qid,
                "question": q,
                "cluster_id": int(cid),
                "cluster_label": cluster_labels_text.get(int(cid), f"Cluster {cid}"),
                "cluster_keywords": keywords_map.get(int(cid), [])
            }
            f.write(json.dumps(obj, ensure_ascii=False) + "\n")
            
    print(f"ğŸ’¾ ä¿å­˜èšç±»æ‘˜è¦åˆ° {SUMMARY_OUTPUT_FILE}...")
    cluster_aggregation = {}
    for qid, cid in zip(ids, labels):
        cid_int = int(cid)
        if cid_int not in cluster_aggregation:
            cluster_aggregation[cid_int] = {
                "cluster_id": cid_int,
                "cluster_label": cluster_labels_text.get(cid_int, f"Cluster {cid_int}"),
                "memory_ids": []
            }
        cluster_aggregation[cid_int]["memory_ids"].append(qid)
    
    with open(SUMMARY_OUTPUT_FILE, "w", encoding="utf-8") as f:
        for cid in sorted(cluster_aggregation.keys()):
            f.write(json.dumps(cluster_aggregation[cid], ensure_ascii=False) + "\n")
            
    print("âœ… å…¨éƒ¨å®Œæˆï¼")

if __name__ == "__main__":
    cluster()