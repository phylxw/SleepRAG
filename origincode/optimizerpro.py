import os
import json
import time
from typing import Dict, List, Tuple, Set
import numpy as np
import torch
from sentence_transformers import SentenceTransformer
# Hydra
import hydra
from omegaconf import DictConfig
from utils.toolfunction import clean_special_chars, has_cuda
from tools.optimize.callllm import init_llm, call_llm, call_llm_batch
from tools.optimize.memoryload import load_clustered_memories, load_cluster_summary

# ================= å…¨å±€å˜é‡ =================
GLOBAL_MODEL = None
GLOBAL_TOKENIZER = None
GLOBAL_SGLANG_CLIENT = None

# ==========================================
# 1. Prompt æ„é€ å‡½æ•°
# ==========================================

def textgrad_correction_prompt(content: str, neg_queries: List[str], good_examples: str, cfg: DictConfig) -> str:
    """
    TextGrad æ ¸å¿ƒ Promptï¼šç»“åˆé”™è¯¯åé¦ˆ(Gradient)å’Œæ­£å‘ç¤ºä¾‹(Momentum)æ¥ä¿®æ­£è®°å¿†
    """
    # å–å‰ 3 ä¸ªé”™è¯¯ Query ä½œä¸ºæ¢¯åº¦ä¿¡å·
    neg_text = "\n".join([f"- {q}" for q in neg_queries[:3]])
    
    # å°è¯•ä» config è¯»å–æ¨¡æ¿ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤ç¡¬ç¼–ç æ¨¡æ¿
    default_template = """
You are optimizing a memory entry for a Retrieval-Augmented Generation (RAG) system.

[Original Memory]
{content}

[Critique / Gradient]
This memory was INCORRECTLY retrieved for the following queries (it misled the system):
{neg_text}

[Positive Guidance / Momentum]
Successful neighboring memories look like this (try to mimic their style/depth):
{good_examples}

[Task]
Rewrite the memory content. 
1. Make it SPECIFIC enough to avoid being retrieved for the incorrect queries above.
2. Maintain its core utility but clarify ambiguities.
3. If the memory contains factual errors, fix them based on common knowledge.

Output ONLY the rewritten memory content.
"""
    # å®‰å…¨è·å–æ¨¡æ¿
    template = default_template
    if hasattr(cfg.optimizer, "prompts") and "textgrad_correction" in cfg.optimizer.prompts:
        template = cfg.optimizer.prompts.textgrad_correction
    
    prompt = template.format(content=content, neg_text=neg_text, good_examples=good_examples)
    return prompt

def summarize_experience_prompt(target_text: str, good_neighbors: List[str], cfg: DictConfig) -> str:
    """æ—§é€»è¾‘ï¼šåˆ©ç”¨é«˜åˆ†é‚»å±…ä¿®æ­£ä½åˆ†è®°å¿† (Imitation)"""
    good_examples_text = "\n".join(f"[{i+1}] {t}" for i, t in enumerate(good_neighbors))
    template = cfg.optimizer.prompts.expand_low_freq
    prompt = template.format(text=target_text, good_examples=good_examples_text)
    return prompt

def expand_low_freq_memory_prompt(text: str, good_examples: str, cfg: DictConfig) -> str:
    """æ—§é€»è¾‘ï¼šè‡ªæˆ‘æ‰©å†™ (Fallback)"""
    template = cfg.optimizer.prompts.expand_low_freq
    prompt = template.format(text=text, good_examples=good_examples)
    return prompt

# ==========================================
# 2. ç­›é€‰é€»è¾‘ (é€‚é… BEMR Stats)
# ==========================================

def select_ids_from_stats(memory_stats: Dict[str, dict], cfg: DictConfig):
    """
    æ ¹æ® BEMR Stats (Alpha/Beta) ç­›é€‰é«˜åˆ†å’Œä½åˆ†è®°å¿†
    """
    # è®¡ç®—èƒœç‡åˆ†æ•°
    scores = []
    for mid, stats in memory_stats.items():
        alpha = stats.get('alpha', 1.0)
        beta = stats.get('beta', 1.0)
        total = alpha + beta
        # è®¡ç®—èƒœç‡ (0.0 - 1.0)
        win_rate = alpha / total if total > 0 else 0.5
        scores.append((mid, win_rate, total))
    
    # æ’åºï¼šæŒ‰èƒœç‡é™åºï¼Œèƒœç‡ç›¸åŒæŒ‰å°è¯•æ¬¡æ•°é™åº
    scores.sort(key=lambda x: (-x[1], -x[2]))
    
    # ç­›é€‰
    top_k = cfg.optimizer.top_k_high
    bottom_k = cfg.optimizer.bottom_k_low
    
    high_ids = [x[0] for x in scores[:top_k]]
    
    # ä½åˆ†ï¼šåªé€‰é‚£äº›å°è¯•è¿‡ä¸”å¤±è´¥è¿‡çš„ (win_rate < 0.4 ä¸” total > 2)
    # è¿™ç§ç­›é€‰èƒ½ä¿è¯ TextGrad æœ‰è¶³å¤Ÿçš„â€œé”™è¯¯æ¢¯åº¦â€å»ä¼˜åŒ–
    bad_ids = [x[0] for x in scores if x[1] < 0.4 and x[2] > 2]
    
    # å¦‚æœæ²¡é€‰å¤Ÿï¼Œå°±ç¡¬å‡‘æœ€åå‡ ä¸ªå«åº•çš„
    if len(bad_ids) < 10:
         bad_ids = [x[0] for x in scores[-bottom_k:]]

    print(f"ğŸ”¥ é«˜åˆ† Anchor (ç”¨äºæŒ‡å¯¼): {len(high_ids)}")
    print(f"ğŸ¥¶ ä½åˆ† Candidates (éœ€è¦ä¿®æ­£): {len(bad_ids)}")
    
    return set(high_ids), set(bad_ids)

# ==========================================
# 3. ä¸»ä¼˜åŒ–é€»è¾‘ (Hydra Managed)
# ==========================================

@hydra.main(version_base=None, config_path="conf", config_name="config")
def optimize_memory(cfg: DictConfig):
    # 0. åˆå§‹åŒ– LLM
    init_llm(cfg)

    # 1. è¯»å…¥è·¯å¾„ (ğŸ”¥ ä½¿ç”¨ yaml ä¸­å®šä¹‰çš„é™æ€è·¯å¾„)
    # ä½ çš„ config.yaml å·²ç»å®šä¹‰å¥½äº†å®Œæ•´çš„è·¯å¾„ï¼Œç›´æ¥ç”¨å³å¯
    cluster_file = cfg.paths.cluster_output
    summary_file = cfg.paths.cluster_summary
    stats_file = cfg.paths.stats_file       # å¯¹åº” ${experiment.tag}_memory_stats.json
    output_file = cfg.paths.optimized_memory # å¯¹åº” ${experiment.tag}_optimized_memory_topk.jsonl

    print(f"ğŸ“‚ [Input] èšç±»ç»“æœ: {cluster_file}")
    print(f"ğŸ“‚ [Input] ç»Ÿè®¡çŠ¶æ€: {stats_file}")
    print(f"ğŸ“‚ [Output] ä¼˜åŒ–ç»“æœ: {output_file}")

    # åŠ è½½æ•°æ®
    if not os.path.exists(stats_file):
        print(f"âŒ æ‰¾ä¸åˆ°çŠ¶æ€æ–‡ä»¶: {stats_file}ï¼Œæ— æ³•è¿›è¡Œ TextGrad ä¼˜åŒ–ï¼")
        return

    # åŠ è½½èšç±»æ•°æ®
    memories, id_order = load_clustered_memories(cluster_file)
    cluster_to_ids = load_cluster_summary(summary_file)
    
    # åŠ è½½ BEMR ç»Ÿè®¡æ•°æ®
    with open(stats_file, 'r', encoding='utf-8') as f:
        memory_stats = json.load(f)

    if not memories:
        print("âŒ æ— æ³•åŠ è½½è®°å¿†æ•°æ®ï¼Œç¨‹åºé€€å‡ºã€‚")
        return

    # 2. ç­›é€‰é›†åˆ (é«˜åˆ†åšè€å¸ˆï¼Œä½åˆ†åšå­¦ç”Ÿ)
    high_ids, bad_ids = select_ids_from_stats(memory_stats, cfg)

    # =========================================================
    # 4. é«˜é¢‘/é«˜åˆ†ä¼˜åŒ– (Pruning: ä¼˜èƒœåŠ£æ±°)
    # =========================================================
    print("\n========== é«˜åˆ†è®°å¿†æ¸…ç†é˜¶æ®µ (Pruning) ==========")
    to_delete_ids = set() 
    
    # æŒ‰ Cluster åˆ†ç»„
    cluster_groups = {}
    for mid, rec in memories.items():
        cid = rec.get("cluster_id")
        if cid is not None:
            cid = int(cid)
            if cid not in cluster_groups: cluster_groups[cid] = []
            cluster_groups[cid].append(mid)
    
    pruned_count = 0
    
    for cid, members in cluster_groups.items():
        if len(members) < 2: continue # ç‹¬ç”Ÿå­ä¸åˆ 
        
        # è·å–è¯¥ Cluster å†…æ‰€æœ‰æˆå‘˜çš„ Stats
        member_stats_list = []
        for mid in members:
            stats = memory_stats.get(mid, {'alpha': 1.0, 'beta': 1.0})
            total = stats['alpha'] + stats['beta']
            win_rate = stats['alpha'] / total if total > 0 else 0.5
            member_stats_list.append({
                'id': mid,
                'win_rate': win_rate,
                'total': total
            })
            
        # æ‰¾å‡ºè¯¥ Cluster çš„â€œæœ€å¼ºç‹è€…â€ (Anchor)
        member_stats_list.sort(key=lambda x: (-x['win_rate'], -x['total']))
        best_mem = member_stats_list[0]
        
        # æ¡ä»¶ A: æœ‰å¼ºåŠ› Anchor (èƒœç‡ > 0.7 ä¸”éªŒè¯è¿‡)
        has_strong_anchor = (best_mem['win_rate'] > 0.7 and best_mem['total'] > 2)
        
        if has_strong_anchor:
            # æ¡ä»¶ B: åˆ é™¤åƒåœ¾å°å¼Ÿ
            for mem in member_stats_list[1:]:
                is_trash = False
                # æƒ…å†µ 1: ç¡®å®çƒ‚ (<40% ä¸”ä¸æ˜¯å†·å¯åŠ¨)
                if mem['win_rate'] < 0.4 and mem['total'] > 2:
                    is_trash = True
                # æƒ…å†µ 2: ä¸¥é‡å¹²æ‰° (Anchor > 90% ä½†å°å¼Ÿ < 50%)
                if best_mem['win_rate'] > 0.9 and mem['win_rate'] < 0.5:
                    is_trash = True
                
                if is_trash:
                    to_delete_ids.add(mem['id'])
                    pruned_count += 1

    print(f"âœ¨ Pruning å®Œæˆï¼Œå…±åˆ é™¤äº† {pruned_count} æ¡åŠ£è´¨å†—ä½™è®°å¿†ã€‚")

    # =========================================================
    # 5. TextGrad æ ¸å¿ƒä¿®æ­£é˜¶æ®µ
    # =========================================================
    print("\n========== TextGrad è®°å¿†ä¿®æ­£é˜¶æ®µ (Gradient Descent) ==========")
    
    # ç­›é€‰éœ€è¦å¤„ç†çš„ ID (åœ¨ bad_ids é‡Œä¸”æœªè¢« Pruning åˆ é™¤)
    low_expand_ids = [mid for mid in bad_ids if mid in memories and mid not in to_delete_ids]
    print(f"ğŸ¯ å¾…ä¼˜åŒ–ç›®æ ‡æ•°é‡: {len(low_expand_ids)}")

    batch_size = cfg.optimizer.llm_batch_size
    batch_prompts = []
    batch_metadata = [] 

    for mid in low_expand_ids:
        rec = memories[mid]
        base_text = rec.get("contents", "")
        cluster_id = rec.get("cluster_id")
        
        # è·å– BEMR Stats
        stats = memory_stats.get(mid, {})
        neg_queries = stats.get('neg_queries', []) # ğŸ”¥ é”™è¯¯åé¦ˆ (Gradient)
        
        # å¯»æ‰¾â€œä¼˜ç­‰ç”Ÿâ€ (Momentum)
        good_neighbors_text = []
        if cluster_id is not None:
            cluster_id = int(cluster_id)
            members = cluster_to_ids.get(cluster_id, [])
            # æ‰¾åŒç±»é‡Œçš„é«˜åˆ† (Score > 0.8)
            for m_id in members:
                m_id = str(m_id)
                if m_id == mid: continue
                s = memory_stats.get(m_id, {})
                s_total = s.get('alpha', 0) + s.get('beta', 0)
                if s_total > 0 and (s.get('alpha', 0)/s_total) > 0.8:
                    good_neighbors_text.append(memories[m_id].get("contents", ""))
            
            # å– Top 3
            good_neighbors_text = good_neighbors_text[:3]
        
        good_examples_str = "\n".join([f"- {t}" for t in good_neighbors_text])

        # ğŸ”¥ åˆ†æ”¯åˆ¤æ–­ï¼šä¼˜å…ˆ TextGrad
        if len(neg_queries) > 0:
            # Case A: TextGrad ä¿®æ­£ (æœ€å¼º)
            prompt = textgrad_correction_prompt(base_text, neg_queries, good_examples_str, cfg)
            opt_type = f"textgrad_with_{len(neg_queries)}_errors"
        elif good_neighbors_text:
            # Case B: æ¨¡ä»¿ä¼˜ç­‰ç”Ÿ (æ¬¡é€‰)
            prompt = summarize_experience_prompt(base_text, good_neighbors_text, cfg)
            opt_type = "neighbor_imitation"
        else:
            # Case C: è‡ªæˆ‘åæ€ (ä¿åº•)
            prompt = expand_low_freq_memory_prompt(base_text, "", cfg)
            opt_type = "self_reflection"

        batch_prompts.append(prompt)
        batch_metadata.append({"mid": mid, "opt_type": opt_type})

        # æ‰§è¡Œ Batch
        if len(batch_prompts) >= batch_size:
            print(f"ğŸš€ [Batch] å¤„ç† {len(batch_prompts)} æ¡ (å« TextGrad)...")
            outputs = call_llm_batch(batch_prompts, cfg)
            
            for meta, output_text in zip(batch_metadata, outputs):
                if output_text and len(output_text) > 10:
                    mid = meta['mid']
                    memories[mid]["contents"] = output_text
                    memories[mid]["opt_type"] = meta['opt_type']
            
            batch_prompts = []
            batch_metadata = []

    # å¤„ç†å‰©ä½™ Batch
    if batch_prompts:
        print(f"ğŸš€ [Batch] å¤„ç†å‰©ä½™ {len(batch_prompts)} æ¡...")
        outputs = call_llm_batch(batch_prompts, cfg)
        for meta, output_text in zip(batch_metadata, outputs):
            if output_text and len(output_text) > 10:
                mid = meta['mid']
                memories[mid]["contents"] = output_text
                memories[mid]["opt_type"] = meta['opt_type']

    # 6. å†™å‡ºç»“æœ
    print("\n========== å†™å‡ºä¼˜åŒ–åçš„è®°å¿†åº“ ==========")
    kept_count = 0
    with open(output_file, "w", encoding="utf-8") as f:
        for mid in id_order:
            if mid not in memories: continue
            if mid in to_delete_ids: continue
            f.write(json.dumps(memories[mid], ensure_ascii=False) + "\n")
            kept_count += 1

    print(f"âœ… å®Œæˆï¼ä¼˜åŒ–åè®°å¿†åº“: {output_file}")
    print(f"   ä¿ç•™: {kept_count} | åˆ é™¤: {len(to_delete_ids)}")

if __name__ == "__main__":
    optimize_memory()