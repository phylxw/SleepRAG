import os
import json
from typing import Dict, List
import hydra
from omegaconf import DictConfig
import logging

# ğŸ¤« æ—¥å¿—é™å™ª
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)

# å‡è®¾ä½ çš„æ–‡ä»¶ç»“æ„å¦‚ä¸‹ï¼Œè¯·ç¡®ä¿ import è·¯å¾„æ­£ç¡®
from tools.optimize.callllm import init_llm, call_llm_batch
from tools.optimize.callexpert import init_expert_llm, call_expert, call_expert_batch
from tools.optimize.memoryload import load_clustered_memories, load_cluster_summary
from optimizeold.select import select_ids_from_stats
from optimizeold.prune import prune
# ä¸€å®šè¦å¼•ç”¨æˆ‘ä»¬åˆšåˆšæ”¹å¥½çš„æ–°ç‰ˆ textgrad_opt
from optimizeold.textgrad_optpro import textgrad_opt 
from optimizeold.evolve import evolve_high_score_opt

@hydra.main(version_base=None, config_path="conf", config_name="config")
def optimize_memory(cfg: DictConfig):
    # =========================================================
    # 0. åˆå§‹åŒ–
    # =========================================================
    init_llm(cfg)          # å­¦ç”Ÿ (Qwen/DeepSeek)
    init_expert_llm(cfg)   # ä¸“å®¶ (Gemini/GPT4)

    # 1. è·¯å¾„é…ç½®
    cluster_file = cfg.paths.cluster_output
    summary_file = cfg.paths.cluster_summary
    stats_file = cfg.paths.stats_file
    output_file = cfg.paths.optimized_memory
    stats_optimized_file = cfg.paths.stats_optimized_file

    # 2. åŠ è½½æ•°æ®
    if not os.path.exists(stats_file):
        print(f"âŒ æ‰¾ä¸åˆ°çŠ¶æ€æ–‡ä»¶: {stats_file}")
        return
    with open(stats_file, 'r', encoding='utf-8') as f:
        memory_stats = json.load(f)

    # memories: dict, id_order: list (æ—§çš„é¡ºåº)
    memories, id_order = load_clustered_memories(cluster_file)
    cluster_to_ids = load_cluster_summary(summary_file)
    
    if not memories: 
        print("âŒ è®°å¿†åº“åŠ è½½ä¸ºç©ºï¼Œé€€å‡ºã€‚")
        return

    # =========================================================
    # 3. ç­›é€‰ (Select)
    # =========================================================
    # high_ids: é«˜åˆ†è®°å¿†, bad_ids: ä½åˆ†è®°å¿†
    high_ids, bad_ids = select_ids_from_stats(memory_stats, cfg)

    # =========================================================
    # 4. å‰ªæ (Prune) - æ ‡è®°è¦åˆ é™¤çš„ ID
    # =========================================================
    to_delete_ids = prune(memories, memory_stats)
    print(f"ğŸ—‘ï¸ è®¡åˆ’åˆ é™¤ {len(to_delete_ids)} æ¡å†—ä½™/æ— æ•ˆè®°å¿†")

    # =========================================================
    # 4.5. é«˜åˆ†è¿›åŒ– (Evolve High Score) - ğŸ”¥ æ–°å¢ç¯èŠ‚
    # =========================================================
    # é’ˆå¯¹é«˜åˆ†ä½†æœ‰ç‘•ç–µçš„è®°å¿†ï¼Œç”Ÿæˆ SUPPLEMENT æˆ– SPLIT
    # æ³¨æ„ï¼šè¿™äº›æ–° ID å·²ç»åœ¨ memory_stats é‡Œåˆå§‹åŒ–è¿‡äº†
    new_supplement_ids = evolve_high_score_opt(cfg, memories, memory_stats, high_ids)

    # =========================================================
    # 5. ä½åˆ†ä¼˜åŒ– (TextGrad with Primitives)
    # =========================================================
    # é’ˆå¯¹ä½åˆ†è®°å¿†è¿›è¡Œä¿®å¤ã€é‡å†™æˆ–æ‰©å±•
    optimized_ids = textgrad_opt(cfg, memories, memory_stats, cluster_to_ids, bad_ids, to_delete_ids)

    # =========================================================
    # 6. å†™å‡ºæ–°è®°å¿†åº“ (Save)
    # =========================================================
    print("\n========== å†™å‡ºä¼˜åŒ–åçš„è®°å¿†åº“ ==========")
    
    # ğŸ”¥ [Critical Fix] ä¿®å¤æ–°è®°å¿†ä¸¢å¤±é—®é¢˜
    # æ‰¾å‡ºåŸæœ‰é¡ºåºé‡Œæ²¡æœ‰çš„æ–° ID (ç”± TextGrad EXPAND å’Œ Evolve SPLIT/SUPPLEMENT äº§ç”Ÿ)
    current_memory_ids = set(memories.keys())
    old_ids_set = set(id_order)
    new_ids = list(current_memory_ids - old_ids_set)
    
    if new_ids:
        print(f"âœ¨ æ£€æµ‹åˆ° {len(new_ids)} æ¡æ–°å¢è®°å¿† (Total New)ï¼Œæ­£åœ¨è¿½åŠ åˆ°ä¿å­˜åˆ—è¡¨...")
        # å°†æ–° ID è¿½åŠ åˆ°ä¿å­˜åˆ—è¡¨æœ«å°¾
        final_save_order = id_order + new_ids
    else:
        final_save_order = id_order

    kept_count = 0
    with open(output_file, "w", encoding="utf-8") as f:
        for mid in final_save_order:
            # 1. å¦‚æœ ID ä¸åœ¨å†…å­˜é‡Œ (å¯èƒ½åŠ è½½æ—¶å°±ä¸¢äº†)ï¼Œè·³è¿‡
            if mid not in memories: continue
            # 2. å¦‚æœ ID è¢«æ ‡è®°åˆ é™¤äº†ï¼Œè·³è¿‡
            if mid in to_delete_ids: continue
            
            # å†™å…¥
            f.write(json.dumps(memories[mid], ensure_ascii=False) + "\n")
            kept_count += 1
            
    print(f"âœ… è®°å¿†åº“å·²ä¿å­˜: {output_file} (å…± {kept_count} æ¡)")

    # =========================================================
    # 7. çŠ¶æ€åŒæ­¥ (Sync Stats)
    # =========================================================
    print("\n========== åŒæ­¥ BEMR çŠ¶æ€ (Stats Sync) ==========")
    
    # 1. ç‰©ç†åˆ é™¤ï¼šä» stats ä¸­å½»åº•ç§»é™¤è¢« pruned çš„ ID
    for del_id in to_delete_ids:
        if del_id in memory_stats:
            del memory_stats[del_id]
            
    # 2. çŠ¶æ€é‡ç½®ï¼šå¯¹æœ¬è½®å‘ç”Ÿè¿‡å˜åŠ¨çš„ ID (Refine/Replace/Expand/Supplement/Split)
    #    æˆ‘ä»¬éœ€è¦åˆå¹¶ optimized_ids (ä½åˆ†ä¼˜åŒ–) å’Œ new_supplement_ids (é«˜åˆ†è¿›åŒ–)
    #    å› ä¸ºå†…å®¹å˜äº†ï¼Œæ—§çš„ alpha/beta å°±ä¸å‡†äº†ï¼Œéœ€è¦é‡ç½®ä¸ºå…ˆéªŒå€¼
    
    # ğŸ”¥ [Fix] åˆå¹¶ä¸¤ä¸ªé›†åˆ
    all_changed_ids = optimized_ids.union(new_supplement_ids)
    
    for opt_id in all_changed_ids:
        if opt_id in memory_stats:
            memory_stats[opt_id]['alpha'] = 1.0
            memory_stats[opt_id]['beta'] = 1.0
            # è¿™é‡Œçš„ query æ¸…ç©ºä¼šåœ¨ä¸‹é¢ç»Ÿä¸€åšï¼Œä½†ä¹Ÿä¸ºäº†ä¿é™©
            memory_stats[opt_id]['neg_queries'] = []
            memory_stats[opt_id]['pos_queries'] = []

    # 3. å…¨å±€æ¸…ç†ï¼šæ¸…ç†æ‰€æœ‰è®°å¿†çš„ Queries (ä¸ºä¸‹ä¸€è½® Evaluation è…¾ç©º)
    #    ä½†ä¿ç•™ æœªä¼˜åŒ–è®°å¿† çš„ alpha/beta (å†å²æˆ˜ç»©)
    cleaned_count = 0
    for mid in memory_stats:
        stats = memory_stats[mid]
        # åªæ¸…ç©º Query åˆ—è¡¨ï¼Œä¿ç•™åˆ†æ•°
        stats['pos_queries'] = []
        stats['neg_queries'] = []
        cleaned_count += 1
            
    print(f" ğŸ—‘ï¸ å·²ç‰©ç†ç§»é™¤ {len(to_delete_ids)} æ¡è¢«åˆ  Stats")
    print(f" ğŸ”„ å·²é‡ç½® {len(all_changed_ids)} æ¡å˜åŠ¨è®°å¿†çš„åˆ†æ•° (Low+High Opt)")
    print(f" âœ¨ å·²æ¸…ç† {cleaned_count} æ¡è®°å¿†çš„ Query ç¼“å­˜")
    
    try:
        with open(stats_optimized_file, 'w', encoding='utf-8') as f:
            json.dump(memory_stats, f, ensure_ascii=False, indent=2)
        print(f"âœ… [BEMR] çŠ¶æ€å·²åŒæ­¥: {stats_optimized_file}")
    except Exception as e:
        print(f"âŒ ä¿å­˜ Stats å¤±è´¥: {e}")

if __name__ == "__main__":
    optimize_memory()