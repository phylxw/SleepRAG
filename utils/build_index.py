from datasets import load_dataset
from huggingface_hub import snapshot_download
from omegaconf import DictConfig, OmegaConf
import os
import json
import tqdm
import bm25s

def build_index(corpus_file: str, index_dir: str):
    """æ„å»º BM25 ç´¢å¼•"""

    print(f"ğŸ”¨ [Index] æ­£åœ¨ä¸º {corpus_file} æ„å»º BM25 ç´¢å¼•...")
    corpus_texts = []
    
    # ä½¿ç”¨ bm25s åº“
    with open(corpus_file, "r", encoding="utf-8") as f:
        for i, line in enumerate(f):
            try:
                item = json.loads(line)
                # ğŸ”¥ [æ ¸å¿ƒä¿®æ”¹] å­—æ®µå…¼å®¹é€»è¾‘
                # ä¼˜å…ˆæ‰¾ 'contents'ï¼Œæ²¡æœ‰å°±æ‰¾ 'question' (MBPP normalized)ï¼Œå†æ²¡æœ‰æ‰¾ 'prompt' æˆ– 'text'
                content = item.get('contents') or item.get('question') or item.get('prompt') or item.get('text')
                
                if content:
                    corpus_texts.append(content)
                else:
                    # å¦‚æœå®åœ¨æ‰¾ä¸åˆ°ï¼Œæ‰“å°è­¦å‘Šä½†ä¸å´©æºƒï¼ˆæˆ–è€…ä½ å¯ä»¥é€‰æ‹©æŠ›å‡ºå¼‚å¸¸ï¼‰
                    print(f"âš ï¸ [Line {i}] è­¦å‘Šï¼šæœªæ‰¾åˆ°æœ‰æ•ˆæ–‡æœ¬å­—æ®µ (contents/question/prompt)ï¼Œå·²è·³è¿‡ã€‚Keys: {list(item.keys())}")
            except json.JSONDecodeError:
                print(f"âš ï¸ [Line {i}] JSON è§£æå¤±è´¥ï¼Œè·³è¿‡ã€‚")
                continue

    if not corpus_texts:
        raise ValueError(f"âŒ ç´¢å¼•æ„å»ºå¤±è´¥ï¼š{corpus_file} ä¸­æ²¡æœ‰æå–åˆ°ä»»ä½•æœ‰æ•ˆæ–‡æœ¬ï¼è¯·æ£€æŸ¥å­—æ®µæ˜ å°„ã€‚")

    print(f"ğŸ“Š æå–åˆ° {len(corpus_texts)} æ¡æ–‡æœ¬ï¼Œå¼€å§‹åˆ†è¯...")
    
    # åé¢ä¿æŒä¸å˜
    corpus_tokens = bm25s.tokenize(corpus_texts)
    retriever_builder = bm25s.BM25()
    retriever_builder.index(corpus_tokens)
    retriever_builder.save(index_dir)
    
    # FlashRAG è¦æ±‚çš„é¢å¤–æ–‡ä»¶
    with open(os.path.join(index_dir, "stopwords.tokenizer.json"), "w") as f:
        json.dump([], f)
    with open(os.path.join(index_dir, "vocab.tokenizer.json"), "w") as f:
        vocab = corpus_tokens.vocab
        # å…¼å®¹æ€§å¤„ç†
        json.dump({"word_to_id": vocab, "stem_to_sid": vocab, "word_to_stem": {k: k for k in vocab}}, f)
    print("âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼")