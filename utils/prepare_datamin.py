from datasets import load_dataset
from omegaconf import DictConfig
import os
import json
from tqdm import tqdm
import random 
# ä¿æŒåŸæœ‰çš„å·¥å…·å¯¼å…¥
from tools.prepare.merge_hmmt import merge_hmmt
from tools.prepare.merge_aime import merge_aime
from tools.prepare.sci_split import prepare_sciknow
from tools.prepare.humaneval_split import prepare_humaneval
from tools.prepare.mbpp_split import prepare_mbpp

def _get_available_column(dataset, candidates, default):
    """è¾…åŠ©å‡½æ•°ï¼šåœ¨æ•°æ®é›†é‡Œè‡ªåŠ¨å¯»æ‰¾å­˜åœ¨çš„åˆ—å"""
    cols = []
    if hasattr(dataset, "column_names"):
        cols = dataset.column_names
    elif hasattr(dataset, "features"):
        cols = dataset.features.keys()
    
    for cand in candidates:
        if cand in cols:
            return cand
    return default

def _format_eval_item(item, q_col, a_col, mode="standard"):
    """
    è¾…åŠ©å‡½æ•°ï¼šç»Ÿä¸€å¤„ç†è¯„ä¼°é›†çš„æ•°æ®æ ¼å¼åŒ–ï¼ˆGPQA/SciKnow/Standardï¼‰
    è¿”å›: (question_text, answer_text)
    """
    q_text = ""
    a_text = ""
    
    if mode == "sciknow":
        # SciKnowEval é€»è¾‘
        question_raw = item.get("question", "")
        choices = item.get("choices", []) 
        answer_raw = item.get("answer", "")
        options_str = ""
        labels = ['A', 'B', 'C', 'D', 'E', 'F']
        if isinstance(choices, list):
            for idx, choice_text in enumerate(choices):
                label = labels[idx] if idx < len(labels) else str(idx)
                options_str += f"\n({label}) {choice_text}"
        else:
            options_str = f"\n{str(choices)}"
        q_text = question_raw + options_str
        a_text = str(answer_raw)

    elif mode == "gpqa":
        # GPQA é€‰æ‹©é¢˜é€»è¾‘
        question_raw = item.get("Question", "")
        correct_ans = item.get("Correct Answer", "")
        # è·å–å¹²æ‰°é¡¹
        options = [correct_ans]
        for k in ["Incorrect Answer 1", "Incorrect Answer 2", "Incorrect Answer 3"]:
            if item.get(k): options.append(item.get(k))
            
        random.shuffle(options)
        labels = ['A', 'B', 'C', 'D']
        try:
            correct_idx = options.index(correct_ans)
            final_ans = labels[correct_idx] 
        except ValueError:
            final_ans = "Error"

        options_str = ""
        for label, content in zip(labels, options):
            options_str += f"\n({label}) {content}"
        
        q_text = question_raw + options_str
        a_text = final_ans 

    else:
        # Standard / Math é€»è¾‘
        q_text = item.get(q_col, "")
        a_text = item.get(a_col, "")
        
    return q_text, a_text

def prepare_data(cfg: DictConfig, corpus_file: str, test_file: str, need_split):
    """
    é‡æ„åçš„æ•°æ®å‡†å¤‡å‡½æ•°ï¼šå®Œå…¨è§£è€¦ Memory å’Œ Eval çš„æ•°æ®æº
    """
    # === 0. ç‰¹æ®Šæ•°æ®é›†å¤„ç† (ä¿æŒä¸å˜) ===
    tag = cfg.experiment.tag
    if tag == "sci": return prepare_sciknow(corpus_file, test_file, cfg, need_split)
    if tag == "humaneval": return prepare_humaneval(corpus_file, test_file, cfg, need_split)
    if tag == "mbpp": return prepare_mbpp(corpus_file, test_file, cfg, need_split)
    if tag == "hmmtex": merge_hmmt(test_file, cfg, need_split); return True
    if tag == "aimeex": merge_aime(test_file, cfg, need_split); return True

    # å­—æ®µæ¢æµ‹é…ç½®
    q_col_cfg = cfg.experiment.field_map.question
    a_col_cfg = cfg.experiment.field_map.answer
    q_candidates = [q_col_cfg, "problem", "question", "input", "content", "Question"]
    a_candidates = [a_col_cfg, "solution", "answer", "ground_truth", "output", "completion", "Correct Answer"]

    # ==========================================
    # Part A: æ„å»ºè®°å¿†åº“ (Corpus) - ä¸¥æ ¼è¯»å– corpus_* é…ç½®
    # ==========================================
    # åªæœ‰å½“æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæˆ–è€…éœ€è¦å¼ºåˆ¶åˆ·æ–°æ—¶æ‰å¤„ç†
    if not os.path.exists(corpus_file) or cfg.parameters.get("force_process_corpus", False):
        c_name = cfg.experiment.get("corpus_dataset_name")
        c_config = cfg.experiment.get("corpus_dataset_config")
        c_split = cfg.experiment.get("corpus_split", "train")

        print(f"\nğŸ“š [Memory] æ­£åœ¨æ„å»ºè®°å¿†åº“: {c_name} | Config: {c_config} | Split: {c_split}")
        
        try:
            ds_corpus = load_dataset(c_name, c_config, split=c_split)
        except Exception as e:
            print(f"âŒ Memory æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
            return False

        # --- 1. é¢˜ç›®ç±»å‹è¿‡æ»¤ (ä¿æŒåŸæœ‰é€»è¾‘) ---
        target_type = cfg.experiment.get("problem_type", "all")
        if target_type and target_type.lower() != "all":
            type_candidates = ["problem_type", "subject", "category", "type"]
            type_col = _get_available_column(ds_corpus, type_candidates, None)
            if type_col:
                print(f"ğŸ” [Filter] è¿‡æ»¤ç±»å‹: '{target_type}' (åˆ—: {type_col})")
                ds_corpus = ds_corpus.filter(
                    lambda x: x[type_col] is not None and target_type.lower() in str(x[type_col]).lower()
                )
            else:
                print(f"âš ï¸ [Warning] æœªæ‰¾åˆ°ç±»å‹åˆ—ï¼Œè·³è¿‡è¿‡æ»¤ã€‚")

        # --- 2. æ€»é‡æ§åˆ¶ ---
        max_limit = cfg.parameters.get("total_num", None)
        if max_limit is not None and len(ds_corpus) > int(max_limit):
            print(f"âœ‚ï¸  [Memory] æˆªå–å‰ {max_limit} æ¡")
            ds_corpus = ds_corpus.select(range(int(max_limit)))

        # --- 3. å†™å…¥ Corpus æ–‡ä»¶ ---
        q_col = _get_available_column(ds_corpus, q_candidates, q_col_cfg)
        a_col = _get_available_column(ds_corpus, a_candidates, a_col_cfg)
        
        # å¦‚æœéœ€è¦ä»corpusé‡Œåˆ‡åˆ†ä¸€éƒ¨åˆ†ç»™éªŒè¯é›†ï¼ˆå…¼å®¹æ—§é€»è¾‘ï¼Œæˆ–é˜²æ­¢valé…ç½®ä¸ºç©ºï¼‰
        # ä½†æ—¢ç„¶ä½ ç°åœ¨å‚æ•°åˆ†å¼€äº†ï¼Œè¿™é‡Œé»˜è®¤å…¨é‡å†™å…¥
        with open(corpus_file, "w", encoding="utf-8") as f:
            for i, item in enumerate(tqdm(ds_corpus, desc="Writing Corpus")):
                q_text = item.get(q_col, "")
                a_text = item.get(a_col, "")
                if q_text:
                    content = f"Question: {q_text}\nAnswer: {a_text}"
                    f.write(json.dumps({"id": str(i), "contents": content}) + "\n")
    else:
        print(f"âœ… [Memory] è®°å¿†åº“å·²å­˜åœ¨: {corpus_file}")


    # ==========================================
    # Part B: å‡†å¤‡è¯„ä¼°é›† (Eval) - æ ¹æ® need_split å†³å®šæ˜¯ç”¨ Val è¿˜æ˜¯ Test é…ç½®
    # ==========================================
    # é€»è¾‘ï¼š
    # å¦‚æœ need_split == True (é€šå¸¸ä»£è¡¨éªŒè¯é˜¶æ®µ)ï¼Œè¯»å– val_* é…ç½®
    # å¦‚æœ need_split == False (é€šå¸¸ä»£è¡¨æµ‹è¯•é˜¶æ®µ)ï¼Œè¯»å– test_* é…ç½®
    # æœ€ç»ˆéƒ½å†™å…¥ test_file (å› ä¸ºå¤–éƒ¨å·¥å…·é€šå¸¸åªè®¤è¿™ä¸ªæ–‡ä»¶è·¯å¾„)
    
    is_val = need_split
    
    if is_val:
        t_name = cfg.experiment.get("val_dataset_name")
        t_config = cfg.experiment.get("val_dataset_config")
        t_split = cfg.experiment.get("val_split", "test") # é»˜è®¤ä»test splitæ‹¿éªŒè¯æ•°æ®
        mode_label = "Validation"
    else:
        t_name = cfg.experiment.get("test_dataset_name")
        t_config = cfg.experiment.get("test_dataset_config")
        t_split = cfg.experiment.get("test_split", "test")
        mode_label = "Test"


    print(f"\nğŸ¯ [{mode_label}] æ­£åœ¨å¤„ç†è¯„ä¼°é›†: {t_name} | Split: {t_split}")
    
    try:
        ds_eval = load_dataset(t_name, t_config, split=t_split)
    except Exception as e:
        print(f"âŒ {mode_label} æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        return False

    # æ¢æµ‹æ¨¡å¼ (GPQA / SciKnow / Standard)
    is_gpqa = "gpqa" in str(t_name).lower()
    is_sciknow = "sci" in str(t_name).lower() or cfg.experiment.tag == "sci"
    
    format_mode = "standard"
    if is_sciknow: format_mode = "sciknow"
    elif is_gpqa: format_mode = "gpqa"
    
    if format_mode == "standard":
        q_col_test = _get_available_column(ds_eval, q_candidates, q_col_cfg)
        a_col_test = _get_available_column(ds_eval, a_candidates, a_col_cfg)
        print(f"   ğŸ‘‰ åˆ—ååŒ¹é…: Q='{q_col_test}', A='{a_col_test}'")
    else:
        print(f"   ğŸ‘‰ æ¨¡å¼æ¿€æ´»: {format_mode.upper()}")
        q_col_test, a_col_test = None, None # ç‰¹æ®Šæ¨¡å¼ä¸éœ€è¦åˆ—å

    # --- æˆªå–ä¸å†™å…¥ ---
    start_idx = int(cfg.parameters.get("start_index", 0) or 0)
    debug_num = cfg.parameters.get("debug_num")
    
    total_len = len(ds_eval)
    if debug_num:
        limit = int(debug_num)
        end_idx = min(start_idx + limit, total_len)
    else:
        end_idx = total_len
    
    # é˜²æ­¢è¶Šç•Œ
    if start_idx >= total_len:
         print(f"âš ï¸ [Warning] start_index ({start_idx}) è¶…å‡ºæ•°æ®èŒƒå›´ ({total_len})")
         selected_data = []
    else:
        selected_data = ds_eval.select(range(start_idx, end_idx))

    print(f"ğŸ“Š å†™å…¥ {mode_label} æ–‡ä»¶: {len(selected_data)} æ¡ (Range: {start_idx}-{end_idx})")

    with open(test_file, "w", encoding="utf-8") as f:
        for i, item in enumerate(selected_data):
            real_id = start_idx + i
            
            # ä½¿ç”¨ç»Ÿä¸€çš„æ ¼å¼åŒ–å‡½æ•°
            q_text, a_text = _format_eval_item(item, q_col_test, a_col_test, mode=format_mode)

            f.write(json.dumps({
                "id": str(real_id),
                "question": q_text,
                "golden_answers": [str(a_text)]
            }) + "\n")

    return True