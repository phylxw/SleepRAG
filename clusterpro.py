import os
import json
import re
import time
import numpy as np
import torch
import matplotlib.pyplot as plt 
from typing import List, Dict
from sentence_transformers import SentenceTransformer
from sklearn.cluster import AgglomerativeClustering, KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.manifold import TSNE 
from sklearn.decomposition import PCA 
from transformers import AutoModelForCausalLM, AutoTokenizer
import umap

# Hydra
import hydra
from omegaconf import DictConfig

# ================= å…¨å±€å˜é‡ (ä¿æŒåŸé€»è¾‘) =================
GLOBAL_MODEL = None
GLOBAL_TOKENIZER = None
GLOBAL_SGLANG_CLIENT = None

# =============== 0. å·¥å…·å‡½æ•° ===============
def clean_special_chars(text: str) -> str:
    if not isinstance(text, str): return text
    return text.replace('\u2028', ' ').replace('\u2029', ' ')

def normalize_text(x: str) -> str:
    x = str(x).lower()
    x = re.sub(r"\d+(\.\d+)?", " <num> ", x) 
    x = re.sub(r"\s+", " ", x).strip()
    return x

def import_torch_and_check_gpu():
    try: return torch.cuda.is_available()
    except: return False

# =============== 1. LLM åˆå§‹åŒ–ä¸è°ƒç”¨ (ä¿æŒä¸å˜ï¼Œå·²é€‚é… SGLang) ===============
def init_llm(cfg: DictConfig):
    global GLOBAL_MODEL, GLOBAL_TOKENIZER, GLOBAL_SGLANG_CLIENT
    
    model_source = cfg.model.source
    
    if model_source == "gemini":
        import google.generativeai as genai
        api_key = os.environ.get("GEMINI_API_KEY")
        if api_key:
            genai.configure(api_key=api_key)
            print(f"ğŸ¤– [Init] Gemini API ({cfg.model.gemini_name}) å·²é…ç½®")
        else:
            print("âš ï¸ [Init] æœªæ£€æµ‹åˆ° GEMINI_API_KEYï¼ŒGemini æ¨¡å¼å¯èƒ½æ— æ³•å·¥ä½œ")
            
    elif model_source == "huggingface":
        hf_name = cfg.model.hf_name
        print(f"ğŸ“¥ [Init] æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹: {hf_name} ...")
        try:
            GLOBAL_TOKENIZER = AutoTokenizer.from_pretrained(hf_name, trust_remote_code=True)
            GLOBAL_MODEL = AutoModelForCausalLM.from_pretrained(
                hf_name,
                device_map="auto",
                torch_dtype="auto",
                trust_remote_code=True
            ).eval()
            print("âœ… [Init] æœ¬åœ°æ¨¡å‹åŠ è½½å®Œæˆï¼")
        except Exception as e:
            print(f"âŒ [Init] æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {e}")

    elif model_source == "sglang":
        try:
            from openai import OpenAI
            api_url = cfg.model.get("sglang_api_url", "http://127.0.0.1:30000/v1")
            # å…¼å®¹ yaml é‡Œé…ç½® sglang_api_key çš„æƒ…å†µï¼Œè™½ç„¶é»˜è®¤æ˜¯ EMPTY
            api_key = "EMPTY" 
            
            GLOBAL_SGLANG_CLIENT = OpenAI(base_url=api_url, api_key=api_key)
            print(f"âœ… [Init] SGLang Client å·²è¿æ¥è‡³ {api_url}")
        except ImportError:
            print("âŒ [Init] ç¼ºå°‘ openai åº“ï¼Œè¯·è¿è¡Œ `pip install openai`")

def call_llm(prompt: str, cfg: DictConfig) -> str:
    model_source = cfg.model.source
    
    if model_source == "gemini":
        import google.generativeai as genai
        if not os.environ.get("GEMINI_API_KEY"): return "Skipped (No Key)"
        model = genai.GenerativeModel(cfg.model.gemini_name) 
        try:
            print("  ğŸ¤– [Gemini] æ­£åœ¨æ€è€ƒ...", end="", flush=True)
            resp = model.generate_content(prompt)
            print(" å®Œæˆ!")
            return clean_special_chars(resp.text.strip())
        except Exception as e:
            print(f"\nâŒ [Gemini Error]: {e}")
            time.sleep(1)
            return "Unknown Topic"

    elif model_source == "huggingface":
        if GLOBAL_MODEL is None: return "Skipped (Model Not Loaded)"
        try:
            print("  ğŸš€ [Local] æ­£åœ¨æ¨ç†...", end="", flush=True)
            messages = [{"role": "user", "content": prompt}]
            text = GLOBAL_TOKENIZER.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            model_inputs = GLOBAL_TOKENIZER([text], return_tensors="pt").to(GLOBAL_MODEL.device)
            with torch.no_grad():
                generated_ids = GLOBAL_MODEL.generate(model_inputs.input_ids, max_new_tokens=50, do_sample=False)
            generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]
            response = GLOBAL_TOKENIZER.batch_decode(generated_ids, skip_special_tokens=True)[0]
            print(" å®Œæˆ!")
            return clean_special_chars(response.strip())
        except Exception as e:
            print(f"\nâŒ [Local Error]: {e}")
            return "Unknown Topic"

    elif model_source == "sglang":
        if GLOBAL_SGLANG_CLIENT is None: return "Skipped (Client Not Initialized)"
        model_name = cfg.model.get("sglang_model_name", "Qwen/Qwen3-4B-Instruct-2507")
        try:
            print("  ğŸš€ [SGLang] æ­£åœ¨æ¨ç†...", end="", flush=True)
            resp = GLOBAL_SGLANG_CLIENT.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
                max_tokens=50
            )
            content = resp.choices[0].message.content
            print(" å®Œæˆ!")
            return clean_special_chars(content.strip())
        except Exception as e:
            print(f"\nâŒ [SGLang Error]: {e}")
            return "Unknown Topic"

    return "Unknown Config"

# =============== 2. åŸºç¡€ IO (å¢å¼ºå¥å£®æ€§) ===============
def load_questions(jsonl_path: str):
    print(f"ğŸ“¥ æ­£åœ¨åŠ è½½æ–‡ä»¶: {jsonl_path}...")
    if not os.path.exists(jsonl_path):
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {jsonl_path}")
        return [], [], []

    ids, questions, raw_contents = [], [], []
    
    with open(jsonl_path, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip(): continue
            try:
                obj = json.loads(line)
            except json.JSONDecodeError: continue
            
            # ä» pre.py çš„è¾“å‡ºä¸­è·å–å†…å®¹
            content = obj.get("contents", "")
            raw_contents.append(content) 

            # ğŸ”¥ å¢å¼ºçš„åˆ†éš”é€»è¾‘ï¼šå…¼å®¹ Question/Problem/Input ç­‰ä¸åŒå‰ç¼€
            # å¦‚æœæ˜¯ pre.py ç”Ÿæˆçš„ï¼Œæ ¼å¼å›ºå®šæ˜¯ "Question: ... \nAnswer: ..."
            if "Question:" in content:
                # å°è¯•åˆ†ç¦» Question å’Œ Answerï¼Œåªèšç±» Question éƒ¨åˆ†
                q_part = content.split("Answer:")[0].replace("Question:", "").strip()
                # å°è¯•åˆ†ç¦» Question å’Œ Answerï¼Œåªèšç±» Answer éƒ¨åˆ†
                # q_part = content.split("Question:")[0].replace("Answer:", "").strip()
                if not q_part and content: q_part = content 
            else:
                q_part = content


            mid = obj.get("memory_id", obj.get("id"))
            ids.append(str(mid))
            questions.append(clean_special_chars(q_part))
            
    print(f"âœ… åŠ è½½å®Œæˆï¼Œå…± {len(questions)} æ¡æ•°æ®")
    return ids, questions, raw_contents

# =============== 3. ä¸»æµç¨‹ï¼šembedding + è‡ªåŠ¨èšç±» ===============
def build_embeddings(questions: List[str], model_name: str, device_cfg: str = "cuda") -> np.ndarray:
    print(f"ğŸš€ æ­£åœ¨è®¡ç®— Embeddings ({model_name})...")
    
    # ä¼˜å…ˆä½¿ç”¨ config é‡Œçš„ deviceï¼Œå¦‚æœä¸å¯ç”¨åˆ™è‡ªåŠ¨æ£€æµ‹
    device = device_cfg if (device_cfg == "cuda" and torch.cuda.is_available()) else "cpu"
    print(f"   >>> ä½¿ç”¨è®¾å¤‡: {device}")
    
    model = SentenceTransformer(model_name, device=device)
    q_norm = [normalize_text(q) for q in questions]
    
    # å¢å¤§ä¸€ç‚¹ batch_size æé«˜é€Ÿåº¦
    emb = model.encode(q_norm, batch_size=64, show_progress_bar=True, normalize_embeddings=True)
    return np.asarray(emb)

def preprocess_embeddings_pca(embeddings: np.ndarray, n_components: int) -> np.ndarray:
    print(f"ğŸ§¹ æ­£åœ¨æ‰§è¡Œ PCA é¢„å¤„ç† (é™ç»´: {embeddings.shape[1]} -> {n_components})...")
    if embeddings.shape[0] < n_components:
        print(f"âš ï¸ æ ·æœ¬æ•° ({embeddings.shape[0]}) å°‘äºç›®æ ‡ç»´åº¦ï¼Œè·³è¿‡ PCAã€‚")
        return embeddings
    
    pca = PCA(n_components=n_components)
    reduced = pca.fit_transform(embeddings)
    
    explained_variance = np.sum(pca.explained_variance_ratio_)
    print(f"   >>> ä¿ç•™æ–¹å·®æ¯”ä¾‹: {explained_variance:.2%}")
    return reduced

def cluster_questions_auto(embeddings: np.ndarray, cfg: DictConfig) -> np.ndarray:
    method = cfg.cluster.method
    
    if method == "kmeans":
        n_clusters = cfg.cluster.kmeans_n_clusters
        print(f"ğŸ¤– æ­£åœ¨æ‰§è¡Œ K-Means èšç±» (N_Clusters={n_clusters})...")
        model = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')
        labels = model.fit_predict(embeddings)
    elif method == "agglomerative":
        threshold = cfg.cluster.distance_threshold
        print(f"ğŸ¤– æ­£åœ¨æ‰§è¡Œå±‚æ¬¡èšç±» (Threshold={threshold})...")
        model = AgglomerativeClustering(
            n_clusters=None, 
            distance_threshold=threshold,
            metric='euclidean', 
            linkage='ward'
        )
        labels = model.fit_predict(embeddings)
        print(f"âœ¨ å±‚æ¬¡èšç±»å‘ç° {len(set(labels))} ä¸ªç±»åˆ«ã€‚")
    else:
        raise ValueError(f"æœªçŸ¥çš„èšç±»æ–¹æ³•: {method}")
        
    return labels

# =============== 4. ç»Ÿè®¡ç»˜å›¾ & å…³é”®è¯ & é™ç»´å¯è§†åŒ– ===============

def plot_cluster_stats(labels: np.ndarray, save_path: str, method_name: str):
    print(f"\nğŸ“Š æ­£åœ¨ç”Ÿæˆç»Ÿè®¡å›¾è¡¨...")
    unique_labels, counts = np.unique(labels, return_counts=True)
    
    # åªç»˜åˆ¶æ•°é‡ > 1 çš„èšç±»
    valid_mask = counts > 1
    valid_labels = unique_labels[valid_mask]
    valid_counts = counts[valid_mask]
    
    if len(valid_counts) == 0:
        print("   âš ï¸ æ²¡æœ‰åŒ…å«å¤šä¸ªé—®é¢˜çš„èšç±»ï¼Œè·³è¿‡ç»˜å›¾ã€‚")
        return

    # æ’åº
    sorted_indices = np.argsort(valid_counts)[::-1]
    sorted_plot_labels = valid_labels[sorted_indices]
    sorted_plot_counts = valid_counts[sorted_indices]
    
    plt.figure(figsize=(12, 6))
    x_ticks = [str(lbl) for lbl in sorted_plot_labels]
    # é™åˆ¶å±•ç¤ºæ•°é‡ï¼Œé˜²æ­¢å¤ªå¯†
    if len(x_ticks) > 50:
        x_ticks = x_ticks[:50]
        sorted_plot_counts = sorted_plot_counts[:50]
        
    plt.bar(x_ticks, sorted_plot_counts, color='steelblue', edgecolor='black', alpha=0.8)
    plt.xlabel('Cluster ID')
    plt.ylabel('Count')
    plt.title(f'Top Cluster Size Distribution ({method_name})')
    plt.xticks(rotation=90 if len(x_ticks) > 20 else 0, fontsize=8)
    plt.grid(axis='y', linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig(save_path)
    print(f"ğŸ–¼ï¸ å›¾è¡¨å·²ä¿å­˜è‡³: {save_path}")

def plot_dimensionality_reduction(embeddings: np.ndarray, labels: np.ndarray, cfg: DictConfig, save_path: str):
    method = cfg.cluster.vis_method
    print(f"\nğŸ¨ æ­£åœ¨ç”Ÿæˆ {method.upper()} å¯è§†åŒ–...")
    
    if embeddings.shape[0] < 5: return

    reducer = None
    if method == "tsne":
        perp = min(cfg.cluster.tsne_perplexity, embeddings.shape[0] - 1)
        reducer = TSNE(n_components=2, perplexity=perp, random_state=42, init='pca', learning_rate='auto')
    elif method == "pca":
        reducer = PCA(n_components=2)
    elif method == "umap":
        if umap is None:
            print("âŒ ç¼ºå°‘ umap-learnï¼Œå›é€€åˆ° t-SNE")
            cfg.cluster.vis_method = "tsne"
            return plot_dimensionality_reduction(embeddings, labels, cfg, save_path)
        reducer = umap.UMAP(n_components=2, random_state=42)
    else:
        print(f"âŒ æœªçŸ¥å¯è§†åŒ–æ–¹æ³•: {method}")
        return

    reduced_emb = reducer.fit_transform(embeddings)

    plt.figure(figsize=(12, 10))
    # ä½¿ç”¨ jet æˆ– tab20 è¿™ç§é¢œè‰²åŒºåˆ†åº¦é«˜çš„ colormap
    plt.scatter(reduced_emb[:, 0], reduced_emb[:, 1], c=labels, cmap='tab20', s=10, alpha=0.6)
    plt.colorbar(label='Cluster ID')
    plt.title(f'{method.upper()} Visualization')
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    print(f"ğŸ–¼ï¸ å¯è§†åŒ–ä¿å­˜è‡³: {save_path}")

def tfidf_keywords_per_cluster(questions, cluster_labels, max_features=5000, top_k=10):
    print("ğŸ” æå–å…³é”®è¯...")
    q_norm = [normalize_text(q) for q in questions]
    try:
        vectorizer = TfidfVectorizer(max_df=0.9, min_df=2, max_features=max_features, stop_words="english")
        X = vectorizer.fit_transform(q_norm)
        vocab = np.array(vectorizer.get_feature_names_out())
        
        cluster_keywords = {}
        for cid in np.unique(cluster_labels):
            idx = np.where(cluster_labels == cid)[0]
            if len(idx) < 2: continue # å¤ªå°‘çš„å°±ä¸æå–äº†
            tfidf_mean = np.asarray(X[idx].mean(axis=0)).ravel()
            top_idx = tfidf_mean.argsort()[::-1][:top_k]
            cluster_keywords[cid] = vocab[top_idx].tolist()
        return cluster_keywords
    except ValueError:
        return {}

def llm_label_cluster(cid, questions, cluster_labels, cluster_keywords, cfg: DictConfig, max_examples=5):
    idx = np.where(cluster_labels == cid)[0]
    # éšæœºé‡‡æ ·å‡ ä¸ªä½œä¸ºç¤ºä¾‹
    examples_idx = np.random.choice(idx, min(len(idx), max_examples), replace=False)
    examples = [questions[i] for i in examples_idx]
    kw = ", ".join(cluster_keywords.get(cid, []))

    prompt = f"""You are a Math Education Expert.
I have grouped similar math problems together.
Keywords: [{kw}]
Examples:
{chr(10).join(f"- {q}" for q in examples)}

Task: Provide a **very short category name** (3-6 words) for this problem type.
Output ONLY the category name. Do not explain.
"""
    return call_llm(prompt, cfg).replace('"', "").strip()

# =============== Main (Hydra Integrated) ===============

@hydra.main(version_base=None, config_path="conf", config_name="config")
def cluster(cfg: DictConfig):
    
    # 0. åˆå§‹åŒ–
    init_llm(cfg)

    # 1. è·¯å¾„æ˜ å°„ (ä» yaml è¯»å–)
    # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šè¾“å…¥æ–‡ä»¶ç°åœ¨æ˜¯ memory_freq (ç”± pre.py ç”Ÿæˆ)
    input_file = cfg.paths.freq_file  
    output_file = cfg.paths.cluster_output
    summary_file = cfg.paths.cluster_summary
    plot_file = cfg.paths.cluster_plot
    vis_plot_file = cfg.paths.cluster_vis
    
    # è‡ªåŠ¨åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    os.makedirs(os.path.dirname(plot_file), exist_ok=True)

    ids, questions, raw_contents = load_questions(input_file)
    if not ids: return

    # 2. Embedding
    embeddings = build_embeddings(questions, cfg.model.embedding_name, cfg.model.device)
    
    # 2.1 PCA é¢„å¤„ç†
    if cfg.cluster.enable_pca_preprocess:
        embeddings = preprocess_embeddings_pca(embeddings, n_components=cfg.cluster.pca_preprocess_dims)

    # 3. èšç±»
    labels = cluster_questions_auto(embeddings, cfg)

    # 4. ç»Ÿè®¡ä¸å¯è§†åŒ–
    plot_cluster_stats(labels, save_path=plot_file, method_name=cfg.cluster.method)
    plot_dimensionality_reduction(embeddings, labels, cfg, save_path=vis_plot_file)

    # 5. åˆ†æå…³é”®è¯ä¸å‘½å
    keywords_map = tfidf_keywords_per_cluster(questions, labels)
    
    print("\n" + "="*20 + " èšç±»ç»“æœåˆ†æ " + "="*20)
    unique, counts = np.unique(labels, return_counts=True)
    sorted_clusters = sorted(zip(unique, counts), key=lambda x: x[1], reverse=True)
    
    cluster_labels_text = {}
    print(f"ğŸ“Š æ€»å…±å‘ç° {len(sorted_clusters)} ä¸ªèšç±»ã€‚")
    print("   (æ­£åœ¨ä¸º Top 10 çƒ­é—¨èšç±»ç”Ÿæˆ LLM å‘½å...)\n")

    for cid, count in sorted_clusters[:10]: # åªç»™å‰10ä¸ªæœ€å¤§çš„å‘½åï¼Œçœ token
        label_text = llm_label_cluster(cid, questions, labels, keywords_map, cfg)
        cluster_labels_text[cid] = label_text
        print(f"   ğŸ·ï¸ Cluster {cid} ({count} é¢˜): {label_text}")
        if cfg.model.source == "gemini": time.sleep(1)

    # 6. ä¿å­˜è¯¦ç»†ç»“æœ (æ¯æ¡æ•°æ®éƒ½å¸¦ cluster_id)
    print(f"\nğŸ’¾ ä¿å­˜è¯¦ç»†ç»“æœåˆ° {output_file}...")
    with open(output_file, "w", encoding="utf-8") as f:
        for qid, q, raw, cid in zip(ids, questions, raw_contents, labels):
            obj = {
                "id": qid,
                "contents": raw,  
                "cluster_id": int(cid),
                "cluster_label": cluster_labels_text.get(int(cid), f"Cluster {cid}"),
                "cluster_keywords": keywords_map.get(int(cid), [])
            }
            f.write(json.dumps(obj, ensure_ascii=False) + "\n")
            
    # 7. ä¿å­˜æ‘˜è¦ç»“æœ (Cluster ä¸ºä¸»é”®)
    print(f"ğŸ’¾ ä¿å­˜èšç±»æ‘˜è¦åˆ° {summary_file}...")
    cluster_aggregation = {}
    for qid, cid in zip(ids, labels):
        cid_int = int(cid)
        if cid_int not in cluster_aggregation:
            cluster_aggregation[cid_int] = {
                "cluster_id": cid_int,
                "cluster_label": cluster_labels_text.get(cid_int, f"Cluster {cid_int}"),
                "count": 0,
                "memory_ids": []
            }
        cluster_aggregation[cid_int]["memory_ids"].append(qid)
        cluster_aggregation[cid_int]["count"] += 1
    
    with open(summary_file, "w", encoding="utf-8") as f:
        for cid in sorted(cluster_aggregation.keys(), key=lambda k: cluster_aggregation[k]['count'], reverse=True):
            f.write(json.dumps(cluster_aggregation[cid], ensure_ascii=False) + "\n")
            
    print("âœ… å…¨éƒ¨å®Œæˆï¼")

if __name__ == "__main__":
    cluster()