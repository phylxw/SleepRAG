import os
import json
import re
import time
import torch
import bm25s
import logging
import ast
import collections
import matplotlib.pyplot as plt
from tqdm import tqdm
from datasets import load_dataset
from huggingface_hub import snapshot_download

# Hydra & OmegaConf
import hydra
from omegaconf import DictConfig, OmegaConf

# FlashRAG
from flashrag.config import Config
from flashrag.pipeline import SequentialPipeline
from flashrag.utils import get_retriever, get_generator, Dataset
from flashrag.prompt import PromptTemplate

# å±è”½ transformers çš„å†—ä½™è­¦å‘Š
import transformers
transformers.logging.set_verbosity_error()

# ==========================================
# 1. å·¥å…·ç±»ä¸ç”Ÿæˆå™¨ (ä¿æŒåŸæ ·é€»è¾‘)
# ==========================================

class GeminiGenerator:
    def __init__(self, model_name, api_key):
        import google.generativeai as genai
        if not api_key:
            raise ValueError("âŒ æœªæ£€æµ‹åˆ° API Keyï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ GEMINI_API_KEY")
        
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model_name)
        print(f"ğŸ¤– Gemini Generator ({model_name}) å·²åŠ è½½")
        self.max_input_len = 30000 

    def generate(self, input_list, **kwargs):
        responses = []
        for prompt in input_list:
            try:
                if isinstance(prompt, list): prompt = " ".join(prompt)
                clean_prompt = str(prompt)
                result = self.model.generate_content(clean_prompt)
                if result.parts:
                    responses.append(result.text)
                else:
                    responses.append("Error: Empty Response (Safety Block)")
                time.sleep(2) 
            except Exception as e:
                print(f"âš ï¸ Gemini API Error: {e}")
                time.sleep(5)
                responses.append("Error")
        return responses


from typing import List
from openai import OpenAI

class SGLangGenerator:
    """ä¸€ä¸ªæœ€å°å®ç°çš„ç”Ÿæˆå™¨ï¼Œé€‚é… FlashRAG çš„ generator.generate(prompts) æ¥å£ã€‚"""
    def __init__(
        self,
        base_url: str,
        model_name: str,
        max_new_tokens: int = 1024,
        batch_size: int = 8,
        temperature: float = 0.0,
    ):
        self.client = OpenAI(
            api_key=os.getenv("SGLANG_API_KEY", "EMPTY"),
            base_url=base_url.rstrip("/"),
        )
        self.model = model_name
        self.max_new_tokens = max_new_tokens
        self.batch_size = batch_size
        self.temperature = temperature
        self.max_input_len = 4096

    def generate(self, prompts: List[str]) -> List[str]:
        outputs: List[str] = []
        for i in range(0, len(prompts), self.batch_size):
            batch = prompts[i : i + self.batch_size]
            for p in batch:
                resp = self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": p}],
                    temperature=self.temperature,
                    max_tokens=self.max_new_tokens,
                )
                outputs.append(resp.choices[0].message.content)
        return outputs

# ==========================================
# 2. è¯„ä¼°å·¥å…· (Math Logic)
# ==========================================

def extract_math_answer(text):
    if not text: return None
    text = str(text)
    idx = text.rfind("\\boxed{")
    if idx != -1:
        content_start = idx + 7 
        balance = 0
        for i in range(content_start, len(text)):
            char = text[i]
            if char == '{':
                balance += 1
            elif char == '}':
                if balance == 0:
                    return text[content_start:i]
                balance -= 1
    
    lines = text.strip().split('\n')
    if lines:
        last_line = lines[-1].strip()
        last_line = re.sub(r'^(The )?Answer( is)?:?', '', last_line, flags=re.IGNORECASE).strip()
        if len(last_line) < 50: 
            return last_line
    return None

def normalize_latex(s):
    if not s: return ""
    s = str(s)
    s = "".join(s.split())
    s = s.replace("\\dfrac", "\\frac")
    s = s.replace("\\text", "")
    return s.strip()

def evaluate_results(results, experiment_name, result_log_file):
    correct = 0
    total = len(results)
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(result_log_file), exist_ok=True)

    with open(result_log_file, "a", encoding="utf-8") as f:
        header = f"\n{'='*20} {experiment_name} {'='*20}\n"
        print(header.strip())
        f.write(header)
        
        for i, item in enumerate(results):
            # å…¼å®¹ FlashRAG Dataset å¯¹è±¡å’Œ dict
            pred = item.pred if hasattr(item, 'pred') else item['pred']
            gold_raw = item.golden_answers[0] if hasattr(item, 'golden_answers') else item['golden_answers'][0]
            question = item.question if hasattr(item, 'question') else item['question']

            gold_val = extract_math_answer(gold_raw)
            if gold_val is None: gold_val = str(gold_raw).strip()

            pred_val = extract_math_answer(pred)
            is_right = False
            
            if gold_val and pred_val:
                norm_gold = normalize_latex(gold_val)
                norm_pred = normalize_latex(pred_val)
                if norm_gold == norm_pred:
                    is_right = True

            if is_right: correct += 1

            log_entry = (
                f"\n[ID]: {i}\n"
                f"[Question]: {str(question)[:100]}...\n"
                f"[Gold Raw]: ... => [Extracted]: {gold_val}\n"
                f"[Pred Raw]: ...{str(pred)[-50:].replace(chr(10), ' ')} => [Extracted]: {pred_val}\n"
                f"[Result]: {'âœ… Correct' if is_right else 'âŒ Wrong'}\n"
                f"{'-'*30}\n"
            )
            f.write(log_entry)
            if i < 5: print(log_entry.strip()) # å‡å°‘ä¸€ç‚¹æ§åˆ¶å°è¾“å‡º

        acc = correct / total * 100
        summary = (
            f"\nğŸ“Š ç»Ÿè®¡ ({experiment_name}):\n"
            f"Total: {total}, Correct: {correct}, Accuracy: {acc:.2f}%\n"
            f"{'='*50}\n"
        )
        print(summary)
        f.write(summary)
    return acc

# ==========================================
# 3. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° (Hydra é€‚é…)
# ==========================================

def prepare_data(cfg: DictConfig, corpus_file: str, test_file: str):
    """å‡†å¤‡æ•°æ®ï¼šä¸‹è½½ã€åˆ‡åˆ†ã€ç”Ÿæˆ corpus.jsonl å’Œ test.jsonl"""
    dataset_name = cfg.experiment.dataset_name
    dataset_config = cfg.experiment.dataset_config
    
    print(f"ğŸ“¥ [Step 1] æ­£åœ¨åŠ è½½æ•°æ®é›†: {dataset_name} (Config: {dataset_config})...")
    try:
        if dataset_config:
            dataset = load_dataset(dataset_name, dataset_config)
        else:
            dataset = load_dataset(dataset_name)
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return False

    q_col = cfg.experiment.field_map.question
    a_col = cfg.experiment.field_map.answer
    
    split_train = "train"
    split_test = "test"

    # --- A. æ„å»ºè®°å¿†åº“ (Train) ---
    if not os.path.exists(corpus_file):
        print(f"ğŸ”¨ [Memory] æ­£åœ¨å°† {split_train} é›†è½¬æ¢ä¸ºè®°å¿†åº“: {corpus_file}...")
        if split_train not in dataset:
            print(f"âš ï¸ è­¦å‘Š: æ•°æ®é›†æ²¡æœ‰ {split_train} åˆ’åˆ†ï¼")
            return False

        with open(corpus_file, "w", encoding="utf-8") as f:
            for i, item in enumerate(tqdm(dataset[split_train])):
                q_text = item.get(q_col, "")
                a_text = item.get(a_col, "")
                # æ„å»ºæ£€ç´¢å†…å®¹
                content = f"Question: {q_text}\nAnswer: {a_text}"
                f.write(json.dumps({"id": str(i), "contents": content}) + "\n")
    else:
        print(f"âœ… [Memory] æ£€æµ‹åˆ°ç°æœ‰è®°å¿†åº“: {corpus_file}ï¼Œè·³è¿‡æ„å»ºã€‚")
    
    # --- B. å‡†å¤‡æµ‹è¯•é›† (Test) ---
    debug_num = cfg.experiment.debug_num
    print(f"ğŸ”¨ [Test] æ­£åœ¨æå–æµ‹è¯•é›† (æ ·æœ¬æ•°: {debug_num if debug_num else 'ALL'})...")
    
    with open(test_file, "w", encoding="utf-8") as f:
        if split_test not in dataset:
             print(f"âŒ é”™è¯¯: æ•°æ®é›†æ²¡æœ‰ {split_test} åˆ’åˆ†ï¼")
             return False
             
        test_data = dataset[split_test]
        if debug_num:
            limit = min(int(debug_num), len(test_data))
            test_data = test_data.select(range(limit))
            
        for i, item in enumerate(test_data):
            q_text = item.get(q_col, "")
            raw_ans = item.get(a_col, "")
            
            f.write(json.dumps({
                "id": str(i),
                "question": q_text,
                "golden_answers": [str(raw_ans)]
            }) + "\n")
    return True

def build_index(corpus_file: str, index_dir: str):
    """æ„å»º BM25 ç´¢å¼•"""
    if os.path.exists(index_dir) and os.path.exists(os.path.join(index_dir, "vocab.tokenizer.json")):
        print(f"âœ… [Index] ç´¢å¼•å·²å­˜åœ¨: {index_dir}ï¼Œè·³è¿‡æ„å»ºã€‚")
        return

    print(f"ğŸ”¨ [Index] æ­£åœ¨ä¸º {corpus_file} æ„å»º BM25 ç´¢å¼•...")
    corpus_texts = []
    # ä½¿ç”¨ bm25s åº“
    with open(corpus_file, "r", encoding="utf-8") as f:
        for line in f:
            corpus_texts.append(json.loads(line)['contents'])
    
    corpus_tokens = bm25s.tokenize(corpus_texts)
    retriever_builder = bm25s.BM25()
    retriever_builder.index(corpus_tokens)
    retriever_builder.save(index_dir)
    
    # FlashRAG è¦æ±‚çš„é¢å¤–æ–‡ä»¶
    with open(os.path.join(index_dir, "stopwords.tokenizer.json"), "w") as f:
        json.dump([], f)
    with open(os.path.join(index_dir, "vocab.tokenizer.json"), "w") as f:
        vocab = corpus_tokens.vocab
        # å…¼å®¹æ€§å¤„ç†
        json.dump({"word_to_id": vocab, "stem_to_sid": vocab, "word_to_stem": {k: k for k in vocab}}, f)
    print("âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼")

def analyze_memory_usage(rag_results, cfg: DictConfig, corpus_file: str, vis_image_file: str):
    """è®°å¿†çƒ­åº¦ç»Ÿè®¡ä¸å¯¼å‡º"""
    # è¿™é‡Œçš„ freq_file ä» config ä¸­è¯»å–
    freq_file = cfg.paths.freq_file
    
    print("\nğŸ” [Analysis] æ­£åœ¨è¿›è¡Œå…¨é‡è®°å¿†çƒ­åº¦ç»Ÿè®¡...")
    
    all_memory_ids = set()
    id_to_content = {} 

    try:
        with open(corpus_file, "r", encoding="utf-8") as f:
            for line in f:
                item = json.loads(line)
                mid = str(item['id'])
                all_memory_ids.add(mid)
                id_to_content[mid] = item.get("contents", "")
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è¯»å–è®°å¿†åº“æ–‡ä»¶ {corpus_file}ï¼Œé”™è¯¯: {e}")
    
    memory_counter = collections.Counter({mid: 0 for mid in all_memory_ids})
    
    # ç»Ÿè®¡å‘½ä¸­
    for item in rag_results:
        retrieved_docs = getattr(item, 'retrieval_result', [])
        for doc in retrieved_docs:
            if isinstance(doc, dict):
                doc_id = str(doc.get('id'))
            else:
                doc_id = str(getattr(doc, 'id', None))
            if doc_id:
                memory_counter[doc_id] += 1

    # æ’åº
    sorted_memories = sorted(memory_counter.items(), key=lambda x: (-x[1], x[0]))
    
    total = len(sorted_memories)
    used = sum(1 for _, v in sorted_memories if v > 0)
    print(f"ğŸ“Š è®°å¿†åº“æ€»é‡: {total} | æ¿€æ´»: {used} | æœªæ¿€æ´»: {total - used}")

    # å¯¼å‡º jsonl (ä½¿ç”¨ config ä¸­å®šä¹‰çš„è·¯å¾„)
    try:
        print(f"ğŸ’¾ [Save] æ­£åœ¨å¯¼å‡ºè®°å¿†è°ƒç”¨é¢‘æ¬¡æ’åºç»“æœåˆ°: {freq_file}")
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(freq_file), exist_ok=True)
        
        with open(freq_file, "w", encoding="utf-8") as f:
            for rank, (mid, freq) in enumerate(sorted_memories, start=1):
                record = {
                    "rank": rank,
                    "memory_id": mid,
                    "freq": int(freq),
                    "contents": id_to_content.get(mid, "")
                }
                f.write(json.dumps(record, ensure_ascii=False) + "\n")
        print("âœ… è°ƒç”¨é¢‘æ¬¡ jsonl å¯¼å‡ºå®Œæˆï¼")
    except Exception as e:
        print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")

    # å¯è§†åŒ– (å¦‚æœ config å¼€å¯)
    if cfg.experiment.visualize_memory:
        print(f"ğŸ¨ [Visual] æ­£åœ¨ç”Ÿæˆé¢‘æ¬¡åˆ†å¸ƒå›¾: {vis_image_file}")
        try:
            ids = [m[0] for m in sorted_memories]
            counts = [m[1] for m in sorted_memories]
            
            display_limit = 30
            if len(ids) > display_limit * 2:
                plot_ids = ids[:display_limit] + ["..."] + ids[-display_limit:]
                plot_counts = counts[:display_limit] + [0] + counts[-display_limit:]
                colors = ['skyblue'] * display_limit + ['white'] + ['salmon'] * display_limit
                edge_colors = ['navy'] * display_limit + ['white'] + ['darkred'] * display_limit
            else:
                plot_ids = ids
                plot_counts = counts
                colors = 'skyblue'
                edge_colors = 'navy'

            plt.figure(figsize=(15, 6))
            bars = plt.bar(plot_ids, plot_counts, color=colors, edgecolor=edge_colors)
            plt.title(f'Memory Usage Distribution', fontsize=14)
            plt.xticks(rotation=90, fontsize=8) 
            
            # æ˜¾ç¤ºæ•°å€¼
            for i, bar in enumerate(bars):
                height = bar.get_height()
                if plot_ids[i] != "...": 
                    plt.text(bar.get_x() + bar.get_width()/2., height, f'{int(height)}',
                             ha='center', va='bottom', fontsize=8)
            
            plt.tight_layout()
            plt.savefig(vis_image_file, dpi=300)
            print("âœ… å›¾ç‰‡ä¿å­˜æˆåŠŸï¼")
        except ImportError:
            print("âŒ ç¼ºå°‘ matplotlib")
    else:
        print("\nğŸ† [Top 10 Hot Memories]")
        for mid, count in sorted_memories[:10]:
            print(f"   ID: {mid:<5} | Count: {count}")

# ==========================================
# 4. ä¸»ç¨‹åº (Hydra Managed)
# ==========================================

@hydra.main(version_base=None, config_path="conf", config_name="config")
def main(cfg: DictConfig):
    
    # 0. åŸºç¡€è®¾ç½®ä¸è·¯å¾„æ„é€ 
    print("Visible GPU count:", torch.cuda.device_count())
    
    # æ„é€ æ–‡ä»¶è·¯å¾„ (å…¨éƒ¨åŸºäº cfg.paths.root)
    root_dir = cfg.paths.root
    dataset_tag = cfg.experiment.dataset_name.split('/')[-1]
    
    # å®šä¹‰ä¸­é—´æ–‡ä»¶è·¯å¾„
    corpus_file = os.path.join(root_dir, f"{dataset_tag}_corpus.jsonl")
    test_file = os.path.join(root_dir, f"{dataset_tag}_test_data.jsonl")
    index_dir = os.path.join(root_dir, f"{dataset_tag}_bm25_index")
    
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    result_log_file = os.path.join(root_dir, f"{dataset_tag}_{cfg.model.source}_{cfg.experiment.mode}_{timestamp}.txt")
    vis_image_file = os.path.join(root_dir, f"memory_distribution_{timestamp}.png")

    if os.path.exists(result_log_file): os.remove(result_log_file)
    print(f"ğŸ“ ç»“æœå°†ä¿å­˜è‡³: {result_log_file}")
    print(f"ğŸ› ï¸ æ¨¡å¼: {cfg.experiment.mode} | æº: {cfg.model.source} | æ•°æ®é›†: {cfg.experiment.dataset_name}")

    # 1. æ•°æ®å‡†å¤‡
    if not prepare_data(cfg, corpus_file, test_file): return
    
    # 2. ç´¢å¼•æ„å»º (å¦‚æœæ˜¯ rag æˆ– all æ¨¡å¼)
    if cfg.experiment.mode in ['rag', 'all']:
        build_index(corpus_file, index_dir)
    
    # 3. åˆå§‹åŒ– Generator
    generator = None
    config = None # FlashRAG config
    
    model_source = cfg.model.source
    
    if model_source == "gemini":
        print(f"ğŸ¤– [Init] åˆå§‹åŒ– Gemini: {cfg.model.gemini_name}...")
        api_key = os.environ.get("GEMINI_API_KEY") 
        generator = GeminiGenerator(cfg.model.gemini_name, api_key)
        
        # æ„é€  FlashRAG é…ç½®å­—å…¸
        gemini_config_dict = {
            "data_dir": root_dir,
            "save_dir": cfg.paths.rag_cache_dir,
            "device": "cpu",
            "retrieval_method": cfg.experiment.retrieval_method,
            "corpus_path": corpus_file,
            "index_path": index_dir,
            "retriever_model_path": index_dir,
            "generator_model": "huggingface", # å ä½
            "generator_model_path": "gpt2",   # å ä½
        }
        config = Config(config_dict=gemini_config_dict)

    elif model_source == "sglang":
        print(f"ğŸš€ [Init] åˆå§‹åŒ– SGLang Client...")
        
        # 1. ä» config è¯»å–
        sglang_base_url = cfg.model.get("sglang_api_url", "http://127.0.0.1:30000/v1")
        # âš ï¸ ç¡®ä¿è¿™é‡Œè¯»åˆ°çš„æ˜¯ "Qwen/Qwen3-4B-Instruct-2507"
        sglang_model_name = cfg.model.get("sglang_model_name", "Qwen/Qwen3-4B-Instruct-2507")
        
        # 2. æ„é€  FlashRAG é…ç½®å­—å…¸
        sglang_config_dict = {
            "data_dir": root_dir,
            "save_dir": cfg.paths.rag_cache_dir,
            "corpus_path": corpus_file,
            "index_path": index_dir,
            "retriever_model_path": index_dir,
            "retrieval_method": cfg.experiment.retrieval_method,
            
            # --- å…³é”®ä¿®æ”¹ ---
            "device": "cpu",
            "gpu_num": 0,
            
            # 1. å‘Šè¯‰ FlashRAG æˆ‘ä»¬åœ¨ç”¨ç±»ä¼¼ OpenAI çš„ç”Ÿæˆåè®® (è™½ç„¶æˆ‘ä»¬å®é™…ä¸Šæ˜¯ç”¨è‡ªå®šä¹‰ Generator è¦†ç›–äº†å®ƒ)
            "generator_model": "openai",   
            
            # 2. ğŸ”¥ğŸ”¥ğŸ”¥ æ ¸å¿ƒä¿®å¤åœ¨è¿™é‡Œ ğŸ”¥ğŸ”¥ğŸ”¥
            # ä¸è¦è®©å®ƒå»åŠ è½½ "openai" çš„ configï¼Œè€Œæ˜¯å»åŠ è½½ Qwen çš„ configï¼
            # PromptTemplate éœ€è¦è¿™ä¸ªè·¯å¾„æ¥ä¸‹è½½ tokenizer config
            "generator_model_path": sglang_model_name, 
            
            "generation_method": "openai", 
            "batch_size": cfg.model.batch_size,
            "max_input_len": cfg.model.max_input_len,
            "max_new_tokens": cfg.model.max_new_tokens,
        }
        
        config = Config(config_dict=sglang_config_dict)

        # 4. åˆå§‹åŒ– Generator
        generator = SGLangGenerator(
            base_url=sglang_base_url,
            model_name=sglang_model_name,
            max_new_tokens=cfg.model.max_new_tokens,
            batch_size=cfg.model.batch_size,
            temperature=0.7, # å¦‚æœéœ€è¦ï¼Œè¿™ä¸ªä¹Ÿå¯ä»¥æåˆ° yaml é‡Œ
        )
        print(f"âœ… SGLang Generator ({sglang_model_name}) å·²è¿æ¥è‡³ {sglang_base_url}")

    elif model_source == "huggingface":
        hf_name = cfg.model.hf_name
        print(f"ğŸ“¥ [Init] æ£€æŸ¥/ä¸‹è½½ HF æ¨¡å‹: {hf_name}...")
        try:
            model_path = snapshot_download(repo_id=hf_name)
        except:
            print("âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥")
            return

        hf_config_dict = {
            "data_dir": root_dir,
            "save_dir": cfg.paths.rag_cache_dir,
            "device": cfg.model.device,
            "gpu_num": torch.cuda.device_count(),
            "generator_model": "huggingface",
            "generator_model_path": model_path,
            "generation_method": "huggingface",
            "batch_size": cfg.model.batch_size,
            "max_input_len": cfg.model.max_input_len,
            "max_new_tokens": cfg.model.max_new_tokens,
        }
        print("ğŸš€ åŠ è½½ HF ç”Ÿæˆå™¨...")
        config = Config(config_dict=hf_config_dict)
        generator = get_generator(config)
        
        # ğŸ”¥ Tokenizer ä¿®æ­£ (ä¿æŒä½ åŸæœ‰çš„ padding ä¿®å¤)
        if hasattr(generator, 'tokenizer'):
            generator.tokenizer.padding_side = 'left' 
            if generator.tokenizer.pad_token is None:
                generator.tokenizer.pad_token = generator.tokenizer.eos_token
                generator.tokenizer.pad_token_id = generator.tokenizer.eos_token_id
            generator.tokenizer.model_max_length = cfg.model.max_input_len
        
        if hasattr(generator, 'model'):
            if hasattr(generator.model.config, 'pad_token_id') and generator.model.config.pad_token_id is None:
                generator.model.config.pad_token_id = generator.tokenizer.pad_token_id
        print(f"âœ… Tokenizer ä¿®æ­£å®Œæˆ")
    
    else:
        print(f"âŒ æœªæ”¯æŒçš„ MODEL_SOURCE: {model_source}")
        return

    # è¯»å–æµ‹è¯•æ•°æ®
    with open(test_file, "r") as f:
        test_dataset_raw = [json.loads(line) for line in f]

    acc_baseline = 0
    acc_rag = 0

    # æ ¼å¼åŒ– Prompt è¾…åŠ©å‡½æ•°
    def format_base_prompt(system_text, user_text):
        if model_source == "gemini":
            return f"{system_text}\n\n{user_text}" if system_text else user_text
        prompt = ""
        if system_text: prompt += f"{system_text}\n\n"
        prompt += f"### Question:\n{user_text}\n\n### Answer:\nLet's think step by step."
        return prompt

    # --- Task A: Baseline ---
    if cfg.experiment.mode in ['baseline', 'all']:
        print("\nâš”ï¸ [Task A] æ­£åœ¨è¿è¡Œ Baseline ...")
        
        baseline_inputs = []
        for item in test_dataset_raw:
            sys_msg = "You are a math expert. Solve the problem in a brief. Don't answer more than 50 words.End your answer with \\boxed{number}."
            formatted_prompt = format_base_prompt(sys_msg, item['question'])
            baseline_inputs.append(formatted_prompt)

        baseline_preds = generator.generate(baseline_inputs)
        
        baseline_results = []
        for item, pred in zip(test_dataset_raw, baseline_preds):
            baseline_results.append({
                "question": item['question'],
                "golden_answers": item['golden_answers'],
                "pred": pred
            })
        acc_baseline = evaluate_results(baseline_results, "Baseline (No RAG)", result_log_file)

    # --- Task B: FlashRAG ---
    if cfg.experiment.mode in ['rag', 'all']:
        print("\nâš”ï¸ [Task B] æ­£åœ¨è¿è¡Œ FlashRAG (Few-shot Retrieval)...")
        
        # å‡†å¤‡ RAG é…ç½®
        rag_config_dict = OmegaConf.to_container(cfg, resolve=True) # ä»…ä»…æ˜¯ä¸ºäº†è·å–ä¸€äº›åŸºç¡€ç±»å‹
        # å°† FlashRAG éœ€è¦çš„ç‰¹å®šå­—æ®µè¦†ç›–è¿›å»
        rag_update = {
            "data_dir": root_dir,
            "save_dir": cfg.paths.rag_cache_dir,
            "retrieval_method": cfg.experiment.retrieval_method,
            "corpus_path": corpus_file,
            "index_path": index_dir,
            "retriever_model_path": index_dir,
            "topk": cfg.experiment.retrieval_topk,
            # Generator é…ç½®ç»§æ‰¿ä¹‹å‰çš„
            "device": cfg.model.device,
            "generator_model_path": config['generator_model_path'] if 'generator_model_path' in config else "gpt2"
        }
        
        # é‡æ–°å®ä¾‹åŒ– Config ä»¥ç¡®ä¿ Retriever èƒ½è¯»åˆ°æ­£ç¡®å‚æ•°
        rag_config = Config(config_dict=rag_update)
        retriever = get_retriever(rag_config)
        
        rag_system_part = cfg.experiment.prompts.rag_system
        
        prompt_tpl = PromptTemplate(rag_config, system_prompt=rag_system_part, user_prompt="")
        pipeline = SequentialPipeline(rag_config, prompt_template=prompt_tpl, retriever=retriever, generator=generator)
        dataset_obj = Dataset(rag_config, test_file)
        
        rag_results = pipeline.run(dataset_obj)
        
        acc_rag = evaluate_results(rag_results, f"FlashRAG ({dataset_tag} Memory)", result_log_file)
        
        # ç»Ÿè®¡è®°å¿†çƒ­åº¦ (ä¼ å…¥ cfg)
        analyze_memory_usage(rag_results, cfg, corpus_file, vis_image_file)

    # --- Summary ---
    if cfg.experiment.mode == 'all':
        summary = (
            f"\n{'='*20} æœ€ç»ˆå¯¹æ¯”ç»“æœ {'='*20}\n"
            f"ğŸ“Š æ•°æ®é›†: {cfg.experiment.dataset_name}\n"
            f"ğŸ¤– æ¨¡å‹: {model_source}\n"
            f"ğŸ“‰ Baseline: {acc_baseline:.2f}%\n"
            f"ğŸ“ˆ FlashRAG: {acc_rag:.2f}%\n"
            f"ğŸš€ æå‡: {acc_rag - acc_baseline:+.2f}%\n"
            f"{'='*50}\n"
        )
        print(summary)
        with open(result_log_file, "a", encoding="utf-8") as f:
            f.write(summary)

if __name__ == "__main__":
    main()