import os
import json
import re
import time
import torch
import bm25s
import logging
import ast
import collections
import matplotlib.pyplot as plt
from tqdm import tqdm
from datasets import load_dataset
from huggingface_hub import snapshot_download

# Hydra & OmegaConf
import hydra
from omegaconf import DictConfig, OmegaConf

# FlashRAG
from flashrag.config import Config
from flashrag.pipeline import SequentialPipeline
from flashrag.utils import get_retriever, get_generator, Dataset
from flashrag.prompt import PromptTemplate

# å±è”½ transformers çš„å†—ä½™è­¦å‘Š
import transformers
transformers.logging.set_verbosity_error()

# ==========================================
# 1. å·¥å…·ç±»ä¸ç”Ÿæˆå™¨ (ä¿æŒåŸæ ·é€»è¾‘)
# ==========================================

class GeminiGenerator:
    def __init__(self, model_name, api_key):
        import google.generativeai as genai
        if not api_key:
            raise ValueError("âŒ æœªæ£€æµ‹åˆ° API Keyï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ GEMINI_API_KEY")
        
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model_name)
        print(f"ğŸ¤– Gemini Generator ({model_name}) å·²åŠ è½½")
        self.max_input_len = 30000 

    def generate(self, input_list, **kwargs):
        responses = []
        for prompt in input_list:
            try:
                if isinstance(prompt, list): prompt = " ".join(prompt)
                clean_prompt = str(prompt)
                result = self.model.generate_content(clean_prompt)
                if result.parts:
                    responses.append(result.text)
                else:
                    responses.append("Error: Empty Response (Safety Block)")
                time.sleep(2) 
            except Exception as e:
                print(f"âš ï¸ Gemini API Error: {e}")
                time.sleep(5)
                responses.append("Error")
        return responses


from typing import List
from openai import OpenAI

class SGLangGenerator:
    """ä¸€ä¸ªæœ€å°å®ç°çš„ç”Ÿæˆå™¨ï¼Œé€‚é… FlashRAG çš„ generator.generate(prompts) æ¥å£ã€‚"""
    def __init__(
        self,
        base_url: str,
        model_name: str,
        max_new_tokens: int = 1024,
        batch_size: int = 8,
        temperature: float = 0.0,
    ):
        self.client = OpenAI(
            api_key=os.getenv("SGLANG_API_KEY", "EMPTY"),
            base_url=base_url.rstrip("/"),
        )
        self.model = model_name
        self.max_new_tokens = max_new_tokens
        self.batch_size = batch_size
        self.temperature = temperature
        self.max_input_len = 4096

    def generate(self, prompts: List[str]) -> List[str]:
        outputs: List[str] = []
        for i in range(0, len(prompts), self.batch_size):
            batch = prompts[i : i + self.batch_size]
            for p in batch:
                resp = self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": p}],
                    temperature=self.temperature,
                    max_tokens=self.max_new_tokens,
                )
                outputs.append(resp.choices[0].message.content)
        return outputs

# ==========================================
# 2. è¯„ä¼°å·¥å…· (Math Logic)
# ==========================================

def extract_math_answer(text):
    if not text: return None
    text = str(text)
    idx = text.rfind("\\boxed{")
    if idx != -1:
        content_start = idx + 7 
        balance = 0
        for i in range(content_start, len(text)):
            char = text[i]
            if char == '{':
                balance += 1
            elif char == '}':
                if balance == 0:
                    return text[content_start:i]
                balance -= 1
    
    lines = text.strip().split('\n')
    if lines:
        last_line = lines[-1].strip()
        last_line = re.sub(r'^(The )?Answer( is)?:?', '', last_line, flags=re.IGNORECASE).strip()
        if len(last_line) < 50: 
            return last_line
    return None

def normalize_latex(s):
    if not s: return ""
    s = str(s)
    s = "".join(s.split())
    s = s.replace("\\dfrac", "\\frac")
    s = s.replace("\\text", "")
    return s.strip()

def evaluate_results(results, experiment_name, result_log_file):
    correct = 0
    total = len(results)
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(result_log_file), exist_ok=True)

    with open(result_log_file, "a", encoding="utf-8") as f:
        header = f"\n{'='*20} {experiment_name} {'='*20}\n"
        print(header.strip())
        f.write(header)
        
        for i, item in enumerate(results):
            # å…¼å®¹ FlashRAG Dataset å¯¹è±¡å’Œ dict
            pred = item.pred if hasattr(item, 'pred') else item['pred']
            gold_raw = item.golden_answers[0] if hasattr(item, 'golden_answers') else item['golden_answers'][0]
            question = item.question if hasattr(item, 'question') else item['question']

            gold_val = extract_math_answer(gold_raw)
            if gold_val is None: gold_val = str(gold_raw).strip()

            pred_val = extract_math_answer(pred)
            is_right = False
            
            if gold_val and pred_val:
                norm_gold = normalize_latex(gold_val)
                norm_pred = normalize_latex(pred_val)
                if norm_gold == norm_pred:
                    is_right = True

            if is_right: correct += 1

            log_entry = (
                f"\n[ID]: {i}\n"
                f"[Question]: {str(question)[:100]}...\n"
                f"[Gold Raw]: ... => [Extracted]: {gold_val}\n"
                f"[Pred Raw]: ...{str(pred)[-50:].replace(chr(10), ' ')} => [Extracted]: {pred_val}\n"
                f"[Result]: {'âœ… Correct' if is_right else 'âŒ Wrong'}\n"
                f"{'-'*30}\n"
            )
            f.write(log_entry)
            if i < 5: print(log_entry.strip()) # å‡å°‘ä¸€ç‚¹æ§åˆ¶å°è¾“å‡º

        acc = correct / total * 100
        summary = (
            f"\nğŸ“Š ç»Ÿè®¡ ({experiment_name}):\n"
            f"Total: {total}, Correct: {correct}, Accuracy: {acc:.2f}%\n"
            f"{'='*50}\n"
        )
        print(summary)
        f.write(summary)
    return acc

# ==========================================
# 3. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° (Hydra é€‚é…)
# ==========================================

def prepare_data(cfg: DictConfig, corpus_file: str, test_file: str):
    """
    å‡†å¤‡æ•°æ®ï¼š
    1. ç”Ÿæˆ corpus.jsonl (è®°å¿†åº“)
    2. ç”Ÿæˆ test.jsonl (æµ‹è¯•é›†ï¼Œåœ¨æ­¤å¤„åº”ç”¨ start_index å’Œ debug_num)
    """
    dataset_name = cfg.experiment.dataset_name
    dataset_config = cfg.experiment.dataset_config
    
    print(f"ğŸ“¥ [Step 1] æ­£åœ¨åŠ è½½æ•°æ®é›†: {dataset_name} (Config: {dataset_config})...")
    try:
        if dataset_config:
            dataset = load_dataset(dataset_name, dataset_config)
        else:
            dataset = load_dataset(dataset_name)
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return False

    q_col = cfg.experiment.field_map.question
    a_col = cfg.experiment.field_map.answer
    
    split_train = "train"
    split_test = "test"

    # --- A. æ„å»ºè®°å¿†åº“ (Train) ---
    if not os.path.exists(corpus_file):
        print(f"ğŸ”¨ [Memory] æ­£åœ¨å°† {split_train} é›†è½¬æ¢ä¸ºè®°å¿†åº“: {corpus_file}...")
        if split_train not in dataset:
            print(f"âš ï¸ è­¦å‘Š: æ•°æ®é›†æ²¡æœ‰ {split_train} åˆ’åˆ†ï¼")
            return False

        with open(corpus_file, "w", encoding="utf-8") as f:
            for i, item in enumerate(tqdm(dataset[split_train])):
                q_text = item.get(q_col, "")
                a_text = item.get(a_col, "")
                content = f"Question: {q_text}\nAnswer: {a_text}"
                f.write(json.dumps({"id": str(i), "contents": content}) + "\n")
    else:
        print(f"âœ… [Memory] æ£€æµ‹åˆ°ç°æœ‰è®°å¿†åº“: {corpus_file}ï¼Œè·³è¿‡æ„å»ºã€‚")
    
    # --- B. å‡†å¤‡æµ‹è¯•é›† (Test) ---
    print(f"ğŸ”¨ [Test] æ­£åœ¨å¤„ç†æµ‹è¯•é›†...")
    
    with open(test_file, "w", encoding="utf-8") as f:
        if split_test not in dataset:
             print(f"âŒ é”™è¯¯: æ•°æ®é›†æ²¡æœ‰ {split_test} åˆ’åˆ†ï¼")
             return False
             
        test_data = dataset[split_test]
        
        # 1. è·å–é…ç½®å‚æ•°
        # start_index: ä»ç¬¬å‡ é¢˜å¼€å§‹ (é»˜è®¤ 0)
        start_idx = int(cfg.experiment.get("start_index", 0) or 0)
        # debug_num: é™åˆ¶æµ‹è¯•å¤šå°‘é¢˜ (é»˜è®¤ Noneï¼Œå³è·‘å®Œå‰©ä½™æ‰€æœ‰)
        debug_num = cfg.experiment.get("debug_num")
        
        total_len = len(test_data)
        
        # 2. è®¡ç®—åˆ‡ç‰‡èŒƒå›´
        # å¦‚æœè®¾ç½®äº† debug_numï¼Œç»“æŸä½ç½®å°±æ˜¯ start + debug_num
        # å¦åˆ™ç»“æŸä½ç½®å°±æ˜¯æ€»é•¿åº¦
        if debug_num:
            limit = int(debug_num)
            end_idx = min(start_idx + limit, total_len)
            print(f"ğŸ› [Config] é™åˆ¶èŒƒå›´: ç¬¬ {start_idx} é¢˜ -> ç¬¬ {end_idx} é¢˜ (å…± {end_idx - start_idx} é¢˜)")
        else:
            end_idx = total_len
            print(f"â© [Config] ä»ç¬¬ {start_idx} é¢˜å¼€å§‹è·‘å®Œæ‰€æœ‰å‰©ä½™é¢˜ç›® (å…± {end_idx - start_idx} é¢˜)")

        # 3. æ‰§è¡Œåˆ‡ç‰‡ (ä½¿ç”¨ HuggingFace çš„ select æ–¹æ³•)
        if start_idx >= total_len:
            print("âš ï¸ Warning: start_index è¶…è¿‡äº†æ•°æ®æ€»é‡ï¼Œæµ‹è¯•é›†å°†ä¸ºç©ºï¼")
            test_data = test_data.select([]) # ç©ºé›†
        else:
            # åˆ›å»ºç´¢å¼•åˆ—è¡¨
            indices = range(start_idx, end_idx)
            test_data = test_data.select(indices)

        print(f"ğŸ“Š æœ€ç»ˆå†™å…¥æµ‹è¯•é›†æ•°é‡: {len(test_data)}")
            
        # 4. å†™å…¥æ–‡ä»¶
        for i, item in enumerate(test_data):
            q_text = item.get(q_col, "")
            raw_ans = item.get(a_col, "")
            
            # ğŸ”¥ å…³é”®ç‚¹ï¼šä¿®æ­£ ID
            # è¿™é‡Œçš„ i æ˜¯ä» 0 å¼€å§‹çš„ï¼Œä½†ä¸ºäº†æ–¹ä¾¿å›æº¯ï¼Œæˆ‘ä»¬å°† ID è®¾ç½®ä¸º "åŸå§‹åç§»é‡ + i"
            # ä¾‹å¦‚ä»ç¬¬ 100 é¢˜å¼€å§‹è·‘ï¼Œç¬¬ä¸€æ¡æ•°æ®çš„ ID å°±æ˜¯ 100
            real_id = start_idx + i
            
            f.write(json.dumps({
                "id": str(real_id),
                "question": q_text,
                "golden_answers": [str(raw_ans)]
            }) + "\n")            
    return True

def build_index(corpus_file: str, index_dir: str):
    """æ„å»º BM25 ç´¢å¼•"""
    if os.path.exists(index_dir) and os.path.exists(os.path.join(index_dir, "vocab.tokenizer.json")):
        print(f"âœ… [Index] ç´¢å¼•å·²å­˜åœ¨: {index_dir}ï¼Œè·³è¿‡æ„å»ºã€‚")
        return

    print(f"ğŸ”¨ [Index] æ­£åœ¨ä¸º {corpus_file} æ„å»º BM25 ç´¢å¼•...")
    corpus_texts = []
    # ä½¿ç”¨ bm25s åº“
    with open(corpus_file, "r", encoding="utf-8") as f:
        for line in f:
            corpus_texts.append(json.loads(line)['contents'])
    
    corpus_tokens = bm25s.tokenize(corpus_texts)
    retriever_builder = bm25s.BM25()
    retriever_builder.index(corpus_tokens)
    retriever_builder.save(index_dir)
    
    # FlashRAG è¦æ±‚çš„é¢å¤–æ–‡ä»¶
    with open(os.path.join(index_dir, "stopwords.tokenizer.json"), "w") as f:
        json.dump([], f)
    with open(os.path.join(index_dir, "vocab.tokenizer.json"), "w") as f:
        vocab = corpus_tokens.vocab
        # å…¼å®¹æ€§å¤„ç†
        json.dump({"word_to_id": vocab, "stem_to_sid": vocab, "word_to_stem": {k: k for k in vocab}}, f)
    print("âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼")

def analyze_memory_usage(rag_results, cfg: DictConfig, corpus_file: str, vis_image_file: str):
    """
    è®°å¿†çƒ­åº¦/æ•ˆç”¨ç»Ÿè®¡ä¸å¯¼å‡º (å¼ºåŒ–å­¦ä¹ ç‰ˆ)
    é€»è¾‘ï¼š
    - æ£€ç´¢å‘½ä¸­ & é¢˜ç›®åšå¯¹: freq += 2 (å¥–åŠ±)
    - æ£€ç´¢å‘½ä¸­ & é¢˜ç›®åšé”™: freq -= 2 (æƒ©ç½š)
    """
    # è¿™é‡Œçš„ freq_file ä» config ä¸­è¯»å–
    freq_file = cfg.paths.freq_file
    
    print("\nğŸ” [Analysis] æ­£åœ¨è¿›è¡Œå…¨é‡è®°å¿†æ•ˆç”¨è¯„åˆ† (RL Scoring)...")
    
    # ================= åˆ¤é¢˜è¾…åŠ©å‡½æ•° (å†…åµŒï¼Œç¡®ä¿ç‹¬ç«‹è¿è¡Œ) =================
    def _local_extract(text):
        if not text: return None
        text = str(text).strip()
        idx = text.rfind("\\boxed{")
        if idx != -1:
            content_start = idx + 7 
            balance = 0
            for i in range(content_start, len(text)):
                if text[i] == '{': balance += 1
                elif text[i] == '}':
                    if balance == 0: return text[content_start:i] 
                    balance -= 1
        lines = text.strip().split('\n')
        if lines:
            last_line = lines[-1].strip()
            if last_line.endswith('.'): last_line = last_line[:-1]
            last_line = last_line.replace('$', '').replace('`', '')
            last_line = re.sub(r'^(The )?Answer( is)?:?', '', last_line, flags=re.IGNORECASE).strip()
            if '=' in last_line: last_line = last_line.split('=')[-1].strip()
            if '\\approx' in last_line: last_line = last_line.split('\\approx')[-1].strip()
            if len(last_line) < 100: return last_line
        return None

    def _local_norm(s):
        if not s: return ""
        s = str(s).replace('$', '').replace('`', '').replace('\\%', '%')
        s = s.replace("\\dfrac", "\\frac").replace("\\text", "")
        s = s.replace("\\left", "").replace("\\right", "").replace("\\mathrm", "")
        s = "".join(s.split())
        if '=' in s: s = s.split('=')[-1]
        if '\\in' in s: s = s.split('\\in')[-1]
        return s.rstrip('.').strip()

    def _check_correct(item):
        """åˆ¤æ–­å•æ¡ç»“æœæ˜¯å¦æ­£ç¡®"""
        pred = item.pred if hasattr(item, 'pred') else item.get('pred')
        gold = item.golden_answers[0] if hasattr(item, 'golden_answers') else item.get('golden_answers')[0]
        
        gold_val = _local_extract(gold) or str(gold).strip()
        pred_val = _local_extract(pred)
        
        if gold_val and pred_val:
            if _local_norm(gold_val) == _local_norm(pred_val):
                return True
        return False
    # ===============================================================

    all_memory_ids = set()
    id_to_content = {} 

    try:
        with open(corpus_file, "r", encoding="utf-8") as f:
            for line in f:
                item = json.loads(line)
                mid = str(item['id'])
                all_memory_ids.add(mid)
                id_to_content[mid] = item.get("contents", "")
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è¯»å–è®°å¿†åº“æ–‡ä»¶ {corpus_file}ï¼Œé”™è¯¯: {e}")
    
    # åˆå§‹åŒ–åˆ†æ•°ï¼Œé»˜è®¤ 0
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ç”¨ score ä»£æ›¿åŸæ¥çš„å•çº¯è®¡æ•°ï¼Œä½†å˜é‡ååœ¨è¾“å‡ºæ—¶ä¾ç„¶å« freq ä»¥å…¼å®¹åç»­è„šæœ¬
    memory_scores = {mid: 0 for mid in all_memory_ids}
    
    # ç»Ÿè®¡å‘½ä¸­å¹¶æ‰“åˆ†
    total_questions = len(rag_results)
    correct_count = 0

    for item in tqdm(rag_results, desc="Scoring Memories"):
        # 1. åˆ¤å®šå½“å‰é¢˜ç›®æ˜¯å¦åšå¯¹
        is_correct = _check_correct(item)
        if is_correct: correct_count += 1
        
        # 2. å†³å®š å¥–åŠ±/æƒ©ç½š åˆ†æ•°
        reward = 2 if is_correct else -2
        
        # 3. æ‰¾åˆ°æ£€ç´¢åˆ°çš„è®°å¿†ï¼Œè¿›è¡ŒåŠ å‡åˆ†
        retrieved_docs = getattr(item, 'retrieval_result', [])
        for doc in retrieved_docs:
            if isinstance(doc, dict):
                doc_id = str(doc.get('id'))
            else:
                doc_id = str(getattr(doc, 'id', None))
            
            if doc_id and doc_id in memory_scores:
                memory_scores[doc_id] += reward

    # æ’åº (æŒ‰åˆ†æ•°ä»é«˜åˆ°ä½)
    sorted_memories = sorted(memory_scores.items(), key=lambda x: (-x[1], x[0]))
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_mem = len(sorted_memories)
    positive_mem = sum(1 for _, v in sorted_memories if v > 0)
    negative_mem = sum(1 for _, v in sorted_memories if v < 0)
    zero_mem = sum(1 for _, v in sorted_memories if v == 0)
    
    print(f"ğŸ“Š è®°å¿†åº“è¯„åˆ†ç»Ÿè®¡:")
    print(f"   - æ€»é‡: {total_mem}")
    print(f"   - æ­£åˆ†(è´¡çŒ®è€…): {positive_mem} ({(positive_mem/total_mem)*100:.1f}%)")
    print(f"   - è´Ÿåˆ†(å¹²æ‰°é¡¹): {negative_mem} ({(negative_mem/total_mem)*100:.1f}%)")
    print(f"   - é›¶åˆ†(å†·é—¨): {zero_mem}")
    print(f"   - å½“å‰é¢˜ç›®æ­£ç¡®ç‡: {correct_count/total_questions*100:.2f}%")

    # å¯¼å‡º jsonl (ä¿æŒ freq å­—æ®µåï¼Œä½†å­˜çš„æ˜¯åˆ†æ•°)
    try:
        print(f"ğŸ’¾ [Save] æ­£åœ¨å¯¼å‡ºè®°å¿†è¯„åˆ†ç»“æœåˆ°: {freq_file}")
        os.makedirs(os.path.dirname(freq_file), exist_ok=True)
        
        with open(freq_file, "w", encoding="utf-8") as f:
            for rank, (mid, score) in enumerate(sorted_memories, start=1):
                record = {
                    "rank": rank,
                    "memory_id": mid,
                    "freq": int(score), # ğŸ”¥ è¿™é‡Œå­˜çš„æ˜¯åˆ†æ•° (-2, 0, 2, 4...)
                    "contents": id_to_content.get(mid, "")
                }
                f.write(json.dumps(record, ensure_ascii=False) + "\n")
        print("âœ… è¯„åˆ†æ–‡ä»¶å¯¼å‡ºå®Œæˆï¼")
    except Exception as e:
        print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")

    # å¯è§†åŒ– (åˆ†æ•°åˆ†å¸ƒå›¾)
    if cfg.experiment.visualize_memory:
        print(f"ğŸ¨ [Visual] æ­£åœ¨ç”Ÿæˆåˆ†æ•°åˆ†å¸ƒå›¾: {vis_image_file}")
        try:
            ids = [m[0] for m in sorted_memories]
            scores = [m[1] for m in sorted_memories]
            
            display_limit = 30
            if len(ids) > display_limit * 2:
                plot_ids = ids[:display_limit] + ["..."] + ids[-display_limit:]
                plot_scores = scores[:display_limit] + [0] + scores[-display_limit:]
                # é¢œè‰²åŒºåˆ†ï¼šæ­£åˆ†è“ï¼Œè´Ÿåˆ†çº¢ï¼Œé›¶åˆ†ç™½
                colors = []
                for s in plot_scores:
                    if s > 0: colors.append('skyblue')
                    elif s < 0: colors.append('salmon')
                    else: colors.append('lightgrey')
            else:
                plot_ids = ids
                plot_scores = scores
                colors = ['skyblue' if s > 0 else 'salmon' if s < 0 else 'lightgrey' for s in plot_scores]

            plt.figure(figsize=(15, 6))
            # ç”»ä¸€æ¡ 0 åˆ†çº¿
            plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
            
            bars = plt.bar(plot_ids, plot_scores, color=colors, edgecolor='navy')
            plt.title(f'Memory Utility Score (Correct=+2, Wrong=-2)', fontsize=14)
            plt.ylabel('Score')
            plt.xticks(rotation=90, fontsize=8) 
            
            # æ˜¾ç¤ºæ•°å€¼
            for i, bar in enumerate(bars):
                height = bar.get_height()
                if plot_ids[i] != "...": 
                    # æ­£åˆ†æ˜¾ç¤ºåœ¨æ¡ä¸Šæ–¹ï¼Œè´Ÿåˆ†æ˜¾ç¤ºåœ¨æ¡ä¸‹æ–¹
                    y_pos = height if height >= 0 else height - (max(scores)*0.05)
                    va = 'bottom' if height >= 0 else 'top'
                    plt.text(bar.get_x() + bar.get_width()/2., y_pos, f'{int(height)}',
                             ha='center', va=va, fontsize=8)
            
            plt.tight_layout()
            plt.savefig(vis_image_file, dpi=300)
            print("âœ… å›¾ç‰‡ä¿å­˜æˆåŠŸï¼")
        except ImportError:
            print("âŒ ç¼ºå°‘ matplotlib")
    else:
        print("\nğŸ† [Top 10 High-Utility Memories]")
        for mid, score in sorted_memories[:10]:
            print(f"   ID: {mid:<5} | Score: {score}")

# ==========================================
# 4. ä¸»ç¨‹åº (Hydra Managed)
# ==========================================

@hydra.main(version_base=None, config_path="conf", config_name="config")
def main(cfg: DictConfig):
    
    # 0. åŸºç¡€è®¾ç½®ä¸è·¯å¾„æ„é€ 
    print("Visible GPU count:", torch.cuda.device_count())
    
    # æ„é€ æ–‡ä»¶è·¯å¾„ (å…¨éƒ¨åŸºäº cfg.paths.root)
    root_dir = cfg.paths.root
    dataset_tag = cfg.experiment.dataset_name.split('/')[-1]
    
    # å®šä¹‰ä¸­é—´æ–‡ä»¶è·¯å¾„
    corpus_file = os.path.join(root_dir, f"{dataset_tag}_corpus.jsonl")
    test_file = os.path.join(root_dir, f"{dataset_tag}_test_data.jsonl")
    index_dir = os.path.join(root_dir, f"{dataset_tag}_bm25_index")
    
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    result_log_file = os.path.join(root_dir, f"{dataset_tag}_{cfg.model.source}_{cfg.experiment.mode}_{timestamp}.txt")
    vis_image_file = os.path.join(root_dir, f"memory_distribution_{timestamp}.png")

    if os.path.exists(result_log_file): os.remove(result_log_file)
    print(f"ğŸ“ ç»“æœå°†ä¿å­˜è‡³: {result_log_file}")
    print(f"ğŸ› ï¸ æ¨¡å¼: {cfg.experiment.mode} | æº: {cfg.model.source} | æ•°æ®é›†: {cfg.experiment.dataset_name}")

    # 1. æ•°æ®å‡†å¤‡
    if not prepare_data(cfg, corpus_file, test_file): return
    
    # 2. ç´¢å¼•æ„å»º (å¦‚æœæ˜¯ rag æˆ– all æ¨¡å¼)
    if cfg.experiment.mode in ['rag', 'all']:
        build_index(corpus_file, index_dir)
    
    # 3. åˆå§‹åŒ– Generator
    generator = None
    config = None # FlashRAG config
    
    model_source = cfg.model.source
    
    if model_source == "gemini":
        print(f"ğŸ¤– [Init] åˆå§‹åŒ– Gemini: {cfg.model.gemini_name}...")
        api_key = os.environ.get("GEMINI_API_KEY") 
        generator = GeminiGenerator(cfg.model.gemini_name, api_key)
        
        # æ„é€  FlashRAG é…ç½®å­—å…¸
        gemini_config_dict = {
            "data_dir": root_dir,
            "save_dir": cfg.paths.rag_cache_dir,
            "device": "cpu",
            "retrieval_method": cfg.experiment.retrieval_method,
            "corpus_path": corpus_file,
            "index_path": index_dir,
            "retriever_model_path": index_dir,
            "generator_model": "huggingface", # å ä½
            "generator_model_path": "gpt2",   # å ä½
        }
        config = Config(config_dict=gemini_config_dict)

    elif model_source == "sglang":
        print(f"ğŸš€ [Init] åˆå§‹åŒ– SGLang Client...")
        
        # 1. ä» config è¯»å–
        sglang_base_url = cfg.model.get("sglang_api_url", "http://127.0.0.1:30000/v1")
        # âš ï¸ ç¡®ä¿è¿™é‡Œè¯»åˆ°çš„æ˜¯ "Qwen/Qwen3-4B-Instruct-2507"
        sglang_model_name = cfg.model.get("sglang_model_name", "Qwen/Qwen3-4B-Instruct-2507")
        
        # 2. æ„é€  FlashRAG é…ç½®å­—å…¸
        sglang_config_dict = {
            "data_dir": root_dir,
            "save_dir": cfg.paths.rag_cache_dir,
            "corpus_path": corpus_file,
            "index_path": index_dir,
            "retriever_model_path": index_dir,
            "retrieval_method": cfg.experiment.retrieval_method,
            
            # --- å…³é”®ä¿®æ”¹ ---
            "device": "cpu",
            "gpu_num": 0,
            
            # 1. å‘Šè¯‰ FlashRAG æˆ‘ä»¬åœ¨ç”¨ç±»ä¼¼ OpenAI çš„ç”Ÿæˆåè®® (è™½ç„¶æˆ‘ä»¬å®é™…ä¸Šæ˜¯ç”¨è‡ªå®šä¹‰ Generator è¦†ç›–äº†å®ƒ)
            "generator_model": "openai",   
            
            # 2. ğŸ”¥ğŸ”¥ğŸ”¥ æ ¸å¿ƒä¿®å¤åœ¨è¿™é‡Œ ğŸ”¥ğŸ”¥ğŸ”¥
            # ä¸è¦è®©å®ƒå»åŠ è½½ "openai" çš„ configï¼Œè€Œæ˜¯å»åŠ è½½ Qwen çš„ configï¼
            # PromptTemplate éœ€è¦è¿™ä¸ªè·¯å¾„æ¥ä¸‹è½½ tokenizer config
            "generator_model_path": sglang_model_name, 
            
            "generation_method": "openai", 
            "batch_size": cfg.model.batch_size,
            "max_input_len": cfg.model.max_input_len,
            "max_new_tokens": cfg.model.max_new_tokens,
        }
        
        config = Config(config_dict=sglang_config_dict)

        # 4. åˆå§‹åŒ– Generator
        generator = SGLangGenerator(
            base_url=sglang_base_url,
            model_name=sglang_model_name,
            max_new_tokens=cfg.model.max_new_tokens,
            batch_size=cfg.model.batch_size,
            temperature=0.7, # å¦‚æœéœ€è¦ï¼Œè¿™ä¸ªä¹Ÿå¯ä»¥æåˆ° yaml é‡Œ
        )
        print(f"âœ… SGLang Generator ({sglang_model_name}) å·²è¿æ¥è‡³ {sglang_base_url}")

    elif model_source == "huggingface":
        hf_name = cfg.model.hf_name
        print(f"ğŸ“¥ [Init] æ£€æŸ¥/ä¸‹è½½ HF æ¨¡å‹: {hf_name}...")
        try:
            model_path = snapshot_download(repo_id=hf_name)
        except:
            print("âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥")
            return

        hf_config_dict = {
            "data_dir": root_dir,
            "save_dir": cfg.paths.rag_cache_dir,
            "device": cfg.model.device,
            "gpu_num": torch.cuda.device_count(),
            "generator_model": "huggingface",
            "generator_model_path": model_path,
            "generation_method": "huggingface",
            "batch_size": cfg.model.batch_size,
            "max_input_len": cfg.model.max_input_len,
            "max_new_tokens": cfg.model.max_new_tokens,
        }
        print("ğŸš€ åŠ è½½ HF ç”Ÿæˆå™¨...")
        config = Config(config_dict=hf_config_dict)
        generator = get_generator(config)
        
        # ğŸ”¥ Tokenizer ä¿®æ­£ (ä¿æŒä½ åŸæœ‰çš„ padding ä¿®å¤)
        if hasattr(generator, 'tokenizer'):
            generator.tokenizer.padding_side = 'left' 
            if generator.tokenizer.pad_token is None:
                generator.tokenizer.pad_token = generator.tokenizer.eos_token
                generator.tokenizer.pad_token_id = generator.tokenizer.eos_token_id
            generator.tokenizer.model_max_length = cfg.model.max_input_len
        
        if hasattr(generator, 'model'):
            if hasattr(generator.model.config, 'pad_token_id') and generator.model.config.pad_token_id is None:
                generator.model.config.pad_token_id = generator.tokenizer.pad_token_id
        print(f"âœ… Tokenizer ä¿®æ­£å®Œæˆ")
    
    else:
        print(f"âŒ æœªæ”¯æŒçš„ MODEL_SOURCE: {model_source}")
        return

    # è¯»å–æµ‹è¯•æ•°æ®
    with open(test_file, "r") as f:
        test_dataset_raw = [json.loads(line) for line in f]

    acc_baseline = 0
    acc_rag = 0

    # æ ¼å¼åŒ– Prompt è¾…åŠ©å‡½æ•°
    def format_base_prompt(system_text, user_text):
        if model_source == "gemini":
            return f"{system_text}\n\n{user_text}" if system_text else user_text
        prompt = ""
        if system_text: prompt += f"{system_text}\n\n"
        prompt += f"### Question:\n{user_text}\n\n### Answer:\nLet's think step by step."
        return prompt

    # --- Task A: Baseline ---
    if cfg.experiment.mode in ['baseline', 'all']:
        print("\nâš”ï¸ [Task A] æ­£åœ¨è¿è¡Œ Baseline ...")
        
        baseline_inputs = []
        for item in test_dataset_raw:
            sys_msg = "You are a math expert. Solve the problem in a brief. Don't answer more than 50 words.End your answer with \\boxed{number}."
            formatted_prompt = format_base_prompt(sys_msg, item['question'])
            baseline_inputs.append(formatted_prompt)

        baseline_preds = generator.generate(baseline_inputs)
        
        baseline_results = []
        for item, pred in zip(test_dataset_raw, baseline_preds):
            baseline_results.append({
                "question": item['question'],
                "golden_answers": item['golden_answers'],
                "pred": pred
            })
        acc_baseline = evaluate_results(baseline_results, "Baseline (No RAG)", result_log_file)

    # --- Task B: FlashRAG ---
    if cfg.experiment.mode in ['rag', 'all']:
        print("\nâš”ï¸ [Task B] æ­£åœ¨è¿è¡Œ FlashRAG (Few-shot Retrieval)...")
        
        # å‡†å¤‡ RAG é…ç½®
        rag_config_dict = OmegaConf.to_container(cfg, resolve=True) # ä»…ä»…æ˜¯ä¸ºäº†è·å–ä¸€äº›åŸºç¡€ç±»å‹
        # å°† FlashRAG éœ€è¦çš„ç‰¹å®šå­—æ®µè¦†ç›–è¿›å»
        rag_update = {
            "data_dir": root_dir,
            "save_dir": cfg.paths.rag_cache_dir,
            "retrieval_method": cfg.experiment.retrieval_method,
            "corpus_path": corpus_file,
            "index_path": index_dir,
            "retriever_model_path": index_dir,
            "topk": cfg.experiment.retrieval_topk,
            # Generator é…ç½®ç»§æ‰¿ä¹‹å‰çš„
            "device": cfg.model.device,
            "generator_model_path": config['generator_model_path'] if 'generator_model_path' in config else "gpt2"
        }
        
        # é‡æ–°å®ä¾‹åŒ– Config ä»¥ç¡®ä¿ Retriever èƒ½è¯»åˆ°æ­£ç¡®å‚æ•°
        rag_config = Config(config_dict=rag_update)
        retriever = get_retriever(rag_config)
        
        rag_system_part = cfg.experiment.prompts.rag_system
        
        prompt_tpl = PromptTemplate(rag_config, system_prompt=rag_system_part, user_prompt="")
        pipeline = SequentialPipeline(rag_config, prompt_template=prompt_tpl, retriever=retriever, generator=generator)
        dataset_obj = Dataset(rag_config, test_file)
        
        rag_results = pipeline.run(dataset_obj)
        
        acc_rag = evaluate_results(rag_results, f"FlashRAG ({dataset_tag} Memory)", result_log_file)
        
        # ç»Ÿè®¡è®°å¿†çƒ­åº¦ (ä¼ å…¥ cfg)
        analyze_memory_usage(rag_results, cfg, corpus_file, vis_image_file)

    # --- Summary ---
    if cfg.experiment.mode == 'all':
        summary = (
            f"\n{'='*20} æœ€ç»ˆå¯¹æ¯”ç»“æœ {'='*20}\n"
            f"ğŸ“Š æ•°æ®é›†: {cfg.experiment.dataset_name}\n"
            f"ğŸ¤– æ¨¡å‹: {model_source}\n"
            f"ğŸ“‰ Baseline: {acc_baseline:.2f}%\n"
            f"ğŸ“ˆ FlashRAG: {acc_rag:.2f}%\n"
            f"ğŸš€ æå‡: {acc_rag - acc_baseline:+.2f}%\n"
            f"{'='*50}\n"
        )
        print(summary)
        with open(result_log_file, "a", encoding="utf-8") as f:
            f.write(summary)

if __name__ == "__main__":
    main()