from transformers import AutoTokenizer, AutoModelForCausalLM
from omegaconf import DictConfig
from typing import List
import os
import torch
from utils.toolfunction import clean_special_chars
import logging
from tqdm import tqdm # [æ–°å¢]
import concurrent.futures
# [æ–°å¢] å±è”½ httpx å’Œ httpcore çš„ INFO æ—¥å¿—ï¼Œé˜²æ­¢åˆ·å±
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)

def init_llm(cfg: DictConfig):
    """åˆå§‹åŒ– LLM"""
    global GLOBAL_MODEL, GLOBAL_TOKENIZER, GLOBAL_SGLANG_CLIENT
    
    model_source = cfg.model.optimize

    if model_source == "gemini":
        api_key = os.environ.get("GEMINI_API_KEY")
        if api_key:
            import google.generativeai as genai
            genai.configure(api_key=api_key)
            print(f"ğŸ¤– [Init] Gemini API ({cfg.model.gemini_name}) å·²é…ç½®")
        else:
            print("âš ï¸ [Init] æœªæ£€æµ‹åˆ° GEMINI_API_KEYï¼ŒGemini ç›¸å…³åŠŸèƒ½ä¼šè¢«è·³è¿‡")
            
    elif model_source == "huggingface":
        hf_name = cfg.model.hf_name
        print(f"ğŸ“¥ [Init] æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹: {hf_name} ...")
        try:
            GLOBAL_TOKENIZER = AutoTokenizer.from_pretrained(hf_name, trust_remote_code=True)
            GLOBAL_MODEL = AutoModelForCausalLM.from_pretrained(
                hf_name,
                device_map="auto",
                torch_dtype="auto",
                trust_remote_code=True
            ).eval()
            
            # ğŸ”¥ [Critical Fix] æ‰¹é‡ç”Ÿæˆå¿…é¡»è®¾ç½® left padding
            GLOBAL_TOKENIZER.padding_side = 'left'
            if GLOBAL_TOKENIZER.pad_token is None:
                GLOBAL_TOKENIZER.pad_token = GLOBAL_TOKENIZER.eos_token
                GLOBAL_TOKENIZER.pad_token_id = GLOBAL_TOKENIZER.eos_token_id
            
            print(f"âœ… [Init] æœ¬åœ°æ¨¡å‹åŠ è½½å®Œæˆï¼(Padding side set to left)")
        except Exception as e:
            print(f"âŒ [Init] æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ’¡ æç¤º: è¯·æ£€æŸ¥ HuggingFace æƒé™å’Œç½‘ç»œ")

    elif model_source == "sglang":
        try:
            from openai import OpenAI
            # ä»é…ç½®è¯»å– URLï¼Œé»˜è®¤æœ¬åœ°ç«¯å£
            api_url = cfg.model.get("sglang_api_url", "http://127.0.0.1:30000/v1")
            api_key = "EMPTY" # SGLang æœ¬åœ°éƒ¨ç½²ä¸éœ€è¦çœŸå® Key
            
            GLOBAL_SGLANG_CLIENT = OpenAI(base_url=api_url, api_key=api_key)
            print(f"âœ… [Init] SGLang Client å·²è¿æ¥è‡³ {api_url}")
        except ImportError:
            print("âŒ [Init] ç¼ºå°‘ openai åº“ï¼Œè¯·è¿è¡Œ `pip install openai`")

def call_llm(prompt: str, cfg: DictConfig, max_new_tokens: int = None, verbose: bool = True) -> str:
    """
    ç»Ÿä¸€çš„å¤§æ¨¡å‹è°ƒç”¨æ¥å£ï¼Œå•æ¡è°ƒç”¨
    æ–°å¢ verbose å‚æ•°ï¼šTrue=æ‰“å°è¿›åº¦(é»˜è®¤), False=é™é»˜æ¨¡å¼(ç”¨äºBatch)
    """
    model_source = cfg.model.optimize
    if max_new_tokens is None:
        max_new_tokens = cfg.model.max_new_tokens

    # --- Gemini ---
    if model_source == "gemini":
        if not os.environ.get("GEMINI_API_KEY"):
            return "Skipped (No GEMINI_API_KEY)"
        try:
            import google.generativeai as genai
            model = genai.GenerativeModel(cfg.model.gemini_name)
            if verbose:
                print(" ğŸ¤– [Gemini] æ­£åœ¨ç”Ÿæˆ...", end="", flush=True)
            resp = model.generate_content(prompt)
            if verbose:
                print(" å®Œæˆ")
            return clean_special_chars(resp.text.strip())
        except Exception as e:
            if verbose: print(f"\nâŒ [Gemini Error]: {e}")
            return ""

    # --- HuggingFace æœ¬åœ° ---
    elif model_source == "huggingface":
        if GLOBAL_MODEL is None:
            if verbose: print("âš ï¸ [Local] LLM å°šæœªåˆå§‹åŒ–")
            return ""

        try:
            if verbose:
                print(" ğŸš€ [Local] æ­£åœ¨ç”Ÿæˆ...", end="", flush=True)
            messages = [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt}
            ]
            text = GLOBAL_TOKENIZER.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            model_inputs = GLOBAL_TOKENIZER(
                [text],
                return_tensors="pt",
                truncation=True,
                max_length=cfg.model.max_input_len,
            ).to(GLOBAL_MODEL.device)

            with torch.no_grad():
                generated_ids = GLOBAL_MODEL.generate(
                    model_inputs.input_ids,
                    attention_mask=model_inputs.attention_mask,
                    max_new_tokens=max_new_tokens,
                    do_sample=False
                )
            # åªå–æ–°å¢çš„éƒ¨åˆ†
            generated_ids = [
                output_ids[len(input_ids):]
                for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
            ]
            response = GLOBAL_TOKENIZER.batch_decode(generated_ids, skip_special_tokens=True)[0]
            if verbose:
                print(" å®Œæˆ")
            return clean_special_chars(response.strip())
        except Exception as e:
            if verbose: print(f"\nâŒ [Local Error]: {e}")
            return ""

    # --- SGLang ---
    elif model_source == "sglang":
        if GLOBAL_SGLANG_CLIENT is None:
            return "Skipped (Client Not Initialized)"
        
        model_name = cfg.model.get("sglang_model_name", "Qwen/Qwen3-4B-Instruct-2507")
        try:
            if verbose:
                print(" ğŸš€ [SGLang] æ­£åœ¨æ¨ç†...", end="", flush=True)
            
            resp = GLOBAL_SGLANG_CLIENT.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.0,
                max_tokens=max_new_tokens
            )
            content = resp.choices[0].message.content
            
            if verbose:
                print(" å®Œæˆ")
            return clean_special_chars(content.strip())
        except Exception as e:
            if verbose: print(f"\nâŒ [SGLang Error]: {e}")
            return ""

    return ""


def call_llm_batch(prompts: List[str], cfg: DictConfig, max_new_tokens: int = None) -> List[str]:
    """æ‰¹é‡è°ƒç”¨ LLM (SGLang å¹¶å‘ä¼˜åŒ– + è¿›åº¦æ¡ç‰ˆ)"""
    if not prompts:
        return []
    
    model_source = cfg.model.optimize
    if max_new_tokens is None:
        max_new_tokens = cfg.model.max_new_tokens

    # --- SGLang å¹¶å‘åŠ é€Ÿ (å¸¦ tqdm è¿›åº¦æ¡) ---
    if model_source == "sglang":
        max_workers = cfg.model.get("batch_size", 32)
        
        results = [None] * len(prompts)
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘ï¼Œå¹¶ä¼ å…¥ verbose=False ç¦æ­¢å†…éƒ¨ print
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_idx = {
                executor.submit(call_llm, p, cfg, max_new_tokens, verbose=False): i 
                for i, p in enumerate(prompts)
            }
            
            # ğŸ”¥ ä¿®æ”¹ç‚¹ï¼šä½¿ç”¨ tqdm åŒ…è£¹è¿­ä»£å™¨ï¼Œæ˜¾ç¤ºè¿›åº¦æ¡
            for future in tqdm(concurrent.futures.as_completed(future_to_idx), total=len(prompts), desc="ğŸš€ SGLang Batch"):
                idx = future_to_idx[future]
                try:
                    results[idx] = future.result()
                except Exception as e:
                    # åªæœ‰å‡ºé”™æ‰æ‰“å°
                    print(f"\nâŒ [Batch Error] Task {idx} failed: {e}")
                    results[idx] = ""
        
        return results

    # --- Gemini (ä¿æŒä¸²è¡Œ) ---
    if model_source == "gemini":
        results = []
        # Gemini ä¹Ÿå¯ä»¥åŠ ä¸ªç®€å•çš„è¿›åº¦æ¡ï¼Œå¦‚æœéœ€è¦çš„è¯
        for p in tqdm(prompts, desc="ğŸ¤– Gemini Batch"):
            results.append(call_llm(p, cfg, max_new_tokens=max_new_tokens))
        return results

    # --- HuggingFace æœ¬åœ° (HFæœ¬èº«æ”¯æŒBatchæ¨ç†ï¼Œé€»è¾‘ä¸å˜) ---
    if model_source == "huggingface":
        if GLOBAL_MODEL is None:
            print("âš ï¸ [Local] LLM å°šæœªåˆå§‹åŒ–")
            return [""] * len(prompts)

        try:
            print(f" ğŸš€ [Local-Batch] æ­£åœ¨æ‰¹é‡ç”Ÿæˆ {len(prompts)} æ¡...", end="", flush=True)
            
            messages_list = [
                [
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": p}
                ]
                for p in prompts
            ]
            text_list = [
                GLOBAL_TOKENIZER.apply_chat_template(
                    msgs,
                    tokenize=False,
                    add_generation_prompt=True
                )
                for msgs in messages_list
            ]

            model_inputs = GLOBAL_TOKENIZER(
                text_list,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=cfg.model.max_input_len,
            ).to(GLOBAL_MODEL.device)

            with torch.no_grad():
                generated_ids = GLOBAL_MODEL.generate(
                    model_inputs.input_ids,
                    attention_mask=model_inputs.attention_mask,
                    max_new_tokens=max_new_tokens,
                    do_sample=False
                )

            results = []
            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids):
                new_token_ids = output_ids[len(input_ids):]
                text = GLOBAL_TOKENIZER.decode(new_token_ids, skip_special_tokens=True)
                results.append(clean_special_chars(text.strip()))
            print(" å®Œæˆ")
            return results

        except Exception as e:
            print(f"\nâŒ [Local-Batch Error]: {e}")
            return [""] * len(prompts)

    return [""] * len(prompts)