from transformers import AutoTokenizer, AutoModelForCausalLM
from omegaconf import DictConfig
from typing import List
import os
import torch
from utils.toolfunction import clean_special_chars


def init_llm(cfg: DictConfig):
    """åˆå§‹åŒ– LLM"""
    global GLOBAL_MODEL, GLOBAL_TOKENIZER, GLOBAL_SGLANG_CLIENT
    
    model_source = cfg.model.optimize

    if model_source == "gemini":
        api_key = os.environ.get("GEMINI_API_KEY")
        if api_key:
            import google.generativeai as genai
            genai.configure(api_key=api_key)
            print(f"ğŸ¤– [Init] Gemini API ({cfg.model.gemini_name}) å·²é…ç½®")
        else:
            print("âš ï¸ [Init] æœªæ£€æµ‹åˆ° GEMINI_API_KEYï¼ŒGemini ç›¸å…³åŠŸèƒ½ä¼šè¢«è·³è¿‡")
            
    elif model_source == "huggingface":
        hf_name = cfg.model.hf_name
        print(f"ğŸ“¥ [Init] æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹: {hf_name} ...")
        try:
            GLOBAL_TOKENIZER = AutoTokenizer.from_pretrained(hf_name, trust_remote_code=True)
            GLOBAL_MODEL = AutoModelForCausalLM.from_pretrained(
                hf_name,
                device_map="auto",
                torch_dtype="auto",
                trust_remote_code=True
            ).eval()
            
            # ğŸ”¥ [Critical Fix] æ‰¹é‡ç”Ÿæˆå¿…é¡»è®¾ç½® left padding
            GLOBAL_TOKENIZER.padding_side = 'left'
            if GLOBAL_TOKENIZER.pad_token is None:
                GLOBAL_TOKENIZER.pad_token = GLOBAL_TOKENIZER.eos_token
                GLOBAL_TOKENIZER.pad_token_id = GLOBAL_TOKENIZER.eos_token_id
            
            print(f"âœ… [Init] æœ¬åœ°æ¨¡å‹åŠ è½½å®Œæˆï¼(Padding side set to left)")
        except Exception as e:
            print(f"âŒ [Init] æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ’¡ æç¤º: è¯·æ£€æŸ¥ HuggingFace æƒé™å’Œç½‘ç»œ")

    elif model_source == "sglang":
        try:
            from openai import OpenAI
            # ä»é…ç½®è¯»å– URLï¼Œé»˜è®¤æœ¬åœ°ç«¯å£
            api_url = cfg.model.get("sglang_api_url", "http://127.0.0.1:30000/v1")
            api_key = "EMPTY" # SGLang æœ¬åœ°éƒ¨ç½²ä¸éœ€è¦çœŸå® Key
            
            GLOBAL_SGLANG_CLIENT = OpenAI(base_url=api_url, api_key=api_key)
            print(f"âœ… [Init] SGLang Client å·²è¿æ¥è‡³ {api_url}")
        except ImportError:
            print("âŒ [Init] ç¼ºå°‘ openai åº“ï¼Œè¯·è¿è¡Œ `pip install openai`")

def call_llm(prompt: str, cfg: DictConfig, max_new_tokens: int = None) -> str:
    """ç»Ÿä¸€çš„å¤§æ¨¡å‹è°ƒç”¨æ¥å£ï¼Œå•æ¡è°ƒç”¨"""
    model_source = cfg.model.optimize
    # å¦‚æœæ²¡ä¼  max_new_tokensï¼Œå°±ç”¨ config é‡Œçš„é»˜è®¤å€¼
    if max_new_tokens is None:
        max_new_tokens = cfg.model.max_new_tokens

    # --- Gemini ---
    if model_source == "gemini":
        if not os.environ.get("GEMINI_API_KEY"):
            return "Skipped (No GEMINI_API_KEY)"
        try:
            import google.generativeai as genai
            model = genai.GenerativeModel(cfg.model.gemini_name)
            print("  ğŸ¤– [Gemini] æ­£åœ¨ç”Ÿæˆ...", end="", flush=True)
            resp = model.generate_content(prompt)
            print(" å®Œæˆ")
            return clean_special_chars(resp.text.strip())
        except Exception as e:
            print(f"\nâŒ [Gemini Error]: {e}")
            return ""

    # --- HuggingFace æœ¬åœ° ---
    elif model_source == "huggingface":
        if GLOBAL_MODEL is None:
            print("âš ï¸ [Local] LLM å°šæœªåˆå§‹åŒ–")
            return ""

        try:
            print("  ğŸš€ [Local] æ­£åœ¨ç”Ÿæˆ...", end="", flush=True)
            messages = [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt}
            ]
            text = GLOBAL_TOKENIZER.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            model_inputs = GLOBAL_TOKENIZER(
                [text],
                return_tensors="pt",
                truncation=True,
                max_length=cfg.model.max_input_len,
            ).to(GLOBAL_MODEL.device)

            with torch.no_grad():
                generated_ids = GLOBAL_MODEL.generate(
                    model_inputs.input_ids,
                    attention_mask=model_inputs.attention_mask,
                    max_new_tokens=max_new_tokens,
                    do_sample=False
                )
            # åªå–æ–°å¢çš„éƒ¨åˆ†
            generated_ids = [
                output_ids[len(input_ids):]
                for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
            ]
            response = GLOBAL_TOKENIZER.batch_decode(generated_ids, skip_special_tokens=True)[0]
            print(" å®Œæˆ")
            return clean_special_chars(response.strip())
        except Exception as e:
            print(f"\nâŒ [Local Error]: {e}")
            return ""

    # --- SGLang ---
    elif model_source == "sglang":
        if GLOBAL_SGLANG_CLIENT is None:
            return "Skipped (Client Not Initialized)"
        
        model_name = cfg.model.get("sglang_model_name", "Qwen/Qwen3-4B-Instruct-2507")
        try:
            print("  ğŸš€ [SGLang] æ­£åœ¨æ¨ç†...", end="", flush=True)
            resp = GLOBAL_SGLANG_CLIENT.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.0,
                max_tokens=max_new_tokens
            )
            content = resp.choices[0].message.content
            print(" å®Œæˆ")
            return clean_special_chars(content.strip())
        except Exception as e:
            print(f"\nâŒ [SGLang Error]: {e}")
            return ""

    return ""

def call_llm_batch(prompts: List[str], cfg: DictConfig, max_new_tokens: int = None) -> List[str]:
    """æ‰¹é‡è°ƒç”¨ LLM"""
    if not prompts:
        return []
    
    model_source = cfg.model.optimize
    if max_new_tokens is None:
        max_new_tokens = cfg.model.max_new_tokens

    # Geminiï¼šç®€å•å¾ªç¯
    if model_source == "gemini":
        results = []
        for p in prompts:
            results.append(call_llm(p, cfg, max_new_tokens=max_new_tokens))
        return results

    # SGLang: ç®€å•å¾ªç¯è°ƒç”¨ (Serverç«¯ä¼šè‡ªåŠ¨å¤„ç†å¹¶å‘)
    if model_source == "sglang":
        results = []
        # è™½ç„¶è¿™é‡Œå†™çš„æ˜¯å¾ªç¯ï¼Œä½† SGLang Server çš„ååå¾ˆé«˜ï¼Œé€Ÿåº¦é€šå¸¸æ¯”æœ¬åœ° HF Batch å¿«
        # å¦‚æœéœ€è¦æè‡´å¹¶å‘ï¼Œå¯ä»¥ä½¿ç”¨ asyncio æˆ– ThreadPoolExecutorï¼Œä½†ç®€å•å¾ªç¯é€šå¸¸è¶³å¤Ÿå¿«ä¸”ç¨³å®š
        for p in prompts:
            results.append(call_llm(p, cfg, max_new_tokens=max_new_tokens))
        return results

    # HuggingFace æœ¬åœ°
    if model_source == "huggingface":
        if GLOBAL_MODEL is None:
            print("âš ï¸ [Local] LLM å°šæœªåˆå§‹åŒ–")
            return [""] * len(prompts)

        try:
            print(f"  ğŸš€ [Local-Batch] æ­£åœ¨æ‰¹é‡ç”Ÿæˆ {len(prompts)} æ¡...", end="", flush=True)
            
            messages_list = [
                [
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": p}
                ]
                for p in prompts
            ]
            text_list = [
                GLOBAL_TOKENIZER.apply_chat_template(
                    msgs,
                    tokenize=False,
                    add_generation_prompt=True
                )
                for msgs in messages_list
            ]

            # æ‰¹é‡ Tokenize + Padding
            model_inputs = GLOBAL_TOKENIZER(
                text_list,
                return_tensors="pt",
                padding=True, # å…³é”®
                truncation=True,
                max_length=cfg.model.max_input_len,
            ).to(GLOBAL_MODEL.device)

            with torch.no_grad():
                generated_ids = GLOBAL_MODEL.generate(
                    model_inputs.input_ids,
                    attention_mask=model_inputs.attention_mask,
                    max_new_tokens=max_new_tokens,
                    do_sample=False
                )

            results = []
            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids):
                new_token_ids = output_ids[len(input_ids):]
                text = GLOBAL_TOKENIZER.decode(new_token_ids, skip_special_tokens=True)
                results.append(clean_special_chars(text.strip()))
            print(" å®Œæˆ")
            return results

        except Exception as e:
            print(f"\nâŒ [Local-Batch Error]: {e}")
            return [""] * len(prompts)

    return [""] * len(prompts)