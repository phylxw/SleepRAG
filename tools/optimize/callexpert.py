import os
import concurrent.futures
from typing import List
from omegaconf import DictConfig
from utils.toolfunction import clean_special_chars

# å®šä¹‰ä¸€ä¸ªå…¨å±€çš„ä¸“å®¶å®¢æˆ·ç«¯
GLOBAL_EXPERT_CLIENT = None

def init_expert_llm(cfg: DictConfig):
    """åˆå§‹åŒ–ä¸“å®¶æ¨¡å‹ (Teacher Model)"""
    global GLOBAL_EXPERT_CLIENT
    expert_cfg = cfg.expert_model
    source = expert_cfg.source
    
    print(f"ğŸ‘¨â€ğŸ« [Expert-Init] æ­£åœ¨åˆå§‹åŒ–ä¸“å®¶æ¨¡å‹: {source} ({expert_cfg.name})...")

    if source == "gemini":
        try:
            import google.generativeai as genai
            api_key = os.environ.get("EXPERT_API_KEY") or os.environ.get("GEMINI_API_KEY")
            genai.configure(api_key=api_key)
            GLOBAL_EXPERT_CLIENT = genai.GenerativeModel(expert_cfg.name)
            print(f"âœ… [Expert-Init] Gemini ({expert_cfg.name}) å°±ç»ª")
        except ImportError:
            print("âŒ [Expert-Init] ç¼ºå°‘ google-generativeai åº“")

    elif source in ["openai", "sglang"]:
        try:
            from openai import OpenAI
            # SGLang ä¹Ÿæ˜¯ç”¨ OpenAI å®¢æˆ·ç«¯
            base_url = os.environ.get("EXPERT_BASE_URL", "https://api.openai.com/v1")
            api_key = os.environ.get("EXPERT_API_KEY")
            
            if source == "sglang":
                base_url = expert_cfg.get("sglang_api_url", "http://127.0.0.1:30000/v1")
                api_key = "EMPTY"

            GLOBAL_EXPERT_CLIENT = OpenAI(base_url=base_url, api_key=api_key)
            print(f"âœ… [Expert-Init] {source.upper()} Client ({expert_cfg.name}) å°±ç»ª")
        except ImportError:
            print("âŒ [Expert-Init] ç¼ºå°‘ openai åº“")
    else:
        print(f"âš ï¸ [Expert-Init] æœªçŸ¥çš„ä¸“å®¶æº: {source}")


def call_expert(prompt: str, cfg: DictConfig) -> str:
    """å•æ¡è°ƒç”¨ (å†…éƒ¨é€»è¾‘ä¿æŒä¸å˜ï¼Œä¾› Batch è°ƒç”¨ä½¿ç”¨)"""
    global GLOBAL_EXPERT_CLIENT
    if GLOBAL_EXPERT_CLIENT is None: return None

    source = cfg.expert_model.source
    model_name = cfg.expert_model.name
    
    try:
        if source == "gemini":
            resp = GLOBAL_EXPERT_CLIENT.generate_content(prompt)
            return clean_special_chars(resp.text.strip())
        
        elif source in ["openai", "sglang"]:
            resp = GLOBAL_EXPERT_CLIENT.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": "You are a helpful and critical AI expert."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                # SGLang ä¸“å®¶é€šå¸¸éœ€è¦é•¿ä¸€ç‚¹çš„è¾“å‡ºç©ºé—´å†™åˆ†æ
                max_tokens=1024 
            )
            return clean_special_chars(resp.choices[0].message.content.strip())

    except Exception as e:
        print(f"âŒ [Expert Error]: {e}")
        return None

import concurrent.futures
from typing import List
from omegaconf import DictConfig
import logging
from tqdm import tqdm  # è®°å¾— pip install tqdm

# 1. å±è”½ httpx å’Œ httpcore çš„ INFO æ—¥å¿—ï¼Œé˜²æ­¢åˆ·å±
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)

def call_expert_batch(prompts: List[str], cfg: DictConfig) -> List[str]:
    """
    ğŸ”¥ [New] æ‰¹é‡å¹¶å‘è°ƒç”¨ä¸“å®¶æ¨¡å‹ (å¸¦è¿›åº¦æ¡ + æ—¥å¿—å±è”½)
    """
    if not prompts: return []
    
    source = cfg.expert_model.source
    
    # 1. å¦‚æœæ˜¯ SGLang/OpenAIï¼Œä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘ (æé€Ÿ + è¿›åº¦æ¡)
    if source in ["sglang", "openai"]:
        # å¹¶å‘æ•°å¯ä»¥è®¾å¤§ä¸€ç‚¹ï¼ŒSGLang å¤„ç†å¾—è¿‡æ¥
        max_workers = 16 
        
        results = [None] * len(prompts)
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤ä»»åŠ¡ï¼Œå»ºç«‹ Future -> Index çš„æ˜ å°„
            future_to_idx = {
                executor.submit(call_expert, p, cfg): i 
                for i, p in enumerate(prompts)
            }
            
            # ä½¿ç”¨ tqdm åŒ…è£¹ as_completedï¼Œæ˜¾ç¤ºå®æ—¶è¿›åº¦
            for future in tqdm(concurrent.futures.as_completed(future_to_idx), total=len(prompts), desc="ğŸ§  Expert Batch"):
                idx = future_to_idx[future]
                try:
                    results[idx] = future.result()
                except Exception as e:
                    print(f"\nâŒ [Expert Batch Error] Task {idx} failed: {e}")
                    results[idx] = ""
                    
        return results

    # 2. å¦‚æœæ˜¯ Geminiï¼Œä¿æŒç®€å•å¾ªç¯ (é˜²æ­¢ 429)ï¼Œä½†åŠ ä¸Šè¿›åº¦æ¡
    results = []
    for p in tqdm(prompts, desc="ğŸ¤– Gemini Expert"):
        results.append(call_expert(p, cfg))
    return results