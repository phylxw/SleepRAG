import os
import concurrent.futures
from typing import List
from omegaconf import DictConfig
from utils.toolfunction import clean_special_chars

# å®šä¹‰ä¸€ä¸ªå…¨å±€çš„ä¸“å®¶å®¢æˆ·ç«¯
GLOBAL_EXPERT_CLIENT = None

def init_expert_llm(cfg: DictConfig):
    """åˆå§‹åŒ–ä¸“å®¶æ¨¡å‹ (Teacher Model)"""
    global GLOBAL_EXPERT_CLIENT
    expert_cfg = cfg.expert_model
    source = expert_cfg.source
    
    print(f"ğŸ‘¨â€ğŸ« [Expert-Init] æ­£åœ¨åˆå§‹åŒ–ä¸“å®¶æ¨¡å‹: {source} ({expert_cfg.name})...")

    if source == "gemini":
        try:
            import google.generativeai as genai
            api_key = os.environ.get("EXPERT_API_KEY") or os.environ.get("GEMINI_API_KEY")
            genai.configure(api_key=api_key)
            GLOBAL_EXPERT_CLIENT = genai.GenerativeModel(expert_cfg.name)
            print(f"âœ… [Expert-Init] Gemini ({expert_cfg.name}) å°±ç»ª")
        except ImportError:
            print("âŒ [Expert-Init] ç¼ºå°‘ google-generativeai åº“")

    elif source in ["openai", "sglang"]:
        try:
            from openai import OpenAI
            # SGLang ä¹Ÿæ˜¯ç”¨ OpenAI å®¢æˆ·ç«¯
            base_url = os.environ.get("EXPERT_BASE_URL", "https://api.openai.com/v1")
            api_key = os.environ.get("EXPERT_API_KEY")
            
            if source == "sglang":
                base_url = expert_cfg.get("sglang_api_url", "http://127.0.0.1:30000/v1")
                api_key = "EMPTY"

            GLOBAL_EXPERT_CLIENT = OpenAI(base_url=base_url, api_key=api_key)
            print(f"âœ… [Expert-Init] {source.upper()} Client ({expert_cfg.name}) å°±ç»ª")
        except ImportError:
            print("âŒ [Expert-Init] ç¼ºå°‘ openai åº“")
    else:
        print(f"âš ï¸ [Expert-Init] æœªçŸ¥çš„ä¸“å®¶æº: {source}")


def call_expert(prompt: str, cfg: DictConfig) -> str:
    """å•æ¡è°ƒç”¨ (å†…éƒ¨é€»è¾‘ä¿æŒä¸å˜ï¼Œä¾› Batch è°ƒç”¨ä½¿ç”¨)"""
    global GLOBAL_EXPERT_CLIENT
    if GLOBAL_EXPERT_CLIENT is None: return None

    source = cfg.expert_model.source
    model_name = cfg.expert_model.name
    
    try:
        if source == "gemini":
            resp = GLOBAL_EXPERT_CLIENT.generate_content(prompt)
            return clean_special_chars(resp.text.strip())
        
        elif source in ["openai", "sglang"]:
            resp = GLOBAL_EXPERT_CLIENT.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": "You are a helpful and critical AI expert."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                # SGLang ä¸“å®¶é€šå¸¸éœ€è¦é•¿ä¸€ç‚¹çš„è¾“å‡ºç©ºé—´å†™åˆ†æ
                max_tokens=1024 
            )
            return clean_special_chars(resp.choices[0].message.content.strip())

    except Exception as e:
        print(f"âŒ [Expert Error]: {e}")
        return None

def call_expert_batch(prompts: List[str], cfg: DictConfig) -> List[str]:
    """
    ğŸ”¥ [New] æ‰¹é‡å¹¶å‘è°ƒç”¨ä¸“å®¶æ¨¡å‹
    å¯¹äº SGLang/OpenAIï¼Œä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘å‘é€è¯·æ±‚ï¼ŒæœåŠ¡ç«¯ä¼šè‡ªåŠ¨ Batch å¤„ç†ã€‚
    """
    if not prompts: return []
    
    source = cfg.expert_model.source
    
    # 1. å¦‚æœæ˜¯ SGLang/OpenAIï¼Œä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘ (è¿™æ˜¯æé€Ÿçš„å…³é”®ï¼)
    if source in ["sglang", "openai"]:
        # å¹¶å‘æ•°å¯ä»¥è®¾å¤§ä¸€ç‚¹ï¼Œæ¯”å¦‚ 16 æˆ– 32ï¼ŒSGLang å¤„ç†å¾—è¿‡æ¥
        max_workers = 16 
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            futures = [executor.submit(call_expert, p, cfg) for p in prompts]
            # è·å–ç»“æœ (ä¿æŒé¡ºåº)
            results = [f.result() for f in futures]
        return results

    # 2. å¦‚æœæ˜¯ Geminiï¼Œè€ƒè™‘åˆ°é€Ÿç‡é™åˆ¶ (Rate Limit)ï¼Œå»ºè®®ä¸²è¡Œæˆ–ä¿å®ˆå¹¶å‘
    # è¿™é‡Œä¿æŒç®€å•å¾ªç¯ï¼Œé¿å… 429 Error
    results = []
    for p in prompts:
        results.append(call_expert(p, cfg))
    return results