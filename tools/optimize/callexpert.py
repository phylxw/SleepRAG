import os
import concurrent.futures
from typing import List
from omegaconf import DictConfig
from utils.toolfunction import clean_special_chars
import logging
from tqdm import tqdm

# å®šä¹‰ä¸€ä¸ªå…¨å±€çš„ä¸“å®¶å®¢æˆ·ç«¯
GLOBAL_EXPERT_CLIENT = None

def init_expert_llm(cfg: DictConfig):
    """åˆå§‹åŒ–ä¸“å®¶æ¨¡å‹ (Teacher Model)"""
    global GLOBAL_EXPERT_CLIENT
    expert_cfg = cfg.expert_model
    source = expert_cfg.source
    
    print(f"ğŸ‘¨â€ğŸ« [Expert-Init] æ­£åœ¨åˆå§‹åŒ–ä¸“å®¶æ¨¡å‹: {source} ({expert_cfg.name})...")

    if source == "gemini":
        try:
            import google.generativeai as genai
            api_key = os.environ.get("EXPERT_API_KEY") or os.environ.get("GEMINI_API_KEY")
            genai.configure(api_key=api_key)
            GLOBAL_EXPERT_CLIENT = genai.GenerativeModel(expert_cfg.name)
            print(f"âœ… [Expert-Init] Gemini ({expert_cfg.name}) å°±ç»ª")
        except ImportError:
            print("âŒ [Expert-Init] ç¼ºå°‘ google-generativeai åº“")

    # ğŸ”¥ [ä¿®æ”¹ç‚¹ 1] å°† qwen åŠ å…¥åˆ° openai å…¼å®¹åˆ—è¡¨
    elif source in ["openai", "sglang", "qwen"]:
        try:
            from openai import OpenAI
            # é»˜è®¤é…ç½® (OpenAI)
            base_url = os.environ.get("EXPERT_BASE_URL", "https://api.openai.com/v1")
            api_key = os.environ.get("EXPERT_API_KEY")
            
            # é’ˆå¯¹ SGLang çš„ç‰¹æ®Šé…ç½®
            if source == "sglang":
                base_url = expert_cfg.get("sglang_api_url", "http://127.0.0.1:30000/v1")
                api_key = "EMPTY"
            
            # ğŸ”¥ [ä¿®æ”¹ç‚¹ 2] é’ˆå¯¹ Qwen (DashScope) çš„ç‰¹æ®Šé…ç½®
            elif source == "qwen":
                # é˜¿é‡Œäº‘ç™¾ç‚¼å…¼å®¹æ¨¡å¼ endpoint
                base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
                # ä¼˜å…ˆè¯»ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEYï¼Œå¦‚æœæ²¡æœ‰åˆ™è¯» EXPERT_API_KEYï¼Œæœ€åæ‰æ˜¯ç¡¬ç¼–ç ï¼ˆä¸æ¨èï¼‰
                api_key = "sk-dab5c76d636e4e4b9567b0c45d73ba83"
                
                # å¦‚æœç¯å¢ƒå˜é‡æ²¡è®¾ï¼Œä¸ºäº†æ–¹ä¾¿è°ƒè¯•ï¼Œä½ å¯ä»¥æš‚æ—¶ç”¨è¿™é‡Œçš„ç¡¬ç¼–ç  (ä½†ç”Ÿäº§ç¯å¢ƒè¯·åˆ æ‰)
                if not api_key:
                    api_key = "sk-dab5c76d636e4e4b9567b0c45d73ba83" # ä½ çš„ Key

            GLOBAL_EXPERT_CLIENT = OpenAI(base_url=base_url, api_key=api_key)
            print(f"âœ… [Expert-Init] {source.upper()} Client ({expert_cfg.name}) å°±ç»ª | URL: {base_url}")
        except ImportError:
            print("âŒ [Expert-Init] ç¼ºå°‘ openai åº“")
    else:
        print(f"âš ï¸ [Expert-Init] æœªçŸ¥çš„ä¸“å®¶æº: {source}")


def call_expert(prompt: str, cfg: DictConfig) -> str:
    """å•æ¡è°ƒç”¨"""
    global GLOBAL_EXPERT_CLIENT
    if GLOBAL_EXPERT_CLIENT is None: return None

    source = cfg.expert_model.source
    model_name = cfg.expert_model.name
    
    try:
        if source == "gemini":
            resp = GLOBAL_EXPERT_CLIENT.generate_content(prompt)
            return clean_special_chars(resp.text.strip())
        
        # ğŸ”¥ [ä¿®æ”¹ç‚¹ 3] Qwen ä¹Ÿèµ°è¿™é‡Œï¼Œä½†æ³¨æ„ï¼šè¿™é‡Œä¸ä½¿ç”¨ stream=True
        # å› ä¸ºåœ¨ä»£ç é€»è¾‘ä¸­ï¼Œæˆ‘ä»¬éœ€è¦å®Œæ•´çš„å­—ç¬¦ä¸²è¿”å›ï¼Œè€Œä¸æ˜¯ç”Ÿæˆå™¨
        elif source in ["openai", "sglang", "qwen"]:
            resp = GLOBAL_EXPERT_CLIENT.chat.completions.create(
                model=model_name, # è¿™é‡Œä¼šä¼ å…¥ qwen-max æˆ– qwen3-max
                messages=[
                    {"role": "system", "content": "You are a helpful and critical AI expert."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                # æ³¨æ„ï¼šå¦‚æœ Qwen æŠ¥é”™ "max_tokens too large"ï¼Œå¯ä»¥é€‚å½“è°ƒå°æˆ–æ³¨é‡Šæ‰
                # qwen-max æ”¯æŒé•¿æ–‡æœ¬ï¼Œä¸€èˆ¬æ²¡é—®é¢˜
                # max_tokens=1024, 
                stream=False  # âŒ å…³æ‰æµå¼ï¼Œæ–¹ä¾¿åç»­å¤„ç†
            )
            return clean_special_chars(resp.choices[0].message.content.strip())

    except Exception as e:
        print(f"âŒ [Expert Error]: {e}")
        return None


# 1. å±è”½ httpx å’Œ httpcore çš„ INFO æ—¥å¿—ï¼Œé˜²æ­¢åˆ·å±
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)

def call_expert_batch(prompts: List[str], cfg: DictConfig) -> List[str]:
    """
    ğŸ”¥ æ‰¹é‡å¹¶å‘è°ƒç”¨ä¸“å®¶æ¨¡å‹
    """
    if not prompts: return []
    
    source = cfg.expert_model.source
    
    # ğŸ”¥ [ä¿®æ”¹ç‚¹ 4] å…è®¸ Qwen è¿›è¡Œå¹¶å‘
    if source in ["sglang", "openai", "qwen"]:
        # Qwen çš„å¹¶å‘é™åˆ¶ï¼š
        # å¦‚æœæ˜¯æ™®é€šè´¦å·ï¼ŒQwen-max çš„å¹¶å‘ (QPS) å¯èƒ½è¾ƒä½ã€‚
        # å¦‚æœæŠ¥é”™ 429 Too Many Requestsï¼Œè¯·æŠŠ max_workers æ”¹å° (ä¾‹å¦‚ 2 æˆ– 4)
        max_workers = 16 
        
        results = [None] * len(prompts)
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_idx = {
                executor.submit(call_expert, p, cfg): i 
                for i, p in enumerate(prompts)
            }
            
            for future in tqdm(concurrent.futures.as_completed(future_to_idx), total=len(prompts), desc=f"ğŸ§  {source.upper()} Batch"):
                idx = future_to_idx[future]
                try:
                    results[idx] = future.result()
                except Exception as e:
                    print(f"\nâŒ [Expert Batch Error] Task {idx} failed: {e}")
                    results[idx] = ""
                    
        return results

    # Gemini ä¿æŒåŸæœ‰é€»è¾‘
    results = []
    for p in tqdm(prompts, desc="ğŸ¤– Gemini Expert"):
        results.append(call_expert(p, cfg))
    return results