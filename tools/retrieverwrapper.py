import numpy as np
import math

# ==========================================
# ğŸ”¥ BEMR æ£€ç´¢åŒ…è£…å™¨ (ä¿®å¤ç‰ˆ)
# ==========================================
class BEMRRetrieverWrapper:
    """
    BEMR æ£€ç´¢åŒ…è£…å™¨ï¼šæ‹¦æˆªåŸå§‹æ£€ç´¢ç»“æœï¼Œåº”ç”¨ UCB å…¬å¼é‡æ’åº
    å¹¶å¼ºåˆ¶æ‰§è¡Œæˆªæ–­ï¼Œè§£å†³ FlashRAG æç¤ºè¯è¿‡é•¿é—®é¢˜ã€‚
    """
    def __init__(self, original_retriever, memory_stats, cfg):
        self.retriever = original_retriever
        self.memory_stats = memory_stats
        self.cfg = cfg
        self.INIT_VAL = cfg.parameters.INIT_VAL
        
        # ğŸ”¥ [ä¿®æ­£1] å¼ºåˆ¶æˆªæ–­é˜ˆå€¼ (Small-K)
        # ä¼˜å…ˆè¯» parameters.final_topk (3)ï¼Œè¯»ä¸åˆ°å°±ç”¨é»˜è®¤å€¼ 3
        if hasattr(cfg, 'parameters'):
            self.final_topk = cfg.parameters.get("final_topk", 3)
        else:
            self.final_topk = 3

        self.lambda1 = cfg.parameters.get('bemr_lambda1', 1.0)
        self.lambda2 = cfg.parameters.get('bemr_lambda2', 0.5)
        
        print(f"ğŸ›¡ï¸ [Wrapper] BEMR æ‹¦æˆªå™¨å°±ç»ª | æœ€ç»ˆæˆªæ–­: Top-{self.final_topk}")

    def _calculate_ucb_score(self, doc_id, sim_score):
        stats = self.memory_stats.get(str(doc_id), {'alpha': self.INIT_VAL , 'beta': self.INIT_VAL})
        alpha = stats['alpha']
        beta = stats['beta']
        total = alpha + beta
        
        mean_utility = alpha / total
        exploration = math.sqrt(math.log(max(total, 1)) / total)
        
        # --- æ ¸å¿ƒå…¬å¼ ---
        # Part A: UCB ä¸»å¯¼éƒ¨åˆ† (æ•ˆç”¨ + æ¢ç´¢)
        ucb_part = (self.lambda1 * mean_utility) + (self.lambda2 * exploration)
        
        # Part B: BM25 å¾®å¼±å½±å“ (Tie-Breaker)
        # 0.001 çš„æƒé‡è¶³ä»¥åœ¨ UCB ç›¸åŒæ—¶åŒºåˆ†é«˜ä¸‹ï¼Œä½†ä¸è¶³ä»¥è®© BM25 å¹²æ‰° UCB çš„åˆ¤æ–­
        bm25_part = 0.001 * sim_score
        
        final_score = ucb_part + bm25_part
        return final_score, ucb_part  # ğŸ”¥ è¿”å›ä¸¤ä¸ªå€¼ï¼Œæ–¹ä¾¿ Debug æ˜¾ç¤ºçº¯ UCB åˆ†æ•°

    # ğŸ”¥ [ä¿®æ­£2] ç­¾åå¿…é¡»å®Œå…¨åŒ¹é… FlashRAG çš„ Retriever æ¥å£
    def search(self, query_list, num=None, return_score=False):
        # ==========================================
        # 1. ç¡®å®šæµ·é€‰æ•°é‡ (Funnel Stage 1)
        # ==========================================
        # å³ä½¿ FlashRAG åªæˆ‘ä»¬è¦ 3 æ¡ï¼Œæˆ‘ä»¬ä¹Ÿè¦å…ˆæŠ“ 20 æ¡å›æ¥æŒ‘ï¼
        INITIAL_POOL_SIZE = 20 
        search_k = max(num if num else 0, INITIAL_POOL_SIZE)
        
        # 2. è°ƒç”¨åº•å±‚ batch_search
        raw_output = self.retriever.batch_search(query_list, num=search_k, return_score=True)
        
        if isinstance(raw_output, tuple):
            batch_hits, batch_scores = raw_output
        else:
            batch_hits = raw_output
            batch_scores = [[0.0] * len(h) for h in batch_hits]

        reranked_results = []
        reranked_scores = []

        # éå†æ¯ä¸€ä¸ª Query
        for q_idx, (hit_list, score_list) in enumerate(zip(batch_hits, batch_scores)):
            
            # --- ğŸ“Š [Debug å‡†å¤‡] ---
            debug_info = [] 
            
            # å½’ä¸€åŒ–å‡†å¤‡
            if not score_list:
                reranked_results.append([])
                reranked_scores.append([])
                continue
                
            min_s, max_s = min(score_list), max(score_list)
            denominator = max_s - min_s if (max_s - min_s) > 1e-6 else 1.0
            
            scored_hits = []
            
            # --- å¾ªç¯å¤„ç† 20 ä¸ªå€™é€‰è®°å¿† ---
            for i, hit in enumerate(hit_list):
                doc_id = hit.get('id')
                raw_bm25 = score_list[i] 
                
                # å½’ä¸€åŒ– BM25 (0~1)
                norm_bm25 = (raw_bm25 - min_s) / denominator
                
                # è®¡ç®—åˆ†æ•° (è·å– Final å’Œ çº¯ UCB)
                final_score, pure_ucb = self._calculate_ucb_score(doc_id, norm_bm25)
                
                # å†™å…¥æ–°åˆ†æ•°
                hit['score'] = final_score
                scored_hits.append(hit)
                
                # è·å–çŠ¶æ€ç”¨äºå±•ç¤º
                stats = self.memory_stats.get(str(doc_id), {'alpha': self.INIT_VAL, 'beta': self.INIT_VAL})
                
                # å­˜å…¥ Debug åˆ—è¡¨
                debug_info.append({
                    "id": doc_id,
                    "bm25_raw": raw_bm25,
                    "bm25_norm": norm_bm25,
                    "pure_ucb": pure_ucb,    # çº¯ UCB åˆ†æ•° (ä¸å« 0.001*BM25)
                    "final_score": final_score, # æœ€ç»ˆæ’åºä¾æ®
                    "stats": f"{stats['alpha']:.1f}/{stats['beta']:.1f}"
                })

            # --- æ’åº ---
            # æŒ‰ Final Score é™åº
            scored_hits.sort(key=lambda x: x['score'], reverse=True)
            
            # --- æˆªæ–­ (Top-K) ---
            cutoff = self.final_topk
            if num and num < self.final_topk:
                cutoff = num
            truncated_hits = scored_hits[:cutoff]
            truncated_scores = [h['score'] for h in truncated_hits]
            
            reranked_results.append(truncated_hits)
            reranked_scores.append(truncated_scores)

            # ==========================================
            # ğŸ•µï¸â€â™‚ï¸ [æ˜¾å¾®é•œ] æ‰“å°è¯¦ç»†æ’ä½è¡¨
            # ==========================================
            # print(f"\nğŸ” [Query {q_idx+1}] æ£€ç´¢è¯¦æƒ…ç›‘æ§ (Top-{search_k} -> Top-{cutoff})")
            # # è¡¨å¤´æ ¼å¼åŒ–ï¼šå¢åŠ äº† Pure UCB å’Œ Final Score
            # print(f"{'Rank':<5} | {'ID':<6} | {'BM25(Raw)':<10} | {'Status(A/B)':<12} | {'Pure UCB':<10} | {'Final Score':<11} | {'Result'}")
            # print("-" * 88)
            
            # å¿…é¡»è®© debug_info ä¹ŸæŒ‰ Final Score æ’åºï¼Œæ‰èƒ½å’Œ Rank å¯¹åº”ä¸Š
            debug_info.sort(key=lambda x: x['final_score'], reverse=True)
            
            # for rank, info in enumerate(debug_info):
            #     is_selected = "âœ… PICK" if rank < cutoff else "âŒ DROP"
                
            #     # æ‰“å°å‰5å å’Œ å2å
            #     if rank < 20 or rank >= len(debug_info) - 2: 
            #         print(f"{rank+1:<5} | {info['id']:<6} | {info['bm25_raw']:<10.2f} | {info['stats']:<12} | {info['pure_ucb']:<10.4f} | {info['final_score']:<11.4f} | {is_selected}")
            #     elif rank == 5:
            #         print(f"{'...':<5} | {'...':<6} | {'...':<10} | {'...':<12} | {'...':<10} | {'...':<11} | ...")
            # print("=" * 88)
            # # ==========================================

        if return_score:
            return reranked_results, reranked_scores
        else:
            return reranked_results
        
    def batch_search(self, query_list, num=None, return_score=False):
        """
        å¼ºåˆ¶é‡å®šå‘ batch_search åˆ°æˆ‘ä»¬éœ€è¦æ‰§è¡Œ UCB é€»è¾‘çš„ search æ–¹æ³•
        """
        print(f"âœ… [Wrapper] æˆåŠŸæ‹¦æˆª batch_searchè¯·æ±‚ï¼Œè½¬å…¥ BEMR é€»è¾‘å¤„ç†...")
        return self.search(query_list, num, return_score)

    def __getattr__(self, name):
        # å»ºè®®ä¿ç•™è¿™ä¸ª print ç”¨äºç›‘æ§æœªæ¥æ˜¯å¦è¿˜æœ‰å…¶ä»–æ–¹æ³•æ³„éœ²
        print(f"âš ï¸ [Wrapper Bypass] æ­£åœ¨é€ä¼ æ–¹æ³•: {name}")
        return getattr(self.retriever, name)