import requests
import re
from concurrent.futures import ThreadPoolExecutor
import os

class CodeEvaluator:
    def __init__(self, server_url="http://localhost:8080", max_workers=16):
        self.server_url = server_url
        self.max_workers = max_workers

    def extract_python_code(self, text: str) -> str:
        """ä» Markdown ä¸­æå– Python ä»£ç """
        pattern = r"```python(.*?)```"
        match = re.search(pattern, text, re.DOTALL)
        if match:
            # ğŸ”¥ æ ¸å¿ƒä¿®æ­£ï¼šåªå»é™¤é¦–å°¾æ¢è¡Œï¼Œä¿ç•™ç¼©è¿›
            return match.group(1).strip('\n')
        return text.strip('\n')

    def evaluate_one(self, dataset_type, pred_str, task_data):
        """
        å•æ¡è¯„æµ‹é€»è¾‘
        """
        code_body = self.extract_python_code(pred_str)
        full_code = ""
        
        if dataset_type == 'humaneval':
            # HumanEval: Prompt + Code + Test
            entry_point = task_data.get('entry_point', 'candidate')
            test_code = task_data.get('test', '')
            full_code = f"{task_data['prompt']}\n{code_body}\n\n{test_code}"

        elif dataset_type == 'mbpp':
            # MBPP: Setup + Code + Test List
            # æ³¨æ„å…¼å®¹æ€§ï¼šæœ‰äº›æ•°æ®å¯èƒ½æ²¡æœ‰ test_setup_code
            test_list = task_data.get('test_list', [])
            setup_code = task_data.get('test_setup_code', "")
            tests_str = "\n".join(test_list)
            full_code = f"{setup_code}\n{code_body}\n\n{tests_str}"
            
        else:
            return 0.0

        try:
            resp = requests.post(f"{self.server_url}/run_code", json={
                'code': full_code,
                'language': 'python'
            }, timeout=10) # å»ºè®®ä¿æŒ 10s é…åˆæœåŠ¡ç«¯é™åˆ¶
            
            if resp.status_code == 200:
                res_json = resp.json()
                if res_json.get('status') == 'Success':
                    return 1.0
            return 0.0
        except Exception as e:
            # ç”Ÿäº§ç¯å¢ƒå¯ä»¥é€‰æ‹©æ‰“å° log æˆ–å¿½ç•¥
            # print(f"âš ï¸ [Eval Error] {e}") 
            return 0.0

    def evaluate_batch(self, dataset_type, pred_list, task_data_list):
        """
        âš¡ æ‰¹é‡å¹¶å‘è¯„æµ‹ (ä¾› sglang æ¨¡å¼ä½¿ç”¨)
        """
        print(f"âš–ï¸ [CodeEval] æ­£åœ¨å¹¶å‘è¯„æµ‹ {len(pred_list)} æ¡ä»£ç  (Workers={self.max_workers})...")
        
        scores = []
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = []
            for pred, item in zip(pred_list, task_data_list):
                futures.append(executor.submit(self.evaluate_one, dataset_type, pred, item))
            
            for future in futures:
                scores.append(future.result())
                
        return scores

def evaluate_code_results(results, experiment_name, result_log_file, dataset_type="humaneval", server_url="http://localhost:8080"):
    """
    ğŸ”¥ æ–°å¢å‡½æ•°ï¼šä¸“é—¨ç”¨äºè¯„æµ‹ä»£ç  (HumanEval/MBPP)
    ç‰¹ç‚¹ï¼š
    1. å†…éƒ¨ä½¿ç”¨ CodeEvaluator.evaluate_batch å®ç°å¹¶å‘åŠ é€Ÿ
    2. æ—¥å¿—æ ¼å¼ä¸æ•°å­¦è¯„æµ‹å®Œå…¨ä¿æŒä¸€è‡´
    """
    # 1. åˆå§‹åŒ–è¯„æµ‹å™¨
    evaluator = CodeEvaluator(server_url=server_url, max_workers=16) # è¿™é‡Œçš„ workers å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´
    
    # 2. å‡†å¤‡æ•°æ® (æ‰¹é‡æå–)
    # æ³¨æ„ï¼šCodeEvaluator éœ€è¦ raw data (dict å½¢å¼) æ¥è·å– test_list ç­‰å­—æ®µ
    # å¦‚æœ results æ˜¯ FlashRAG çš„å¯¹è±¡ï¼Œé€šå¸¸å¯ä»¥å°† item è½¬ä¸º dict æˆ–ç›´æ¥ä¼ 
    task_data_list = []
    preds_list = []
    
    for item in results:
        # å…¼å®¹æ€§å¤„ç†ï¼šå¦‚æœæ˜¯å¯¹è±¡åˆ™è½¬ dictï¼Œå¦‚æœæ˜¯ dict åˆ™ç›´æ¥ç”¨
        data_dict = item.__dict__ if hasattr(item, '__dict__') else item
        task_data_list.append(data_dict)
        
        # è·å–é¢„æµ‹å€¼
        pred = item.pred if hasattr(item, 'pred') else item.get('pred', "")
        preds_list.append(pred)

    # 3. ğŸ”¥ å¹¶å‘æ‰¹é‡è¯„æµ‹ (è¿™æ˜¯é€Ÿåº¦çš„å…³é”®)
    print(f"ğŸš€ [Eval] æ­£åœ¨å¹¶å‘è¯„æµ‹ {len(results)} æ¡ä»£ç æ•°æ® ({dataset_type})...")
    scores = evaluator.evaluate_batch(dataset_type, preds_list, task_data_list)
    
    # 4. ç»Ÿè®¡ä¸æ—¥å¿—è®°å½• (æ¨¡ä»¿ evaluate_math_results çš„é£æ ¼)
    correct = 0
    total = len(results)
    
    os.makedirs(os.path.dirname(result_log_file), exist_ok=True)

    with open(result_log_file, "a", encoding="utf-8") as f:
        header = f"\n{'='*20} {experiment_name} (Code) {'='*20}\n"
        print(header.strip()) 
        f.write(header)
        
        for i, (item, score, pred) in enumerate(zip(results, scores, preds_list)):
            is_right = (score == 1.0)
            if is_right:
                correct += 1
            
            # è·å–é—®é¢˜ç”¨äºå±•ç¤º (Code ä»»åŠ¡é€šå¸¸ prompt å°±æ˜¯ question)
            # å°è¯•è·å– prompt, text æˆ– question å­—æ®µ
            q_text = ""
            if hasattr(item, 'prompt'): q_text = item.prompt
            elif hasattr(item, 'text'): q_text = item.text # MBPP
            elif isinstance(item, dict): q_text = item.get('prompt', item.get('text', ""))
            
            # ä¸ºäº†æ—¥å¿—å¥½çœ‹ï¼Œæå–çº¯ä»£ç éƒ¨åˆ†å±•ç¤º
            extracted_code = evaluator.extract_python_code(pred)
            
            # [cite_start]æ—¥å¿—è®°å½• [cite: 19, 20]
            log_entry = (
                f"\n[ID]: {i}\n"
                f"[Question/Prompt]: {str(q_text)[:80]}...\n" # é˜²æ­¢ Prompt å¤ªé•¿
                f"[Pred Extracted]: \n{extracted_code[:200]}...\n" # åªå±•ç¤ºä»£ç å‰200å­—ç¬¦
                f"[Result]: {'âœ… Correct' if is_right else 'âŒ Wrong (Pass@1)'}\n"
                f"{'-'*30}\n"
            )
            f.write(log_entry)
            
            # æ§åˆ¶å°é¢„è§ˆå‰ 3 æ¡
            if i < 3:
                print(log_entry.strip())

        # [cite_start]ç»Ÿè®¡æœ€ç»ˆå‡†ç¡®ç‡ [cite: 21]
        acc = correct / total * 100 if total > 0 else 0
        summary = (
            f"\nğŸ“Š ç»Ÿè®¡ ({experiment_name}):\n"
            f"Dataset: {dataset_type.upper()}\n"
            f"Total: {total}, Correct: {correct}, Accuracy (Pass@1): {acc:.2f}%\n"
            f"{'='*50}\n"
        )
        print(summary)
        f.write(summary)

    return acc

# ç®€å•çš„è‡ªæµ‹å…¥å£ï¼Œç¡®è®¤æ–‡ä»¶æ²¡æ‹·é”™
if __name__ == "__main__":
    print("ğŸš€ [Self-Test] å¼€å§‹è°ƒè¯•...")
    evaluator = CodeEvaluator(server_url="http://localhost:8080")

    # 1. HumanEval æµ‹è¯•
    he_task_data = {
        "prompt": "def multiply(a, b):",
        "entry_point": "multiply",
        "test": "assert multiply(2, 3) == 6"
    }
    # æ³¨æ„ï¼šè¿™é‡Œçš„ return å‰é¢æœ‰4ä¸ªç©ºæ ¼
    he_pred = """```python
    return a * b
```"""
    
    print("\n-------------------------------------")
    print("ğŸ§ª æµ‹è¯•åœºæ™¯ 1: HumanEval")
    score_he = evaluator.evaluate_one("humaneval", he_pred, he_task_data)
    print(f"â¡ï¸ ç»“æœ: {score_he}")

    # 2. MBPP æµ‹è¯•
    print("\n-------------------------------------")
    print("ğŸ§ª æµ‹è¯•åœºæ™¯ 2: MBPP")
    mbpp_task_data = {
        "test_setup_code": "import math",
        "test_list": ["assert get_sqrt(4) == 2.0"]
    }
    mbpp_pred = "def get_sqrt(n): return math.sqrt(n)"
    score_mbpp = evaluator.evaluate_one("mbpp", mbpp_pred, mbpp_task_data)
    print(f"â¡ï¸ ç»“æœ: {score_mbpp}")