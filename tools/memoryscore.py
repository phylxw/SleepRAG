
import os
from omegaconf import DictConfig, OmegaConf
import json
from tqdm import tqdm
from tools.evaluate import judge_math_item
import matplotlib.pyplot as plt
from tools.score.bemr import _calculate_bemr_final_score
import copy

def _load_memory_corpus(corpus_file: str):
    """è¾…åŠ©å‡½æ•°ï¼šè¯»å–è®°å¿†åº“æ–‡ä»¶"""
    all_memory_ids = set()
    id_to_content = {} 
    try:
        with open(corpus_file, "r", encoding="utf-8") as f:
            for line in f:
                item = json.loads(line)
                mid = str(item['id'])
                all_memory_ids.add(mid)
                id_to_content[mid] = item.get("contents", "")
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è¯»å–è®°å¿†åº“æ–‡ä»¶ {corpus_file}ï¼Œé”™è¯¯: {e}")
    return all_memory_ids, id_to_content

def _calculate_scores(rag_results, all_memory_ids, cfg: DictConfig, old_stats=None):
    """
    ä¿®æ”¹ç‰ˆï¼šåŸºäº BEMR (Bayesian-EM Memory Refinement) è®¡ç®—åˆ†æ•°
    åŠŸèƒ½ï¼š
    1. ç»§æ‰¿ä¸Šä¸€è½®çŠ¶æ€ (æŒç»­å­¦ä¹ )
    2. æ›´æ–° Alpha/Beta (è´å¶æ–¯æ›´æ–°)
    3. æ•è·å¯¼è‡´é”™è¯¯çš„ Query (ä½œä¸º TextGrad çš„æ¢¯åº¦)
    """
    
    # 1. ç»§æ‰¿æˆ–åˆå§‹åŒ–ç»Ÿè®¡é‡
    if old_stats:
        # æ·±æ‹·è´ä»¥é˜²ä¿®æ”¹åŸå¼•ç”¨
        memory_stats = copy.deepcopy(old_stats)
        # è¡¥é½å¯èƒ½æ–°å¢çš„è®°å¿† ID (é˜²æ­¢ Key Error)
        for mid in all_memory_ids:
            if mid not in memory_stats:
                memory_stats[mid] = {'alpha': 1.0, 'beta': 1.0, 'pos_queries': [], 'neg_queries': []}
    else:
        # å†·å¯åŠ¨ï¼šå…¨éƒ¨åˆå§‹åŒ–ä¸º Prior (1.0, 1.0)
        memory_stats = {mid: {'alpha': 1.0, 'beta': 1.0, 'pos_queries': [], 'neg_queries': []} for mid in all_memory_ids}

    correct_count = 0
    
    # 2. éå†ç»“æœæ›´æ–°çŠ¶æ€
    for item in tqdm(rag_results, desc="Scoring & Capturing Gradients (BEMR)"):
        # å‡è®¾ judge_math_item åœ¨å¤–éƒ¨ä½œç”¨åŸŸå¯ç”¨
        is_correct, _, _ = judge_math_item(item)
        if is_correct: correct_count += 1

        # è·å–å½“å‰ Query (è¿™æ˜¯ TextGrad çš„â€œæ¢¯åº¦â€æ¥æº)
        current_query = getattr(item, 'question', '')

        retrieved_docs = getattr(item, 'retrieval_result', [])
        
        for doc in retrieved_docs:
            doc_id = str(doc.get('id')) if isinstance(doc, dict) else str(getattr(doc, 'id', None))
            
            # åªè¦ doc_id å­˜åœ¨äºæˆ‘ä»¬çš„åº“ä¸­ï¼Œå°±è¿›è¡Œæ›´æ–°
            if doc_id and doc_id in memory_stats:
                if is_correct:
                    # âœ… ç­”å¯¹ï¼šAlpha + 1
                    memory_stats[doc_id]['alpha'] += 1.0
                    # [E-Step] è®°å½•æ­£æ ·æœ¬ (ç”¨äºä¿®æ­£ Key)
                    if current_query and current_query not in memory_stats[doc_id]['pos_queries']:
                        memory_stats[doc_id]['pos_queries'].append(current_query)
                else:
                    # âŒ ç­”é”™ï¼šBeta + 1
                    memory_stats[doc_id]['beta'] += 1.0
                    # [TextGrad] è®°å½•è´Ÿæ ·æœ¬ (ç”¨äºä¿®æ­£ Content) -> è¿™å°±æ˜¯æ¢¯åº¦ï¼
                    if current_query and current_query not in memory_stats[doc_id]['neg_queries']:
                        memory_stats[doc_id]['neg_queries'].append(current_query)

    # 3. è®¡ç®—ç”¨äºå¯è§†åŒ–çš„æ ‡é‡åˆ†æ•° (Mean Utility)
    # æ³¨æ„ï¼šmemory_stats æ‰æ˜¯æˆ‘ä»¬è¦å­˜ç›˜çš„æ ¸å¿ƒæ•°æ®ï¼Œfinal_scores_map åªæ˜¯ç»™ print/vis ç”¨çš„
    final_scores_map = {}
    for mid, stats in memory_stats.items():
        # è¿™é‡Œè®¡ç®—ç®€å•çš„å‡å€¼ç”¨äºçƒ­åº¦å±•ç¤º: alpha / (alpha + beta)
        # ä½ ä¹Ÿå¯ä»¥è°ƒç”¨ _calculate_bemr_final_score ç®— UCB åˆ†æ•°
        total = stats['alpha'] + stats['beta']
        score = stats['alpha'] / total if total > 0 else 0.5
        final_scores_map[mid] = score
    
    # è¿”å›ä¸‰ä¸ªå€¼ï¼šå¯è§†åŒ–åˆ†æ•°è¡¨ï¼Œå®Œæ•´çš„ç»Ÿè®¡çŠ¶æ€ï¼Œæ­£ç¡®æ•°
    return final_scores_map, memory_stats, correct_count

def _print_stats_and_save(memory_scores, id_to_content, total_questions, correct_count, freq_file ,is_write = True):
    """è¾…åŠ©å‡½æ•°ï¼šæ‰“å°ç»Ÿè®¡ä¿¡æ¯å¹¶ä¿å­˜ JSONL ç»“æœ"""
    # æ’åº (æŒ‰åˆ†æ•°ä»é«˜åˆ°ä½)
    sorted_memories = sorted(memory_scores.items(), key=lambda x: (-x[1], x[0]))
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_mem = len(sorted_memories)
    positive_mem = sum(1 for _, v in sorted_memories if v > 0.51)
    negative_mem = sum(1 for _, v in sorted_memories if v < 0.49)
    zero_mem = sum(1 for _, v in sorted_memories if v < 0.51 and v > 0.49)
    
    print(f"ğŸ“Š è®°å¿†åº“è¯„åˆ†ç»Ÿè®¡:")
    print(f"   - æ€»é‡: {total_mem}")
    print(f"   - æ­£åˆ†(è´¡çŒ®è€…): {positive_mem} ({(positive_mem/total_mem)*100:.1f}%)")
    print(f"   - è´Ÿåˆ†(å¹²æ‰°é¡¹): {negative_mem} ({(negative_mem/total_mem)*100:.1f}%)")
    print(f"   - é›¶åˆ†(å†·é—¨): {zero_mem}")
    print(correct_count)
    print(total_questions)
    print(f"   - å½“å‰é¢˜ç›®æ­£ç¡®ç‡: {correct_count/total_questions*100:.2f}%")

    if is_write :
        # å¯¼å‡º jsonl
        try:
            print(f"ğŸ’¾ [Save] æ­£åœ¨å¯¼å‡ºè®°å¿†è¯„åˆ†ç»“æœåˆ°: {freq_file}")
            os.makedirs(os.path.dirname(freq_file), exist_ok=True)
            
            with open(freq_file, "w", encoding="utf-8") as f:
                for rank, (mid, score) in enumerate(sorted_memories, start=1):
                    record = {
                        "rank": rank,
                        "memory_id": mid,
                        "freq": round(score, 3), # ğŸ”¥ è¿™é‡Œå­˜çš„æ˜¯åˆ†æ•°
                        "contents": id_to_content.get(mid, "")
                    }
                    f.write(json.dumps(record, ensure_ascii=False) + "\n")
            print("âœ… è¯„åˆ†æ–‡ä»¶å¯¼å‡ºå®Œæˆï¼")
        except Exception as e:
            print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")
        
    return sorted_memories

def _visualize_results(cfg: DictConfig, sorted_memories, vis_image_file: str):
    """è¾…åŠ©å‡½æ•°ï¼šç”Ÿæˆåˆ†æ•°åˆ†å¸ƒå›¾"""
    if cfg.experiment.visualize_memory:
        print(f"ğŸ¨ [Visual] æ­£åœ¨ç”Ÿæˆåˆ†æ•°åˆ†å¸ƒå›¾: {vis_image_file}")
        try:
            ids = [m[0] for m in sorted_memories]
            scores = [m[1] for m in sorted_memories]
            
            display_limit = 30
            if len(ids) > display_limit * 2:
                plot_ids = ids[:display_limit] + ["..."] + ids[-display_limit:]
                plot_scores = scores[:display_limit] + [0] + scores[-display_limit:]
                # é¢œè‰²åŒºåˆ†
                colors = []
                for s in plot_scores:
                    if s > 0: colors.append('skyblue')
                    elif s < 0: colors.append('salmon')
                    else: colors.append('lightgrey')
            else:
                plot_ids = ids
                plot_scores = scores
                colors = ['skyblue' if s > 0 else 'salmon' if s < 0 else 'lightgrey' for s in plot_scores]

            plt.figure(figsize=(15, 6))
            plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
            
            bars = plt.bar(plot_ids, plot_scores, color=colors, edgecolor='navy')
            plt.title(f'Memory  Score', fontsize=14)
            plt.ylabel('Score')
            plt.xticks(rotation=90, fontsize=8) 
            
            # æ˜¾ç¤ºæ•°å€¼
            for i, bar in enumerate(bars):
                height = bar.get_height()
                if plot_ids[i] != "...": 
                    y_pos = height if height >= 0 else height - (max(scores)*0.05)
                    va = 'bottom' if height >= 0 else 'top'
                    plt.text(bar.get_x() + bar.get_width()/2., y_pos, f'{int(height*1000)/1000}',
                             ha='center', va=va, fontsize=8)
            
            plt.tight_layout()
            plt.savefig(vis_image_file, dpi=300)
            print("âœ… å›¾ç‰‡ä¿å­˜æˆåŠŸï¼")
        except ImportError:
            print("âŒ ç¼ºå°‘ matplotlib")
    else:
        print("\nğŸ† [Top 10 High-Utility Memories]")
        for mid, score in sorted_memories[:10]:
            print(f"   ID: {mid:<5} | Score: {score}")