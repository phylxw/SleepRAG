
import os
from omegaconf import DictConfig, OmegaConf
import json
from tqdm import tqdm
from tools.evaluate import judge_math_item
import matplotlib.pyplot as plt
from tools.score.bemr import _calculate_bemr_final_score
import copy


def _load_memory_corpus(corpus_file: str):
    """è¾…åŠ©å‡½æ•°ï¼šè¯»å–è®°å¿†åº“æ–‡ä»¶"""
    all_memory_ids = set()
    id_to_content = {} 
    try:
        with open(corpus_file, "r", encoding="utf-8") as f:
            for line in f:
                item = json.loads(line)
                mid = str(item['id'])
                all_memory_ids.add(mid)
                id_to_content[mid] = item.get("contents", "")
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è¯»å–è®°å¿†åº“æ–‡ä»¶ {corpus_file}ï¼Œé”™è¯¯: {e}")
    return all_memory_ids, id_to_content

def _calculate_scores(rag_results, all_memory_ids, cfg, old_stats=None, baseline_scores=None):
    """
    ä¿®æ”¹ç‰ˆï¼šæ”¯æŒ Counterfactual Update (å·®åˆ†æ›´æ–° / è¾¹é™…æ•ˆç”¨)
    """
    INIT_VAL = cfg.parameters.INIT_VAL
    # 1. ç»§æ‰¿æˆ–åˆå§‹åŒ–ç»Ÿè®¡é‡ (ä¸å˜)
    if old_stats:
        memory_stats = copy.deepcopy(old_stats)
        # ç¡®ä¿æ‰€æœ‰ memory_id éƒ½åœ¨ stats é‡Œ
        for mid in all_memory_ids:
            if mid not in memory_stats:
                memory_stats[mid] = {'alpha': INIT_VAL, 'beta': INIT_VAL, 'pos_queries': [], 'neg_queries': []}
    else:
        memory_stats = {mid: {'alpha': INIT_VAL, 'beta': INIT_VAL, 'pos_queries': [], 'neg_queries': []} for mid in all_memory_ids}

    correct_count = 0
    
    # 2. éå†ç»“æœæ›´æ–°çŠ¶æ€ (æ³¨æ„ï¼šä½¿ç”¨ enumerate è·å–ç´¢å¼• i)
    for i, item in enumerate(tqdm(rag_results, desc="Scoring & Capturing Gradients (BEMR)")):
        
        # --- A. è·å– RAG æ­£ç¡®æ€§ (With Memory) ---
        if cfg.experiment.tag in ["humaneval", "mbpp"]:
            is_rag_correct = (item.score == 1.0)
        else:
            try:
                is_rag_correct, _, _ = judge_math_item(item)
            except Exception:
                is_rag_correct = False
        
        if is_rag_correct: correct_count += 1

        # --- B. ğŸ”¥ è·å– Baseline æ­£ç¡®æ€§ (Without Memory) ---
        if baseline_scores and i < len(baseline_scores):
            # Baseline çš„åˆ†æ•°å¦‚æœæ˜¯ 1.0 ä¹Ÿå°±æ˜¯å¯¹ï¼Œ0.0 æ˜¯é”™
            is_base_correct = (baseline_scores[i] == 1.0)
        else:
            # å¦‚æœæ²¡æœ‰æä¾› Baseline (ç¬¬ä¸€è½®æˆ–è¢«å…³æ‰)ï¼Œä¸ºäº†å®‰å…¨ï¼š
            # ç­–ç•¥1: å‡è®¾ Baseline å…¨é”™ -> é€€åŒ–å›æ—§ç®—æ³• (åªè¦ RAG å¯¹äº†å°±å¥–åŠ±)
            # ç­–ç•¥2: å‡è®¾ Baseline å…¨å¯¹ -> æå…¶ä¿å®ˆ (é™¤é RAG ä¹Ÿæ˜¯å¯¹çš„å¦åˆ™ä¸å¥–åŠ±)
            # è¿™é‡Œé€‰ç”¨ç­–ç•¥1ï¼Œä¿æŒå…¼å®¹æ€§
            is_base_correct = False 

        # --- C. æ„é€  TextGrad ç”¨çš„ Query ---
        q = getattr(item, 'question', '') or getattr(item, 'prompt', '') or ''
        q = q.strip()
        gold_list = getattr(item, 'golden_answers', [])
        a = gold_list[0] if gold_list else "No Answer Provided"
        current_query = f"[Question]: {q}\n   [Target Answer]: {str(a)[:500]}"

        # --- D. ğŸ”¥ æ›´æ–°è®°å¿†æƒé‡ (æ ¸å¿ƒé€»è¾‘) ---
        retrieved_docs = getattr(item, 'retrieval_result', [])
        
        for doc in retrieved_docs:
            doc_id = str(doc.get('id')) if isinstance(doc, dict) else str(getattr(doc, 'id', None))
            
            if doc_id and doc_id in memory_stats:
                
                # ğŸ”¥ğŸ”¥ğŸ”¥ [å·®åˆ†æ›´æ–°çœŸå€¼è¡¨] ğŸ”¥ğŸ”¥ğŸ”¥
                
                # Case 1: é›ªä¸­é€ç‚­ (Critical Success) [Baseé”™ -> RAGå¯¹]
                # è¿™æ˜¯æœ€å®è´µçš„è®°å¿†ï¼Œå¤§å¹…å¥–åŠ±
                if is_rag_correct and not is_base_correct:
                    memory_stats[doc_id]['alpha'] += 2.0  # å»ºè®®ç»™ 2.0 æˆ–æ›´é«˜ï¼ŒåŠ é€Ÿæ”¶æ•›
                    if current_query not in memory_stats[doc_id]['pos_queries']:
                        memory_stats[doc_id]['pos_queries'].append(current_query)
                
                # Case 2: å¸®å€’å¿™ (Toxic Failure) [Baseå¯¹ -> RAGé”™]
                # è¿™æ˜¯æœ€æœ‰å®³çš„è®°å¿†ï¼Œå¤§å¹…æƒ©ç½š
                elif not is_rag_correct and is_base_correct:
                    memory_stats[doc_id]['beta'] += 2.0   # ä¸¥å‰æƒ©ç½š
                    if current_query not in memory_stats[doc_id]['neg_queries']:
                        memory_stats[doc_id]['neg_queries'].append(current_query)
                
                # Case 3: é”¦ä¸Šæ·»èŠ± (Redundant) [Baseå¯¹ -> RAGå¯¹]
                # è¯´æ˜è¿™é¢˜å¾ˆç®€å•ï¼Œè®°å¿†å¯èƒ½æœ‰ç”¨ä¹Ÿå¯èƒ½æ²¡ç”¨ã€‚
                # ç»™äºˆå¾®å°å¥–åŠ±æˆ–ä¸å¥–åŠ±ï¼Œé˜²æ­¢â€œä¸‡é‡‘æ²¹â€è®°å¿†åˆ·åˆ†
                elif is_rag_correct and is_base_correct:
                    memory_stats[doc_id]['alpha'] += 0.05  # å¾®å°å¥–åŠ±ï¼Œç»´æŒæ´»è·ƒåº¦
                
                # Case 4: æ— èƒ½ä¸ºåŠ› (Useless) [Baseé”™ -> RAGé”™]
                # è®°å¿†æ²¡èµ·ä½œç”¨ï¼Œä½†ä¹Ÿæ²¡æŠŠæœ¬æ¥å¯¹çš„æé”™ã€‚
                # ç»™äºˆä¸­ç­‰æƒ©ç½šï¼Œå› ä¸ºå®ƒå ç”¨äº†æ£€ç´¢ä½ä½†æ²¡è§£å†³é—®é¢˜
                elif not is_rag_correct and not is_base_correct:
                    memory_stats[doc_id]['beta'] += 0.25
                    # ä¹Ÿå¯ä»¥åŠ å…¥è´Ÿæ ·æœ¬é˜Ÿåˆ—ï¼Œä¾› Expert åˆ†æâ€œä¸ºä»€ä¹ˆæ²¡å¸®ä¸Šå¿™â€
                    if current_query not in memory_stats[doc_id]['neg_queries']:
                        memory_stats[doc_id]['neg_queries'].append(current_query)

    # 5. è®¡ç®—æœ€ç»ˆæ ‡é‡åˆ†æ•°
    final_scores_map = {}
    for mid, stats in memory_stats.items():
        total = stats['alpha'] + stats['beta']
        # è®¡ç®— Beta åˆ†å¸ƒæœŸæœ›å€¼
        score = stats['alpha'] / total if total > 0 else 0.5
        final_scores_map[mid] = score
    
    return final_scores_map, memory_stats, correct_count

def _print_stats_and_save(memory_scores, id_to_content, total_questions, correct_count, freq_file ,is_write = True):
    """è¾…åŠ©å‡½æ•°ï¼šæ‰“å°ç»Ÿè®¡ä¿¡æ¯å¹¶ä¿å­˜ JSONL ç»“æœ"""
    # æ’åº (æŒ‰åˆ†æ•°ä»é«˜åˆ°ä½)
    sorted_memories = sorted(memory_scores.items(), key=lambda x: (-x[1], x[0]))
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_mem = len(sorted_memories)
    positive_mem = sum(1 for _, v in sorted_memories if v > 0.51)
    negative_mem = sum(1 for _, v in sorted_memories if v < 0.49)
    zero_mem = sum(1 for _, v in sorted_memories if v < 0.51 and v > 0.49)
    
    print(f"ğŸ“Š è®°å¿†åº“è¯„åˆ†ç»Ÿè®¡:")
    print(f"   - æ€»é‡: {total_mem}")
    print(f"   - æ­£åˆ†(è´¡çŒ®è€…): {positive_mem} ({(positive_mem/total_mem)*100:.1f}%)")
    print(f"   - è´Ÿåˆ†(å¹²æ‰°é¡¹): {negative_mem} ({(negative_mem/total_mem)*100:.1f}%)")
    print(f"   - é›¶åˆ†(å†·é—¨): {zero_mem}")
    print(correct_count)
    print(total_questions)
    print(f"   - å½“å‰é¢˜ç›®æ­£ç¡®ç‡: {correct_count/total_questions*100:.2f}%")

    if is_write :
        # å¯¼å‡º jsonl
        try:
            print(f"ğŸ’¾ [Save] æ­£åœ¨å¯¼å‡ºè®°å¿†è¯„åˆ†ç»“æœåˆ°: {freq_file}")
            os.makedirs(os.path.dirname(freq_file), exist_ok=True)
            
            with open(freq_file, "w", encoding="utf-8") as f:
                for rank, (mid, score) in enumerate(sorted_memories, start=1):
                    record = {
                        "rank": rank,
                        "memory_id": mid,
                        "freq": round(score, 3), # ğŸ”¥ è¿™é‡Œå­˜çš„æ˜¯åˆ†æ•°
                        "contents": id_to_content.get(mid, "")
                    }
                    f.write(json.dumps(record, ensure_ascii=False) + "\n")
            print("âœ… è¯„åˆ†æ–‡ä»¶å¯¼å‡ºå®Œæˆï¼")
        except Exception as e:
            print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")
        
    return sorted_memories

def _visualize_results(cfg: DictConfig, sorted_memories, vis_image_file: str):
    """è¾…åŠ©å‡½æ•°ï¼šç”Ÿæˆåˆ†æ•°åˆ†å¸ƒå›¾"""
    if cfg.experiment.visualize_memory:
        print(f"ğŸ¨ [Visual] æ­£åœ¨ç”Ÿæˆåˆ†æ•°åˆ†å¸ƒå›¾: {vis_image_file}")
        try:
            ids = [m[0] for m in sorted_memories]
            scores = [m[1] for m in sorted_memories]
            
            display_limit = 30
            if len(ids) > display_limit * 2:
                plot_ids = ids[:display_limit] + ["..."] + ids[-display_limit:]
                plot_scores = scores[:display_limit] + [0] + scores[-display_limit:]
                # é¢œè‰²åŒºåˆ†
                colors = []
                for s in plot_scores:
                    if s > 0: colors.append('skyblue')
                    elif s < 0: colors.append('salmon')
                    else: colors.append('lightgrey')
            else:
                plot_ids = ids
                plot_scores = scores
                colors = ['skyblue' if s > 0 else 'salmon' if s < 0 else 'lightgrey' for s in plot_scores]

            plt.figure(figsize=(15, 6))
            plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
            
            bars = plt.bar(plot_ids, plot_scores, color=colors, edgecolor='navy')
            plt.title(f'Memory  Score', fontsize=14)
            plt.ylabel('Score')
            plt.xticks(rotation=90, fontsize=8) 
            
            # æ˜¾ç¤ºæ•°å€¼
            for i, bar in enumerate(bars):
                height = bar.get_height()
                if plot_ids[i] != "...": 
                    y_pos = height if height >= 0 else height - (max(scores)*0.05)
                    va = 'bottom' if height >= 0 else 'top'
                    plt.text(bar.get_x() + bar.get_width()/2., y_pos, f'{int(height*1000)/1000}',
                             ha='center', va=va, fontsize=8)
            
            plt.tight_layout()
            plt.savefig(vis_image_file, dpi=300)
            print("âœ… å›¾ç‰‡ä¿å­˜æˆåŠŸï¼")
        except ImportError:
            print("âŒ ç¼ºå°‘ matplotlib")
    else:
        print("\nğŸ† [Top 10 High-Utility Memories]")
        for mid, score in sorted_memories[:10]:
            print(f"   ID: {mid:<5} | Score: {score}")