import os
import json
import random
from tqdm import tqdm
from datasets import load_dataset
from omegaconf import DictConfig
from collections import Counter # å¼•å…¥è®¡æ•°å™¨ï¼Œçœ‹çœ‹æ•°æ®åˆ†å¸ƒ

def _format_sciknow_instance(item):
    """
    æ™ºèƒ½è§£æ SciKnowEval çš„å„ç§å¥‡è‘©æ ¼å¼
    """
    question_raw = item.get("question", "").strip()
    choices = item.get("choices", None)
    answer_raw = item.get("answer", "")
    type_str = item.get("type", "")
    
    # å…¼å®¹æ•°å­—ç­”æ¡ˆ 0 -> 'A'
    if isinstance(answer_raw, int):
        labels = ["A", "B", "C", "D", "E", "F"]
        if 0 <= answer_raw < len(labels):
            answer_raw = labels[answer_raw]
        else:
            answer_raw = str(answer_raw) # å…œåº•

    # 1. å¿…æ€æŠ€ï¼šä¸¥æ ¼æ£€æŸ¥ç©ºå€¼ (ä¿®å¤ 0 è¢«è¯¯æ€çš„ bug)
    if answer_raw is None or str(answer_raw).strip() == "":
        return "", "", False

    if "true_or_false" in str(type_str).lower() and not choices:
        choices = ["True", "False"] 
        # é¡ºä¾¿æŠŠç­”æ¡ˆå½’ä¸€åŒ–ï¼šå¦‚æœç­”æ¡ˆæ˜¯ "True" -> è½¬æˆ "A", "False" -> "B"
        if str(answer_raw).lower() == "true": answer_raw = "A"
        if str(answer_raw).lower() == "false": answer_raw = "B"
    
    options_str = ""
    
    # === æƒ…å†µ A: æ ‡å‡†å­—å…¸æ ¼å¼ ===
    if isinstance(choices, dict) and "text" in choices:
        texts = choices["text"]
        labels = choices.get("label", [])
        if not labels: 
            labels = ["A", "B", "C", "D", "E", "F"][:len(texts)]
        for l, t in zip(labels, texts):
            options_str += f"\n({l}) {t}"

    # === æƒ…å†µ B: åˆ—è¡¨æ ¼å¼ ===
    elif isinstance(choices, list):
        labels = ["A", "B", "C", "D", "E", "F"]
        for idx, text in enumerate(choices):
            label = labels[idx] if idx < len(labels) else str(idx)
            options_str += f"\n({label}) {text}"
            
    # === æƒ…å†µ C: å­—ç¬¦ä¸² ===
    elif isinstance(choices, str):
        options_str = f"\n{choices}"

    q_text = question_raw + options_str
    a_text = str(answer_raw).strip()
    
    return q_text, a_text, True

def prepare_sciknow(corpus_path: str, test_path: str, cfg: DictConfig , need_split) -> bool:
    is_val = need_split
    # if os.path.exists(corpus_path) and os.path.exists(test_path):
    #     print(f"âœ… [SciKnow] æ£€æµ‹åˆ°ç°æœ‰çš„ Corpus å’Œ Test æ–‡ä»¶ (è·³è¿‡åˆ‡åˆ†)")
    #     return True

    print(f"âš¡ [Auto-Split] æ­£åœ¨ä¸‹è½½ SciKnowEval...")
    try:
        ds = load_dataset("hicai-zju/SciKnowEval", split="test") 
    except Exception as e:
        print(f"âŒ SciKnowEval ä¸‹è½½å¤±è´¥: {e}")
        return False
    
    raw_data = list(ds)
    print(f"   ğŸ“Š åŸå§‹æ•°æ®å…¨é‡: {len(raw_data)}")

    # =========================================================
    # ğŸ” ä¸Šå¸è§†è§’ï¼šæ‰«æåˆ†å¸ƒ
    # =========================================================
    print("\nğŸ§ [Debug] æ­£åœ¨æ‰«ææ•°æ®åˆ†å¸ƒ...")
    domain_counter = Counter()
    type_counter = Counter()
    valid_candidates = []
    
    for item in tqdm(raw_data, desc="Scanning"):
        ans = item.get("answer")
        if ans is None or str(ans).strip() == "": continue
            
        d = item.get("domain", "Unknown")
        if isinstance(d, list) and d: d = d[0]
        t = item.get("type", "Unknown")
        
        domain_counter[str(d)] += 1
        type_counter[str(t)] += 1
        valid_candidates.append(item)

    print(f"\nğŸ“ˆ [æ•°æ®ç»Ÿè®¡æŠ¥å‘Š]")
    print(f"   ğŸ‘‰ å¯ç”¨é¢†åŸŸ: {domain_counter}")
    print(f"   ğŸ‘‰ å¯ç”¨é¢˜å‹: {type_counter}")
    
    if len(valid_candidates) == 0:
        print("\nâŒ é”™è¯¯: æ•°æ®é›†æ— æœ‰æ•ˆç­”æ¡ˆæ ·æœ¬ã€‚")
        return False

    # =========================================================
    # ğŸ”¥ è¿‡æ»¤é€»è¾‘ (Domain + True/False)
    # =========================================================
    target_domain = cfg.experiment.get("target_domain")
    print(f"\n   ğŸ§¹ æ¸…æ´—æ•°æ® (Domain: {target_domain} | Type: True/False Only)...")
    
    final_data = []
    skipped_domain = 0
    skipped_type = 0
    
    for item in valid_candidates:
        # 1. é¢†åŸŸè¿‡æ»¤
        d = item.get("domain", "")
        if isinstance(d, list) and d: d = d[0]
        
        if target_domain and d != target_domain:
            skipped_domain += 1
            continue

        # 2. é¢˜å‹è¿‡æ»¤ (åªä¿ç•™ True/False)
        t = item.get("type", "")
        if "true_or_false" not in str(t).lower():
            skipped_type += 1
            continue
            
        final_data.append(item)

    print(f"   ğŸš« è¿‡æ»¤ç»Ÿè®¡: é¢†åŸŸä¸ç¬¦={skipped_domain} | éåˆ¤æ–­é¢˜={skipped_type}")
    print(f"   âœ… æœ‰æ•ˆæ•°æ®: {len(final_data)} æ¡")

    if len(final_data) == 0:
        print(f"âŒ é”™è¯¯: ç­›é€‰åæ•°æ®ä¸º 0ã€‚è¯·æ”¾å®½æ¡ä»¶ã€‚")
        return False
        
    # =========================================================
    # å¤„ç†æµç¨‹ (æ‰“ä¹± -> æˆªæ–­ -> åˆ‡åˆ† -> å†™å…¥)
    # =========================================================
    
    random.seed(42) 
    random.shuffle(final_data)
    
    # 1. æ€»é‡æˆªæ–­ (total_limit)
    total_limit = cfg.experiment.get("total_limit")
    if total_limit:
        limit_val = int(total_limit)
        if limit_val < len(final_data):
            print(f"   âœ‚ï¸ [Total Limit] æˆªå–å‰ {limit_val} æ¡ç”¨äºå®éªŒ")
            final_data = final_data[:limit_val]

    # =========================================================
    # ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šåŒå±‚åˆ‡åˆ†é€»è¾‘
    # =========================================================
    
    # --- Stage 1: ç‰©ç†éš”ç¦» (80% æ½œåœ¨è®°å¿†æ±  vs 20% æœ€ç»ˆæµ‹è¯•é›†) ---
    # è¿™æ˜¯ä¸€æˆä¸å˜çš„ï¼Œä¿è¯æœ€ç»ˆæµ‹è¯•é›† (final_test_pool) æ°¸è¿œä¸è¢«æ±¡æŸ“
    split_idx_1 = int(len(final_data) * 0.8)
    corpus_pool = final_data[:split_idx_1]      # 80%
    final_test_pool = final_data[split_idx_1:]  # 20%
    
    print(f"   ğŸ“‰ [Stage 1] ç‰©ç†éš”ç¦»: æ½œåœ¨è®°å¿†æ±  {len(corpus_pool)} æ¡ | æœ€ç»ˆä¿ç•™æµ‹è¯•é›† {len(final_test_pool)} æ¡")

    # --- Stage 2: æ ¹æ® is_val å†³å®šå®é™…ä½¿ç”¨çš„æ® ---
    if is_val:
        # âœ… éªŒè¯/ä¼˜åŒ–æ¨¡å¼ï¼š
        # ä» 80% çš„ corpus_pool é‡Œï¼Œå†åˆ‡åˆ†å‡ºéªŒè¯é›† (é»˜è®¤ 10%)
        # å‰©ä¸‹çš„ 90% åšè®°å¿†ã€‚final_test_pool åœ¨è¿™é‡Œä¸ä½¿ç”¨ã€‚
        split_ratio = cfg.parameters.get("split_ratio", 0.9)
        split_idx_2 = int(len(corpus_pool) * split_ratio)
        
        real_corpus_data = corpus_pool[:split_idx_2]      # å®é™…å†™å…¥è®°å¿†åº“çš„
        target_test_data = corpus_pool[split_idx_2:]      # å®é™…å†™å…¥æµ‹è¯•æ–‡ä»¶(éªŒè¯é›†)çš„
        
        print(f"   ğŸ”€ [Validation Mode] å¯åŠ¨éªŒè¯æ¨¡å¼:")
        print(f"     ğŸ‘‰ ä»è®°å¿†æ± ä¸­åˆ’åˆ† {len(target_test_data)} æ¡åšéªŒè¯ (Split Ratio: {split_ratio})")
        print(f"     ğŸ‘‰ å®é™…è®°å¿†åº“å¤§å°: {len(real_corpus_data)}")
        
    else:
        # ğŸš€ æœ€ç»ˆæµ‹è¯•æ¨¡å¼ï¼š
        # è®°å¿†åº“ä½¿ç”¨å®Œæ•´çš„ corpus_pool (80%)
        # æµ‹è¯•é›†ä½¿ç”¨ä¹‹å‰éš”ç¦»å¥½çš„ final_test_pool (20%)
        real_corpus_data = corpus_pool
        target_test_data = final_test_pool
        
        print(f"   ğŸš€ [Final Test Mode] å¯åŠ¨æœ€ç»ˆæµ‹è¯•æ¨¡å¼:")
        print(f"     ğŸ‘‰ ä½¿ç”¨å®Œæ•´çš„æ½œåœ¨è®°å¿†æ±  ({len(real_corpus_data)} æ¡)")
        print(f"     ğŸ‘‰ ä½¿ç”¨é¢„ç•™çš„æœ€ç»ˆæµ‹è¯•é›† ({len(target_test_data)} æ¡)")

    # =========================================================
    # å†™å…¥æµç¨‹ (ä½¿ç”¨ real_corpus_data å’Œ target_test_data)
    # =========================================================

    # 3. å†™å…¥ Memory
    os.makedirs(os.path.dirname(corpus_path), exist_ok=True)
    with open(corpus_path, "w", encoding="utf-8") as f:
        count = 0
        # ğŸ”¥ æ³¨æ„ï¼šè¿™é‡Œéå†çš„æ˜¯ real_corpus_data
        for i, item in enumerate(tqdm(real_corpus_data, desc="Writing Corpus")):
            q_text, a_text, is_valid = _format_sciknow_instance(item)
            if is_valid:
                content = f"Question: {q_text}\nAnswer: {a_text}"
                f.write(json.dumps({"id": str(count), "contents": content}) + "\n")
                count += 1
            
    # 4. å†™å…¥ Test (ä¿ç•™ Debug åˆ‡ç‰‡é€»è¾‘)
    os.makedirs(os.path.dirname(test_path), exist_ok=True)
    
    # è¯»å–è°ƒè¯•å‚æ•°
    start_index = int(cfg.parameters.get("start_index", 0) or 0)
    debug_num = cfg.parameters.get("debug_num")
    
    # å¯¹ target_test_data è¿›è¡Œåˆ‡ç‰‡å¤„ç†
    if debug_num:
        limit = int(debug_num)
        end_idx = min(start_index + limit, len(target_test_data))
        test_data_slice = target_test_data[start_index : end_idx]
        print(f"   ğŸ› [Debug] ä»…å†™å…¥ {len(test_data_slice)} æ¡æµ‹è¯•æ•°æ® (Start: {start_index})")
    else:
        test_data_slice = target_test_data[start_index:]
        print(f"   ğŸ“Š [Full] å†™å…¥ {len(test_data_slice)} æ¡æµ‹è¯•æ•°æ®")

    with open(test_path, "w", encoding="utf-8") as f:
        count = 0 # é‡ç½® ID
        for i, item in enumerate(tqdm(test_data_slice, desc="Writing Test")):
            q_text, a_text, is_valid = _format_sciknow_instance(item)
            if is_valid:
                f.write(json.dumps({
                    "id": str(count), 
                    "question": q_text,
                    "golden_answers": [a_text]
                }) + "\n")
                count += 1
            
    print("âœ… SciKnowEval å¤„ç†å®Œæˆï¼")
    return True