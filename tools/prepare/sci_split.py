import os
import json
import random
from tqdm import tqdm
from datasets import load_dataset
from omegaconf import DictConfig
from collections import Counter # å¼•å…¥è®¡æ•°å™¨ï¼Œçœ‹çœ‹æ•°æ®åˆ†å¸ƒ

def _format_sciknow_instance(item):
    """
    æ™ºèƒ½è§£æ SciKnowEval çš„å„ç§å¥‡è‘©æ ¼å¼
    """
    question_raw = item.get("question", "").strip()
    choices = item.get("choices", None)
    answer_raw = item.get("answer", "")
    type_str = item.get("type", "")
    
    # å…¼å®¹æ•°å­—ç­”æ¡ˆ 0 -> 'A'
    if isinstance(answer_raw, int):
        labels = ["A", "B", "C", "D", "E", "F"]
        if 0 <= answer_raw < len(labels):
            answer_raw = labels[answer_raw]
        else:
            answer_raw = str(answer_raw) # å…œåº•

    # 1. å¿…æ€æŠ€ï¼šä¸¥æ ¼æ£€æŸ¥ç©ºå€¼ (ä¿®å¤ 0 è¢«è¯¯æ€çš„ bug)
    if answer_raw is None or str(answer_raw).strip() == "":
        return "", "", False

    if "true_or_false" in str(type_str).lower() and not choices:
        choices = ["True", "False"] 
        # é¡ºä¾¿æŠŠç­”æ¡ˆå½’ä¸€åŒ–ï¼šå¦‚æœç­”æ¡ˆæ˜¯ "True" -> è½¬æˆ "A", "False" -> "B"
        if str(answer_raw).lower() == "true": answer_raw = "A"
        if str(answer_raw).lower() == "false": answer_raw = "B"
    
    options_str = ""
    
    # === æƒ…å†µ A: æ ‡å‡†å­—å…¸æ ¼å¼ ===
    if isinstance(choices, dict) and "text" in choices:
        texts = choices["text"]
        labels = choices.get("label", [])
        if not labels: 
            labels = ["A", "B", "C", "D", "E", "F"][:len(texts)]
        for l, t in zip(labels, texts):
            options_str += f"\n({l}) {t}"

    # === æƒ…å†µ B: åˆ—è¡¨æ ¼å¼ ===
    elif isinstance(choices, list):
        labels = ["A", "B", "C", "D", "E", "F"]
        for idx, text in enumerate(choices):
            label = labels[idx] if idx < len(labels) else str(idx)
            options_str += f"\n({label}) {text}"
            
    # === æƒ…å†µ C: å­—ç¬¦ä¸² ===
    elif isinstance(choices, str):
        options_str = f"\n{choices}"

    q_text = question_raw + options_str
    a_text = str(answer_raw).strip()
    
    return q_text, a_text, True

def prepare_sciknow(corpus_path: str, test_path: str, cfg: DictConfig) -> bool:
    
    # if os.path.exists(corpus_path) and os.path.exists(test_path):
    #     print(f"âœ… [SciKnow] æ£€æµ‹åˆ°ç°æœ‰çš„ Corpus å’Œ Test æ–‡ä»¶ (è·³è¿‡åˆ‡åˆ†)")
    #     return True

    print(f"âš¡ [Auto-Split] æ­£åœ¨ä¸‹è½½ SciKnowEval...")
    try:
        ds = load_dataset("hicai-zju/SciKnowEval", split="test") 
    except Exception as e:
        print(f"âŒ SciKnowEval ä¸‹è½½å¤±è´¥: {e}")
        return False
    
    raw_data = list(ds)
    print(f"   ğŸ“Š åŸå§‹æ•°æ®å…¨é‡: {len(raw_data)}")

    # =========================================================
    # ğŸ” ä¸Šå¸è§†è§’ï¼šæ‰«æåˆ†å¸ƒ
    # =========================================================
    print("\nğŸ§ [Debug] æ­£åœ¨æ‰«ææ•°æ®åˆ†å¸ƒ...")
    domain_counter = Counter()
    type_counter = Counter()
    valid_candidates = []
    
    for item in tqdm(raw_data, desc="Scanning"):
        ans = item.get("answer")
        if ans is None or str(ans).strip() == "": continue
            
        d = item.get("domain", "Unknown")
        if isinstance(d, list) and d: d = d[0]
        t = item.get("type", "Unknown")
        
        domain_counter[str(d)] += 1
        type_counter[str(t)] += 1
        valid_candidates.append(item)

    print(f"\nğŸ“ˆ [æ•°æ®ç»Ÿè®¡æŠ¥å‘Š]")
    print(f"   ğŸ‘‰ å¯ç”¨é¢†åŸŸ: {domain_counter}")
    print(f"   ğŸ‘‰ å¯ç”¨é¢˜å‹: {type_counter}")
    
    if len(valid_candidates) == 0:
        print("\nâŒ é”™è¯¯: æ•°æ®é›†æ— æœ‰æ•ˆç­”æ¡ˆæ ·æœ¬ã€‚")
        return False

    # =========================================================
    # ğŸ”¥ è¿‡æ»¤é€»è¾‘ (Domain + True/False)
    # =========================================================
    target_domain = cfg.experiment.get("target_domain")
    print(f"\n   ğŸ§¹ æ¸…æ´—æ•°æ® (Domain: {target_domain} | Type: True/False Only)...")
    
    final_data = []
    skipped_domain = 0
    skipped_type = 0
    
    for item in valid_candidates:
        # 1. é¢†åŸŸè¿‡æ»¤
        d = item.get("domain", "")
        if isinstance(d, list) and d: d = d[0]
        
        if target_domain and d != target_domain:
            skipped_domain += 1
            continue

        # 2. é¢˜å‹è¿‡æ»¤ (åªä¿ç•™ True/False)
        t = item.get("type", "")
        if "true_or_false" not in str(t).lower():
            skipped_type += 1
            continue
            
        final_data.append(item)

    print(f"   ğŸš« è¿‡æ»¤ç»Ÿè®¡: é¢†åŸŸä¸ç¬¦={skipped_domain} | éåˆ¤æ–­é¢˜={skipped_type}")
    print(f"   âœ… æœ‰æ•ˆæ•°æ®: {len(final_data)} æ¡")

    if len(final_data) == 0:
        print(f"âŒ é”™è¯¯: ç­›é€‰åæ•°æ®ä¸º 0ã€‚è¯·æ”¾å®½æ¡ä»¶ã€‚")
        return False
        
    # =========================================================
    # å¤„ç†æµç¨‹ (æ‰“ä¹± -> æˆªæ–­ -> åˆ‡åˆ† -> å†™å…¥)
    # =========================================================
    
    random.seed(42) 
    random.shuffle(final_data)
    
    # 1. æ€»é‡æˆªæ–­ (total_limit)
    total_limit = cfg.experiment.get("total_limit")
    if total_limit:
        limit_val = int(total_limit)
        if limit_val < len(final_data):
            print(f"   âœ‚ï¸ [Total Limit] æˆªå–å‰ {limit_val} æ¡ç”¨äºå®éªŒ")
            final_data = final_data[:limit_val]

    # 2. 80/20 åˆ‡åˆ†
    split_idx = int(len(final_data) * 0.8)
    corpus_data = final_data[:split_idx]
    test_data = final_data[split_idx:]
    
    print(f"   ğŸ“‰ åˆ‡åˆ†ç»“æœ: Memoryåº“ {len(corpus_data)} æ¡ | Testé›† {len(test_data)} æ¡")
    
    # 3. å†™å…¥ Memory
    os.makedirs(os.path.dirname(corpus_path), exist_ok=True)
    with open(corpus_path, "w", encoding="utf-8") as f:
        count = 0
        for i, item in enumerate(tqdm(corpus_data, desc="Writing Corpus")):
            q_text, a_text, is_valid = _format_sciknow_instance(item)
            if is_valid:
                content = f"Question: {q_text}\nAnswer: {a_text}"
                f.write(json.dumps({"id": str(count), "contents": content}) + "\n")
                count += 1
            
    # 4. å†™å…¥ Test (ğŸ”¥ è¡¥ä¸Š Debug åˆ‡ç‰‡é€»è¾‘)
    os.makedirs(os.path.dirname(test_path), exist_ok=True)
    
    # ğŸ”¥ğŸ”¥ğŸ”¥ [æ–°å¢] è¯»å– debug_num å’Œ start_index
    start_index = int(cfg.parameters.get("start_index", 0) or 0)
    debug_num = cfg.parameters.get("debug_num")
    
    if debug_num:
        limit = int(debug_num)
        end_idx = min(start_index + limit, len(test_data))
        # å¯¹ test_data è¿›è¡Œåˆ‡ç‰‡ï¼Œåªå†™å…¥è¿™ä¸€å°éƒ¨åˆ†
        test_data_slice = test_data[start_index : end_idx]
        print(f"   ğŸ› [Debug Mode] Testé›†åˆ‡ç‰‡: ä»…å†™å…¥ {len(test_data_slice)} æ¡ (Start: {start_index})")
    else:
        # å¦‚æœæ²¡å¼€ debugï¼Œå°±å†™å…¨é‡ (ä» start_index å¼€å§‹åˆ°æœ€åï¼Œæˆ–è€…å…¨é‡)
        test_data_slice = test_data[start_index:]
        print(f"   ğŸ“Š [Full Mode] å†™å…¥ Testé›†: {len(test_data_slice)} æ¡")

    with open(test_path, "w", encoding="utf-8") as f:
        count = 0 # é‡ç½® ID
        for i, item in enumerate(tqdm(test_data_slice, desc="Writing Test")):
            q_text, a_text, is_valid = _format_sciknow_instance(item)
            if is_valid:
                f.write(json.dumps({
                    "id": str(count), # ID ä» 0 å¼€å§‹
                    "question": q_text,
                    "golden_answers": [a_text]
                }) + "\n")
                count += 1
            
    print("âœ… SciKnowEval å¤„ç†å®Œæˆï¼")
    return True