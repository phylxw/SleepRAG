import os
import json
import random
from tqdm import tqdm
from datasets import load_dataset
from omegaconf import DictConfig
from collections import Counter

def _format_sciknow_instance(item):
    """
    æ™ºèƒ½è§£æ SciKnowEval (MCQ ä¸“ç”¨ç‰ˆ - é€‚é… answerKey)
    
    ä¿®æ”¹ç‚¹ï¼š
    1. ä¼˜å…ˆè¯»å– 'answerKey'ï¼Œå¦‚æœä¸ºç©ºæ‰è¯» 'answer'ã€‚
    2. ä¸“é—¨å¤„ç† answerKey ä¸ºæ•°å­—ç´¢å¼• (0->A, 1->B) çš„æƒ…å†µã€‚
    """
    question_raw = item.get("question", "").strip()
    choices = item.get("choices", None)
    
    # === ğŸ”¥ ä¿®æ”¹ç‚¹ 1: ä¼˜å…ˆè¯»å– answerKey ===
    # SciKnowEval çš„é€‰æ‹©é¢˜æ ‡å‡†ç­”æ¡ˆé€šå¸¸åœ¨è¿™é‡Œ
    answer_raw = item.get("answerKey")
    
    # å…œåº•ï¼šå¦‚æœ answerKey æ²¡ä¸œè¥¿ï¼Œå†å›å»è¯» answer
    if answer_raw is None or str(answer_raw).strip() == "":
        answer_raw = item.get("answer", "")

    # 1. åˆ¤ç©º
    if answer_raw is None or str(answer_raw).strip() == "":
        return "", "", False

    # 2. å¦‚æœæ²¡æœ‰é€‰é¡¹ï¼Œé‚£è‚¯å®šä¸æ˜¯é€‰æ‹©é¢˜ï¼Œç›´æ¥è·³è¿‡
    if not choices:
        return "", "", False

    # === å½’ä¸€åŒ–å¤„ç† ===
    ans_str = str(answer_raw).strip()
    labels_pool = ["A", "B", "C", "D", "E", "F", "G", "H"]
    final_answer = ans_str 
    options_str = ""

    # === å¤„ç†é€‰é¡¹åˆ—è¡¨ (List) ===
    if isinstance(choices, list):
        # æ„é€  (A) xxx (B) xxx
        for idx, text in enumerate(choices):
            label = labels_pool[idx] if idx < len(labels_pool) else str(idx)
            options_str += f"\n({label}) {text}"
            
            # æ–‡æœ¬åå‘åŒ¹é… (é˜²æ­¢ answerKey ç»™çš„æ˜¯ "Carbon" è¿™ç§æ–‡æœ¬)
            if ans_str == str(text) or ans_str == str(text).strip():
                final_answer = label

        # === ğŸ”¥ ä¿®æ”¹ç‚¹ 2: å¤„ç†æ•°å­—ç´¢å¼•ç­”æ¡ˆ ===
        # æƒ…å†µ A: answerKey æ˜¯æ•´æ•°ç±»å‹ (e.g., 0)
        if isinstance(answer_raw, int) and 0 <= answer_raw < len(choices):
            final_answer = labels_pool[answer_raw]
        
        # æƒ…å†µ B: answerKey æ˜¯æ•°å­—å­—ç¬¦ä¸² (e.g., "0")
        elif ans_str.isdigit():
            idx = int(ans_str)
            if 0 <= idx < len(choices):
                final_answer = labels_pool[idx]

    # === å¤„ç†é€‰é¡¹å­—å…¸ (Dict) ===
    # æ ¼å¼é€šå¸¸æ˜¯ {'text': ['a', 'b'], 'label': ['A', 'B']}
    elif isinstance(choices, dict) and "text" in choices:
        texts = choices["text"]
        labels = choices.get("label", labels_pool[:len(texts)])
        
        for l, t in zip(labels, texts):
            options_str += f"\n({l}) {t}"
            # æ–‡æœ¬åå‘åŒ¹é…
            if ans_str == str(t) or ans_str == str(t).strip():
                final_answer = l
                
    # === å…œåº•: ç¡®ä¿æœ€ç»ˆç­”æ¡ˆæ˜¯ A/B/C/D è¿™æ ·çš„å­—æ¯ ===
    if len(str(final_answer)) == 1 and str(final_answer).upper() in labels_pool:
        final_answer = str(final_answer).upper()

    # æ‹¼è£…
    q_text = question_raw + options_str
    
    return q_text, final_answer, True

def prepare_sciknow(corpus_path: str, test_path: str, cfg: DictConfig , need_split) -> bool:
    # 1. æ£€æµ‹è®°å¿†åº“æ˜¯å¦å·²å­˜åœ¨
    memory_exists = os.path.exists(corpus_path)
    if memory_exists:
        print(f"âœ… [Cache] è®°å¿†åº“æ–‡ä»¶å·²å­˜åœ¨ï¼Œå°†è·³è¿‡ç”Ÿæˆæ­¥éª¤: {corpus_path}")
    else:
        print(f"âš ï¸ [Init] è®°å¿†åº“ç¼ºå¤±ï¼Œå‡†å¤‡ç”Ÿæˆ...")
    
    is_val = need_split
    
    print(f"âš¡ [Auto-Split] æ­£åœ¨ä¸‹è½½ SciKnowEval...")
    try:
        ds = load_dataset("hicai-zju/SciKnowEval", split="test") 
    except Exception as e:
        print(f"âŒ SciKnowEval ä¸‹è½½å¤±è´¥: {e}")
        return False
    
    raw_data = list(ds)
    print(f"   ğŸ“Š åŸå§‹æ•°æ®å…¨é‡: {len(raw_data)}")

    # =========================================================
    # ğŸ” æ‰«ææ•°æ®åˆ†å¸ƒ (ä¿®æ”¹å¤„ï¼šä¿®å¤åˆ¤ç©ºé€»è¾‘)
    # =========================================================
    print("\nğŸ§ [Debug] æ­£åœ¨æ‰«ææ•°æ®åˆ†å¸ƒ...")
    domain_counter = Counter()
    type_counter = Counter()
    valid_candidates = []
    
    for item in tqdm(raw_data, desc="Scanning"):
        # ğŸ”¥ ä¿®æ”¹ç‚¹ 1: åªè¦ answer æˆ– answerKey æœ‰ä¸€ä¸ªä¸ä¸ºç©ºï¼Œå°±ç®—æœ‰æ•ˆæ•°æ®
        ans = item.get("answer")
        ans_key = item.get("answerKey")
        
        has_ans = (ans is not None and str(ans).strip() != "")
        has_key = (ans_key is not None and str(ans_key).strip() != "")
        
        if not has_ans and not has_key:
            continue
            
        d = item.get("domain", "Unknown")
        if isinstance(d, list) and d: d = d[0]
        t = item.get("type", "Unknown")
        
        domain_counter[str(d)] += 1
        type_counter[str(t)] += 1
        valid_candidates.append(item)

    print(f"\nğŸ“ˆ [æ•°æ®ç»Ÿè®¡æŠ¥å‘Š]")
    print(f"   ğŸ‘‰ å¯ç”¨é¢†åŸŸ: {domain_counter}")
    print(f"   ğŸ‘‰ å¯ç”¨é¢˜å‹: {type_counter}")
    
    if len(valid_candidates) == 0:
        print("\nâŒ é”™è¯¯: æ•°æ®é›†æ— æœ‰æ•ˆç­”æ¡ˆæ ·æœ¬ã€‚")
        return False

    # =========================================================
    # ğŸ§¹ æ¸…æ´—æ•°æ® (MCQ Only)
    # =========================================================
    target_domain = cfg.experiment.get("target_domain")
    print(f"\n   ğŸ§¹ æ¸…æ´—æ•°æ® (Domain: {target_domain} | Type: MCQ Only)...")
    
    final_data = []
    skipped_domain = 0
    skipped_type = 0
    
    for item in valid_candidates:
        # 1. é¢†åŸŸè¿‡æ»¤
        d = item.get("domain", "")
        if isinstance(d, list) and d: d = d[0]
        
        if target_domain and d != target_domain:
            skipped_domain += 1
            continue

        # 2. é¢˜å‹è¿‡æ»¤ (åªç•™ MCQ)
        t = str(item.get("type", "")).lower()
        if "mcq" not in t and "multiple_choice" not in t:
            skipped_type += 1
            continue
            
        final_data.append(item)

    print(f"   ğŸš« è¿‡æ»¤ç»Ÿè®¡: é¢†åŸŸä¸ç¬¦={skipped_domain} | é MCQ é¢˜å‹={skipped_type}")
    print(f"   âœ… æœ‰æ•ˆæ•°æ®: {len(final_data)} æ¡")

    if len(final_data) == 0:
        print(f"âŒ é”™è¯¯: ç­›é€‰åæ•°æ®ä¸º 0ã€‚è¯·æ”¾å®½æ¡ä»¶ã€‚")
        return False
        
    # =========================================================
    # å¤„ç†æµç¨‹
    # =========================================================
    
    random.seed(42) 
    # random.shuffle(final_data)
    
    # 1. æ€»é‡æˆªæ–­
    total_limit = cfg.experiment.get("total_limit")
    if total_limit:
        limit_val = int(total_limit)
        if limit_val < len(final_data):
            print(f"   âœ‚ï¸ [Total Limit] æˆªå–å‰ {limit_val} æ¡ç”¨äºå®éªŒ")
            final_data = final_data[:limit_val]

    # --- Stage 1: ç‰©ç†éš”ç¦» (80% æ½œåœ¨è®°å¿†æ±  vs 20% æœ€ç»ˆæµ‹è¯•é›†) ---
    split_idx_1 = int(len(final_data) * 0.8)
    corpus_pool = final_data[:split_idx_1]      
    final_test_pool = final_data[split_idx_1:]  
    
    print(f"   ğŸ“‰ [Stage 1] ç‰©ç†éš”ç¦»: æ½œåœ¨è®°å¿†æ±  {len(corpus_pool)} æ¡ | æœ€ç»ˆä¿ç•™æµ‹è¯•é›† {len(final_test_pool)} æ¡")

    # --- Stage 2: æ ¹æ® is_val å†³å®šå®é™…ä½¿ç”¨çš„æ•°æ® ---
    if is_val:
        split_ratio = cfg.parameters.get("split_ratio", 0.9)
        split_idx_2 = int(len(corpus_pool) * split_ratio)
        
        real_corpus_data = corpus_pool[:split_idx_2]      # å†™å…¥ memory.jsonl
        target_test_data = corpus_pool[split_idx_2:]      # å†™å…¥ test.jsonl (åšéªŒè¯)
        
        print(f"   ğŸ”€ [Validation Mode] éªŒè¯æ¨¡å¼: è®°å¿†åº“ {len(real_corpus_data)} | éªŒè¯é›† {len(target_test_data)}")
        
    else:
        real_corpus_data = corpus_pool
        target_test_data = final_test_pool
        
        print(f"   ğŸš€ [Final Test Mode] æµ‹è¯•æ¨¡å¼: è®°å¿†åº“ {len(real_corpus_data)} | æµ‹è¯•é›† {len(target_test_data)}")

    # 3. å†™å…¥ Memory
    # ã€ä¿®æ”¹ç‚¹ 3ã€‘å¢åŠ æ¡ä»¶åˆ¤æ–­ï¼šå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æ‰å†™
    if not memory_exists:
        os.makedirs(os.path.dirname(corpus_path), exist_ok=True)
        with open(corpus_path, "w", encoding="utf-8") as f:
            count = 0
            for i, item in enumerate(tqdm(real_corpus_data, desc="Writing Corpus")):
                q_text, a_text, is_valid = _format_sciknow_instance(item)
                if is_valid:
                    content = f"Question: {q_text}\nAnswer: {a_text}"
                    f.write(json.dumps({"id": str(count), "contents": content}, ensure_ascii=False) + "\n")
                    count += 1
        print(f"   ğŸ’¾ è®°å¿†åº“å·²ç”Ÿæˆ: {count} æ¡")
    else:
        print(f"   â© è®°å¿†åº“å·²å­˜åœ¨ï¼Œè·³è¿‡å†™å…¥ã€‚")
            
    # 4. å†™å…¥ Test
    os.makedirs(os.path.dirname(test_path), exist_ok=True)
    
    start_index = int(cfg.parameters.get("start_index", 0) or 0)
    debug_num = cfg.parameters.get("debug_num")
    
    if debug_num:
        limit = int(debug_num)
        end_idx = min(start_index + limit, len(target_test_data))
        test_data_slice = target_test_data[start_index : end_idx]
        print(f"   ğŸ› [Debug] ä»…å†™å…¥ {len(test_data_slice)} æ¡æµ‹è¯•æ•°æ®")
    else:
        test_data_slice = target_test_data[start_index:]
        print(f"   ğŸ“Š [Full] å†™å…¥ {len(test_data_slice)} æ¡æµ‹è¯•æ•°æ®")

    with open(test_path, "w", encoding="utf-8") as f:
        count = 0 
        for i, item in enumerate(tqdm(test_data_slice, desc="Writing Test")):
            q_text, a_text, is_valid = _format_sciknow_instance(item)
            if is_valid:
                f.write(json.dumps({
                    "id": str(count), 
                    "question": q_text,
                    "golden_answers": [a_text]
                }, ensure_ascii=False) + "\n")
                count += 1
            
    print("âœ… SciKnowEval (åˆ¤æ–­é¢˜+é€‰æ‹©é¢˜) å¤„ç†å®Œæˆï¼")
    return True