import os
import json
from datasets import load_dataset

import os
import json
from datasets import load_dataset

def normalize_code_instance(item, dataset_type="mbpp"):
    """
    æ•°æ®æ¸…æ´—ä¸æ ‡å‡†åŒ– (ä¿æŒåŸé€»è¾‘ä¸å˜)
    """
    normalized = {}
    
    if dataset_type == "mbpp":
        # MBPP (sanitized) 
        question = item.get("prompt") or item.get("text")
        answer = item.get("code")
        
        normalized = {
            "id": str(item.get("task_id", "")),
            # ğŸ”¥ [æ ¸å¿ƒä¿®å¤] FlashRAG å¼ºåˆ¶è¦æ±‚ 'contents' å­—æ®µ
            "contents": question, 
            "question": question,
            "golden_answers": [answer] if answer else [],
            **item 
        }
        
    # ... (humaneval part if needed)
    
    return normalized

def prepare_mbpp(corpus_file, test_file, cfg, need_split):
    """
    MBPP ä¸“ç”¨å‡†å¤‡å‡½æ•°
    
    Args:
        corpus_file: è®°å¿†åº“ä¿å­˜è·¯å¾„ (æ„å»ºè‡ª MBPP Train set)
        test_file: æµ‹è¯•é›†ä¿å­˜è·¯å¾„ (æ„å»ºè‡ª MBPP Validation æˆ– Test set)
        cfg: Hydra é…ç½®å¯¹è±¡
    """

    print(f"\nğŸ”¨ [Prepare] è¿›å…¥ MBPP æ•°æ®å‡†å¤‡æµç¨‹...")

    # ==========================================
    # 2. å‡†å¤‡è®°å¿†åº“ (Corpus - ä»…ä½¿ç”¨è®­ç»ƒé›†)
    # ==========================================
    if not os.path.exists(corpus_file):
        print(f"ğŸ“š [Corpus] æ­£åœ¨æ„å»º MBPP è®°å¿†åº“ (Train Set Only)...")
        try:
            # âš ï¸ å…³é”®ç‚¹ï¼šå› ä¸ºæˆ‘ä»¬è¦æµ‹ MBPPï¼Œæ‰€ä»¥è®°å¿†åº“åªèƒ½åŒ…å« train (å¯èƒ½åŒ…å« prompt split)ï¼Œ
            # ç»å¯¹ä¸èƒ½åŒ…å« validation å’Œ testï¼Œå¦åˆ™å°±æ˜¯æ•°æ®æ³„æ¼ã€‚
            corpus_split = "train+prompt" 
            
            mbpp_corpus_ds = load_dataset("google-research-datasets/mbpp", "sanitized", split=corpus_split)
            
            os.makedirs(os.path.dirname(corpus_file), exist_ok=True)
            with open(corpus_file, 'w', encoding='utf-8') as f:
                for item in mbpp_corpus_ds:
                    processed = normalize_code_instance(item, dataset_type="mbpp")
                    f.write(json.dumps(processed, ensure_ascii=False) + "\n")
            
            print(f"    âœ… MBPP è®°å¿†åº“å·²ä¿å­˜: {corpus_file} (æ¥æºäº {corpus_split}, å…± {len(mbpp_corpus_ds)} æ¡)")
        except Exception as e:
            print(f"    âŒ åŠ è½½ MBPP Corpus å¤±è´¥: {e}")
            return False

    # ==========================================
    # 3. å‡†å¤‡æµ‹è¯•é›† (Validation æˆ– Test)
    # ==========================================
    # è·å–ç›®æ ‡ split é…ç½®ï¼Œé»˜è®¤ä¸º test
    is_val = need_split
    
    
    try:
        # 1. ç¬¬ä¸€æ­¥ï¼šæ ¹æ®é…ç½®åŠ è½½ MBPP åŸç”Ÿçš„ validation æˆ– test åˆ†å‰²
        # MBPP åŸç”Ÿæ”¯æŒ: 'train', 'validation', 'test', 'prompt'
        if is_val:
            print(f"    ğŸš€ æ¨¡å¼: éªŒè¯é›† (Validation Split)")
            candidate_ds = load_dataset("google-research-datasets/mbpp", "sanitized", split="validation")
        else:
            print(f"    ğŸš€ æ¨¡å¼: æœ€ç»ˆæµ‹è¯• (Test Split)")
            candidate_ds = load_dataset("google-research-datasets/mbpp", "sanitized", split="test")

        # 2. ç¬¬äºŒæ­¥ï¼šåº”ç”¨ start_index å’Œ debug_num è¿›è¡ŒäºŒæ¬¡åˆ‡ç‰‡ (ä¿æŒåŸé€»è¾‘)
        p_start = int(cfg.parameters.get("start_index", 0) or 0)
        p_debug = cfg.parameters.get("debug_num") # å¯èƒ½ä¸º None
        
        candidate_len = len(candidate_ds)
        
        # è®¡ç®—åˆ‡ç‰‡ç»ˆç‚¹
        if p_debug:
            limit = int(p_debug)
            p_end = min(p_start + limit, candidate_len)
            print(f"    âœ‚ï¸ [Debug Mode] å¯ç”¨åˆ‡ç‰‡: Relative Index {p_start} -> {p_end} (å…± {p_end - p_start} æ¡)")
        else:
            p_end = candidate_len
            print(f"    ğŸ“Š [Full Mode] å…¨é‡æ¨¡å¼: Relative Index {p_start} -> End ({candidate_len} æ¡)")
            
        # å¼‚å¸¸æ£€æŸ¥
        if p_start >= candidate_len:
            print(f"    âš ï¸ [Warning] start_index ({p_start}) è¶…å‡ºäº†å½“å‰æ•°æ®é›†é•¿åº¦ ({candidate_len})ï¼Œå°†ç”Ÿæˆç©ºæ–‡ä»¶ï¼")
            final_ds = []
        else:
            # æ‰§è¡Œåˆ‡ç‰‡
            final_ds = candidate_ds.select(range(p_start, p_end))

        # 3. ä¿å­˜æ–‡ä»¶
        os.makedirs(os.path.dirname(test_file), exist_ok=True)
        with open(test_file, 'w', encoding='utf-8') as f:
            for item in final_ds:
                processed = normalize_code_instance(item, dataset_type="mbpp")
                f.write(json.dumps(processed, ensure_ascii=False) + "\n")
        
        print(f"    âœ… MBPP æµ‹è¯•/éªŒè¯é›† å·²ä¿å­˜: {test_file} (æœ€ç»ˆå†™å…¥ {len(final_ds)} æ¡)")
        
    except Exception as e:
        print(f"    âŒ åŠ è½½ MBPP Test/Val å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

    return True