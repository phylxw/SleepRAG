import os
import json
from datasets import load_dataset

def normalize_code_instance(item, dataset_type="humaneval"):
    """
    æ•°æ®æ¸…æ´—ä¸æ ‡å‡†åŒ–
    ğŸ”¥ æ ¸å¿ƒç›®æ ‡ï¼šä¿ç•™åŸå§‹å­—æ®µï¼Œå¹¶æ»¡è¶³ FlashRAG å¯¹ contents å­—æ®µçš„è¦æ±‚
    """
    normalized = {}
    
    if dataset_type == "mbpp":
        # MBPP (sanitized) 
        question = item.get("prompt") or item.get("text")
        answer = item.get("code")
        
        normalized = {
            "id": str(item.get("task_id", "")),
            
            # ğŸ”¥ [æ ¸å¿ƒä¿®å¤] FlashRAG å¼ºåˆ¶è¦æ±‚ 'contents' å­—æ®µç”¨äºæ£€ç´¢å’Œè¯­è¨€æ£€æµ‹
            "contents": question, 
            
            "question": question,
            "golden_answers": [answer] if answer else [],
            **item # ä¿ç•™åŸå§‹å­—æ®µ (test_list ç­‰)
        }
        
    elif dataset_type == "humaneval":
        # HumanEval
        question = item.get("prompt")
        answer = item.get("canonical_solution")
        
        normalized = {
            "id": str(item.get("task_id", "")),
            
            # ğŸ”¥ [æ ¸å¿ƒä¿®å¤] åŒä¸Šï¼ŒåŠ ä¸Š contents
            "contents": question,
            
            "question": question,
            "golden_answers": [answer] if answer else [],
            **item # ä¿ç•™ entry_point ç­‰
        }
    
    return normalized

def prepare_humaneval(corpus_file, test_file, cfg, need_split):
    """
    HumanEval ä¸“ç”¨å‡†å¤‡å‡½æ•°
    
    Args:
        corpus_file: è®°å¿†åº“ä¿å­˜è·¯å¾„ (åº”ä¸º mbpp)
        test_file: æµ‹è¯•é›†ä¿å­˜è·¯å¾„ (åº”ä¸º humaneval split)
        cfg: Hydra é…ç½®å¯¹è±¡
        need_split: (åœ¨æ­¤ä»»åŠ¡ä¸­æš‚ä¸ç”¨äº Corpus åˆ‡åˆ†ï¼Œä¸»è¦é€»è¾‘ç”± cfg æ§åˆ¶)
    """

    print(f"\nğŸ”¨ [Prepare] è¿›å…¥ Code Generation æ•°æ®å‡†å¤‡æµç¨‹...")
    is_val = need_split
    # ==========================================
    # 2. å‡†å¤‡è®°å¿†åº“ (MBPP Sanitized)
    # ==========================================
    if not os.path.exists(corpus_file):
        print(f"ğŸ“š [Corpus] æ­£åœ¨æ„å»º MBPP (sanitized) è®°å¿†åº“...")
        try:
            # ä½¿ç”¨ sanitized ç‰ˆæœ¬ï¼Œè´¨é‡æ›´é«˜ï¼Œé€‚åˆåš RAG åº•åº§
            target_split = cfg.experiment.get("corpus_split", "train+validation+test+prompt")
            mbpp_ds = load_dataset("google-research-datasets/mbpp", "sanitized", split=target_split)
            
            os.makedirs(os.path.dirname(corpus_file), exist_ok=True)
            with open(corpus_file, 'w', encoding='utf-8') as f:
                for item in mbpp_ds:
                    processed = normalize_code_instance(item, dataset_type="mbpp")
                    f.write(json.dumps(processed, ensure_ascii=False) + "\n")
            
            print(f"    âœ… MBPP è®°å¿†åº“å·²ä¿å­˜: {corpus_file} ({len(mbpp_ds)} æ¡)")
        except Exception as e:
            print(f"    âŒ åŠ è½½ MBPP å¤±è´¥: {e}")
            return False


    # ==========================================
    # 3. å‡†å¤‡æµ‹è¯•é›† (HumanEval Split)
    # ==========================================
    try:
        he_ds = load_dataset("openai_humaneval", split="test") # HumanEval åªæœ‰ test split (164æ¡)
        total_len = len(he_ds)
        mid_point = total_len // 2 # 82
        
        if is_val:
            print(f"    ğŸš€ æ¨¡å¼: éªŒè¯é›† (Validation)")
            print(f"    âœ‚ï¸ åŸå§‹èŒƒå›´: å‰ {mid_point} é¢˜ (Index 0-{mid_point-1})")
            # é€‰å‡ºå‰ä¸€åŠ
            candidate_ds = he_ds.select(range(0, mid_point))
        else:
            print(f"    ğŸš€ æ¨¡å¼: æœ€ç»ˆæµ‹è¯• (Test)")
            print(f"    âœ‚ï¸ åŸå§‹èŒƒå›´: å {total_len - mid_point} é¢˜ (Index {mid_point}-{total_len-1})")
            # é€‰å‡ºåä¸€åŠ
            candidate_ds = he_ds.select(range(mid_point, total_len))

        # 2. ç¬¬äºŒæ­¥ï¼šåº”ç”¨ start_index å’Œ debug_num è¿›è¡ŒäºŒæ¬¡åˆ‡ç‰‡
        # è·å–å‚æ•°ï¼Œå¸¦é»˜è®¤å€¼å¤„ç†
        p_start = int(cfg.parameters.get("start_index", 0) or 0)
        p_debug = cfg.parameters.get("debug_num") # å¯èƒ½ä¸º None
        
        candidate_len = len(candidate_ds)
        
        # è®¡ç®—åˆ‡ç‰‡ç»ˆç‚¹
        if p_debug:
            limit = int(p_debug)
            p_end = min(p_start + limit, candidate_len)
            print(f"    âœ‚ï¸ [Debug Mode] å¯ç”¨åˆ‡ç‰‡: Relative Index {p_start} -> {p_end} (å…± {p_end - p_start} æ¡)")
        else:
            p_end = candidate_len
            print(f"    ğŸ“Š [Full Mode] å…¨é‡æ¨¡å¼: Relative Index {p_start} -> End ({candidate_len} æ¡)")
            
        # å¼‚å¸¸æ£€æŸ¥ï¼šå¦‚æœ start è¶…è¿‡äº†é•¿åº¦
        if p_start >= candidate_len:
            print(f"    âš ï¸ [Warning] start_index ({p_start}) è¶…å‡ºäº†å½“å‰æ•°æ®é›†é•¿åº¦ ({candidate_len})ï¼Œå°†ç”Ÿæˆç©ºæ–‡ä»¶ï¼")
            final_ds = []
        else:
            # æ‰§è¡Œåˆ‡ç‰‡ (æ³¨æ„ï¼šè¿™é‡Œçš„ range æ˜¯ç›¸å¯¹äº candidate_ds çš„ 0 å¼€å§‹çš„)
            final_ds = candidate_ds.select(range(p_start, p_end))

        # 3. ä¿å­˜æ–‡ä»¶
        os.makedirs(os.path.dirname(test_file), exist_ok=True)
        with open(test_file, 'w', encoding='utf-8') as f:
            for item in final_ds:
                processed = normalize_code_instance(item, dataset_type="humaneval")
                f.write(json.dumps(processed, ensure_ascii=False) + "\n")
        
        print(f"    âœ… HumanEval æµ‹è¯•/éªŒè¯é›† å·²ä¿å­˜: {test_file} (æœ€ç»ˆå†™å…¥ {len(final_ds)} æ¡)")
        
    except Exception as e:
        print(f"    âŒ åŠ è½½ HumanEval å¤±è´¥: {e}")
        # ä¸ºäº†è°ƒè¯•æ–¹ä¾¿ï¼Œæ‰“å°å®Œæ•´çš„é”™è¯¯å †æ ˆ
        import traceback
        traceback.print_exc()
        return False

    return True

