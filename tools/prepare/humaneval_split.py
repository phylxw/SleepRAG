import os
import json
from datasets import load_dataset

def normalize_code_instance(item, dataset_type="humaneval"):
    """
    æ•°æ®æ¸…æ´—ä¸æ ‡å‡†åŒ–
    ğŸ”¥ æ ¸å¿ƒç›®æ ‡ï¼šä¿ç•™åŸå§‹å­—æ®µï¼Œå¹¶æ»¡è¶³ FlashRAG å¯¹ contents å­—æ®µçš„è¦æ±‚
    """
    normalized = {}
    
    if dataset_type == "mbpp":
        # MBPP (sanitized) 
        question = item.get("prompt") or item.get("text")
        answer = item.get("code")
        
        normalized = {
            "id": str(item.get("task_id", "")),
            
            # ğŸ”¥ [æ ¸å¿ƒä¿®å¤] FlashRAG å¼ºåˆ¶è¦æ±‚ 'contents' å­—æ®µç”¨äºæ£€ç´¢å’Œè¯­è¨€æ£€æµ‹
            "contents": question, 
            
            "question": question,
            "golden_answers": [answer] if answer else [],
            **item # ä¿ç•™åŸå§‹å­—æ®µ (test_list ç­‰)
        }
        
    elif dataset_type == "humaneval":
        # HumanEval
        question = item.get("prompt")
        answer = item.get("canonical_solution")
        
        normalized = {
            "id": str(item.get("task_id", "")),
            
            # ğŸ”¥ [æ ¸å¿ƒä¿®å¤] åŒä¸Šï¼ŒåŠ ä¸Š contents
            "contents": question,
            
            "question": question,
            "golden_answers": [answer] if answer else [],
            **item # ä¿ç•™ entry_point ç­‰
        }
    
    return normalized

def prepare_humaneval(corpus_file, test_file, cfg, need_split):
    """
    HumanEval ä¸“ç”¨å‡†å¤‡å‡½æ•°
    
    Args:
        corpus_file: è®°å¿†åº“ä¿å­˜è·¯å¾„ (åº”ä¸º mbpp)
        test_file: æµ‹è¯•é›†ä¿å­˜è·¯å¾„ (åº”ä¸º humaneval split)
        cfg: Hydra é…ç½®å¯¹è±¡
        need_split: (åœ¨æ­¤ä»»åŠ¡ä¸­æš‚ä¸ç”¨äº Corpus åˆ‡åˆ†ï¼Œä¸»è¦é€»è¾‘ç”± cfg æ§åˆ¶)
    """

    print(f"\nğŸ”¨ [Prepare] è¿›å…¥ Code Generation æ•°æ®å‡†å¤‡æµç¨‹...")

    # ==========================================
    # 2. å‡†å¤‡è®°å¿†åº“ (MBPP Sanitized)
    # ==========================================
    if not os.path.exists(corpus_file):
        print(f"ğŸ“š [Corpus] æ­£åœ¨æ„å»º MBPP (sanitized) è®°å¿†åº“...")
        try:
            # ä½¿ç”¨ sanitized ç‰ˆæœ¬ï¼Œè´¨é‡æ›´é«˜ï¼Œé€‚åˆåš RAG åº•åº§
            mbpp_ds = load_dataset("google-research-datasets/mbpp", "sanitized", split="train")
            
            os.makedirs(os.path.dirname(corpus_file), exist_ok=True)
            with open(corpus_file, 'w', encoding='utf-8') as f:
                for item in mbpp_ds:
                    processed = normalize_code_instance(item, dataset_type="mbpp")
                    f.write(json.dumps(processed, ensure_ascii=False) + "\n")
            
            print(f"    âœ… MBPP è®°å¿†åº“å·²ä¿å­˜: {corpus_file} ({len(mbpp_ds)} æ¡)")
        except Exception as e:
            print(f"    âŒ åŠ è½½ MBPP å¤±è´¥: {e}")
            return False


    # ==========================================
    # 3. å‡†å¤‡æµ‹è¯•é›† (HumanEval Split)
    # ==========================================
    print(f"ğŸ§ª [Test] æ­£åœ¨æ„å»º HumanEval æµ‹è¯•é›†...")
    try:
        he_ds = load_dataset("openai_humaneval", split="test") # HumanEval åªæœ‰ test split (164æ¡)
        total_len = len(he_ds)
        mid_point = total_len // 2 # 82
        
        # è¯»å–é…ç½®ä¸­çš„ split æ„å›¾
        # é»˜è®¤ä¸º "test" (ååŠéƒ¨åˆ†)ï¼Œå¦‚æœæ˜¯ "validation" åˆ™å–å‰åŠéƒ¨åˆ†
        target_split = cfg.experiment.get("test_split", "test")
        
        if target_split == "validation":
            print(f"    ğŸš€ æ¨¡å¼: éªŒè¯é›† (Validation)")
            print(f"    âœ‚ï¸ åˆ‡åˆ†: å‰ {mid_point} é¢˜ (Index 0-{mid_point-1})")
            selected_ds = he_ds.select(range(0, mid_point))
        else:
            print(f"    ğŸš€ æ¨¡å¼: æœ€ç»ˆæµ‹è¯• (Test)")
            print(f"    âœ‚ï¸ åˆ‡åˆ†: å {total_len - mid_point} é¢˜ (Index {mid_point}-{total_len-1})")
            selected_ds = he_ds.select(range(mid_point, total_len))

        os.makedirs(os.path.dirname(test_file), exist_ok=True)
        with open(test_file, 'w', encoding='utf-8') as f:
            for item in selected_ds:
                processed = normalize_code_instance(item, dataset_type="humaneval")
                f.write(json.dumps(processed, ensure_ascii=False) + "\n")
        
        print(f"    âœ… HumanEval ({target_split}) å·²ä¿å­˜: {test_file} ({len(selected_ds)} æ¡)")
        
    except Exception as e:
        print(f"    âŒ åŠ è½½ HumanEval å¤±è´¥: {e}")
        return False

    return True

