import os
import json
import re
import time
import numpy as np
import torch
import matplotlib.pyplot as plt
from typing import List, Dict, Any, Optional
import matplotlib.pyplot as plt
import seaborn as sns  # <--- æ–°å¢è¿™è¡Œ
from sentence_transformers import SentenceTransformer

from sklearn.cluster import AgglomerativeClustering, KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score

from transformers import AutoModelForCausalLM, AutoTokenizer

# Optional deps (safe import)
try:
    import umap  # umap-learn
except Exception:
    umap = None

try:
    import hdbscan  # pip install hdbscan
except Exception:
    hdbscan = None

# Hydra
import hydra
from omegaconf import DictConfig

# ================= å…¨å±€å˜é‡ (ä¿æŒåŸé€»è¾‘) =================
GLOBAL_MODEL = None
GLOBAL_TOKENIZER = None
GLOBAL_SGLANG_CLIENT = None


# =============== 0. å·¥å…·å‡½æ•° ===============
def clean_special_chars(text: str) -> str:
    if not isinstance(text, str):
        return text
    return text.replace('\u2028', ' ').replace('\u2029', ' ')

def normalize_text(x: str) -> str:
    """
    NOTE: ä¿æŒåŸæœ‰è¡Œä¸ºï¼ˆå°å†™ã€æ•°å­—å½’ä¸€ã€ç©ºæ ¼è§„æ•´ï¼‰ï¼Œé¿å…æ”¹å˜æ•´ä½“åŠŸèƒ½ã€‚
    å¦‚æœä½ æ•°æ®é‡Œæœ‰å¤§é‡ä¸­æ–‡ï¼Œå¯ä»¥è€ƒè™‘åœ¨è¿™é‡Œé¢å¤–åšå…¨è§’/åŠè§’ã€æ ‡ç‚¹æ¸…æ´—ï¼ˆå¯é€‰ï¼‰ã€‚
    """
    x = str(x).lower()
    x = re.sub(r"\d+(\.\d+)?", " <num> ", x)
    x = re.sub(r"\s+", " ", x).strip()
    return x

def import_torch_and_check_gpu():
    try:
        return torch.cuda.is_available()
    except Exception:
        return False

def _cfg_get(cfg: DictConfig, key: str, default: Any) -> Any:
    """
    OmegaConf å®‰å…¨å–å€¼ï¼šå…¼å®¹æ—§ config æ²¡æœ‰æ–°å­—æ®µçš„æƒ…å†µï¼ˆä¸ä¼šç ´ååŸåŠŸèƒ½ï¼‰ã€‚
    """
    try:
        return cfg.get(key, default)  # DictConfig supports .get
    except Exception:
        return default


# =============== 1. LLM åˆå§‹åŒ–ä¸è°ƒç”¨ (ä¿æŒä¸å˜ï¼Œå·²é€‚é… SGLang) ===============
def init_llm(cfg: DictConfig):
    global GLOBAL_MODEL, GLOBAL_TOKENIZER, GLOBAL_SGLANG_CLIENT

    model_source = cfg.model.source

    if model_source == "gemini":
        import google.generativeai as genai
        api_key = os.environ.get("GEMINI_API_KEY")
        if api_key:
            genai.configure(api_key=api_key)
            print(f"ğŸ¤– [Init] Gemini API ({cfg.model.gemini_name}) å·²é…ç½®")
        else:
            print("âš ï¸ [Init] æœªæ£€æµ‹åˆ° GEMINI_API_KEYï¼ŒGemini æ¨¡å¼å¯èƒ½æ— æ³•å·¥ä½œ")

    elif model_source == "huggingface":
        hf_name = cfg.model.hf_name
        print(f"ğŸ“¥ [Init] æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹: {hf_name} ...")
        try:
            GLOBAL_TOKENIZER = AutoTokenizer.from_pretrained(hf_name, trust_remote_code=True)
            GLOBAL_MODEL = AutoModelForCausalLM.from_pretrained(
                hf_name,
                device_map="auto",
                torch_dtype="auto",
                trust_remote_code=True
            ).eval()
            print("âœ… [Init] æœ¬åœ°æ¨¡å‹åŠ è½½å®Œæˆï¼")
        except Exception as e:
            print(f"âŒ [Init] æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {e}")

    elif model_source == "sglang":
        try:
            from openai import OpenAI
            api_url = cfg.model.get("sglang_api_url", "http://127.0.0.1:30000/v1")
            api_key = "EMPTY"
            GLOBAL_SGLANG_CLIENT = OpenAI(base_url=api_url, api_key=api_key)
            print(f"âœ… [Init] SGLang Client å·²è¿æ¥è‡³ {api_url}")
        except ImportError:
            print("âŒ [Init] ç¼ºå°‘ openai åº“ï¼Œè¯·è¿è¡Œ `pip install openai`")

def call_llm(prompt: str, cfg: DictConfig) -> str:
    model_source = cfg.model.source

    if model_source == "gemini":
        import google.generativeai as genai
        if not os.environ.get("GEMINI_API_KEY"):
            return "Skipped (No Key)"
        model = genai.GenerativeModel(cfg.model.gemini_name)
        try:
            print("  ğŸ¤– [Gemini] æ­£åœ¨æ€è€ƒ...", end="", flush=True)
            resp = model.generate_content(prompt)
            print(" å®Œæˆ!")
            return clean_special_chars(resp.text.strip())
        except Exception as e:
            print(f"\nâŒ [Gemini Error]: {e}")
            time.sleep(1)
            return "Unknown Topic"

    elif model_source == "huggingface":
        if GLOBAL_MODEL is None:
            return "Skipped (Model Not Loaded)"
        try:
            print("  ğŸš€ [Local] æ­£åœ¨æ¨ç†...", end="", flush=True)
            messages = [{"role": "user", "content": prompt}]
            text = GLOBAL_TOKENIZER.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            model_inputs = GLOBAL_TOKENIZER([text], return_tensors="pt").to(GLOBAL_MODEL.device)
            with torch.no_grad():
                generated_ids = GLOBAL_MODEL.generate(model_inputs.input_ids, max_new_tokens=50, do_sample=False)
            generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]
            response = GLOBAL_TOKENIZER.batch_decode(generated_ids, skip_special_tokens=True)[0]
            print(" å®Œæˆ!")
            return clean_special_chars(response.strip())
        except Exception as e:
            print(f"\nâŒ [Local Error]: {e}")
            return "Unknown Topic"

    elif model_source == "sglang":
        if GLOBAL_SGLANG_CLIENT is None:
            return "Skipped (Client Not Initialized)"
        model_name = cfg.model.get("sglang_model_name", "Qwen/Qwen3-4B-Instruct-2507")
        try:
            print("  ğŸš€ [SGLang] æ­£åœ¨æ¨ç†...", end="", flush=True)
            resp = GLOBAL_SGLANG_CLIENT.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
                max_tokens=50
            )
            content = resp.choices[0].message.content
            print(" å®Œæˆ!")
            return clean_special_chars(content.strip())
        except Exception as e:
            print(f"\nâŒ [SGLang Error]: {e}")
            return "Unknown Topic"

    return "Unknown Config"


# =============== 2. åŸºç¡€ IO (å¢å¼ºå¥å£®æ€§) ===============
def load_questions(jsonl_path: str):
    print(f"ğŸ“¥ æ­£åœ¨åŠ è½½æ–‡ä»¶: {jsonl_path}...")
    if not os.path.exists(jsonl_path):
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {jsonl_path}")
        return [], [], []

    ids, questions, raw_contents = [], [], []

    with open(jsonl_path, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
            try:
                obj = json.loads(line)
            except json.JSONDecodeError:
                continue

            content = obj.get("contents", "")
            raw_contents.append(content)

            # ä¿æŒåŸé€»è¾‘ï¼šé»˜è®¤èšç±» Question éƒ¨åˆ†
            if "Question:" in content:
                q_part = content.split("Answer:")[0].replace("Question:", "").strip()
                if not q_part and content:
                    q_part = content
            else:
                q_part = content

            mid = obj.get("memory_id", obj.get("id"))
            ids.append(str(mid))
            questions.append(clean_special_chars(q_part))

    print(f"âœ… åŠ è½½å®Œæˆï¼Œå…± {len(questions)} æ¡æ•°æ®")
    return ids, questions, raw_contents


# =============== 3. ä¸»æµç¨‹ï¼šembedding + èšç±» ===============
def build_embeddings(questions: List[str], model_name: str, device_cfg: str = "cuda") -> np.ndarray:
    print(f"ğŸš€ æ­£åœ¨è®¡ç®— Embeddings ({model_name})...")

    device = device_cfg if (device_cfg == "cuda" and torch.cuda.is_available()) else "cpu"
    print(f"   >>> ä½¿ç”¨è®¾å¤‡: {device}")

    model = SentenceTransformer(model_name, device=device)
    q_norm = [normalize_text(q) for q in questions]

    # normalize_embeddings=True è®©å‘é‡éƒ½è½åœ¨å•ä½çƒé¢ä¸Šï¼šæ›´é€‚åˆ cosine / spherical clustering
    emb = model.encode(
        q_norm,
        batch_size=64,
        show_progress_bar=True,
        normalize_embeddings=True
    )
    return np.asarray(emb)

def preprocess_embeddings_pca(embeddings: np.ndarray, n_components: int) -> np.ndarray:
    print(f"ğŸ§¹ æ­£åœ¨æ‰§è¡Œ PCA é¢„å¤„ç† (é™ç»´: {embeddings.shape[1]} -> {n_components})...")
    if embeddings.shape[0] < n_components:
        print(f"âš ï¸ æ ·æœ¬æ•° ({embeddings.shape[0]}) å°‘äºç›®æ ‡ç»´åº¦ï¼Œè·³è¿‡ PCAã€‚")
        return embeddings

    pca = PCA(n_components=n_components, random_state=42)
    reduced = pca.fit_transform(embeddings)

    explained_variance = float(np.sum(pca.explained_variance_ratio_))
    print(f"   >>> ä¿ç•™æ–¹å·®æ¯”ä¾‹: {explained_variance:.2%}")
    return reduced


def _auto_kmeans(embeddings: np.ndarray, cfg: DictConfig) -> np.ndarray:
    """
    KMeans è‡ªåŠ¨é€‰ Kï¼šç”¨ Silhouette Score é€‰æœ€ä¼˜ Kï¼ˆå¯¹æ—§é…ç½®å…¼å®¹ï¼‰ã€‚
    """
    n = embeddings.shape[0]
    k_min = int(_cfg_get(cfg.cluster, "kmeans_k_min", 2))
    k_max = int(_cfg_get(cfg.cluster, "kmeans_k_max", min(50, max(3, int(np.sqrt(n)) + 5))))
    k_max = min(k_max, n - 1)

    if k_max < k_min:
        print("âš ï¸ æ•°æ®å¤ªå°‘ï¼Œå›é€€åˆ° KMeans(n_clusters=2)")
        model = KMeans(n_clusters=2, random_state=42, n_init='auto')
        return model.fit_predict(embeddings)

    sample_size = int(_cfg_get(cfg.cluster, "silhouette_sample", 2000))
    sample_size = min(sample_size, n)

    best_k, best_score, best_labels = None, -1.0, None
    print(f"ğŸ” [KMeans-Auto] æœç´¢ K in [{k_min}, {k_max}] (silhouette sample={sample_size}) ...")

    for k in range(k_min, k_max + 1):
        km = KMeans(n_clusters=k, random_state=42, n_init='auto')
        labels = km.fit_predict(embeddings)

        # silhouette è¦æ±‚è‡³å°‘ 2 ä¸ªç°‡ä¸”æ¯ç°‡è‡³å°‘ 1 ä¸ªç‚¹
        if len(set(labels)) < 2:
            continue
        try:
            score = silhouette_score(
                embeddings, labels,
                metric="euclidean",
                sample_size=sample_size,
                random_state=42
            )
        except Exception:
            continue

        if score > best_score:
            best_score, best_k, best_labels = score, k, labels

    if best_labels is None:
        print("âš ï¸ [KMeans-Auto] æœªæ‰¾åˆ°æœ‰æ•ˆ Kï¼Œå›é€€åˆ° KMeans(n_clusters=cfg.cluster.kmeans_n_clusters)")
        n_clusters = int(cfg.cluster.kmeans_n_clusters)
        model = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')
        return model.fit_predict(embeddings)

    print(f"âœ… [KMeans-Auto] é€‰æ‹© K={best_k}, silhouette={best_score:.4f}")
    return best_labels


def cluster_questions_auto(embeddings: np.ndarray, cfg: DictConfig) -> np.ndarray:
    """
    æ”¹è¿›ç‚¹ï¼ˆä¸ç ´ååŸåŠŸèƒ½ï¼‰ï¼š
    - å¢åŠ  method=hdbscan / method=kmeans_auto
    - æ”¹è¿› agglomerativeï¼šé»˜è®¤ç”¨ cosine + averageï¼ˆæ›´å¥‘åˆå•ä½å‘é‡ embeddingï¼‰
    """
    method = cfg.cluster.method

    if method == "kmeans":
        n_clusters = int(cfg.cluster.kmeans_n_clusters)
        print(f"ğŸ¤– æ­£åœ¨æ‰§è¡Œ K-Means èšç±» (N_Clusters={n_clusters})...")
        model = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')
        labels = model.fit_predict(embeddings)

    elif method == "kmeans_auto":
        labels = _auto_kmeans(embeddings, cfg)

    elif method == "agglomerative":
        threshold = float(cfg.cluster.distance_threshold)
        linkage = _cfg_get(cfg.cluster, "agglom_linkage", "average")  # ward / average / complete / single
        metric = _cfg_get(cfg.cluster, "agglom_metric", "cosine")     # euclidean / cosine

        # ward åªæ”¯æŒ euclidean
        if linkage == "ward":
            metric = "euclidean"

        print(f"ğŸ¤– æ­£åœ¨æ‰§è¡Œå±‚æ¬¡èšç±» (linkage={linkage}, metric={metric}, threshold={threshold})...")
        model = AgglomerativeClustering(
            n_clusters=None,
            distance_threshold=threshold,
            metric=metric,
            linkage=linkage
        )
        labels = model.fit_predict(embeddings)
        print(f"âœ¨ å±‚æ¬¡èšç±»å‘ç° {len(set(labels))} ä¸ªç±»åˆ«ã€‚")

    elif method == "hdbscan":
        if hdbscan is None:
            print("âŒ æœªå®‰è£… hdbscanï¼Œå›é€€åˆ° agglomerativeã€‚è¯·è¿è¡Œ: pip install hdbscan")
            cfg.cluster.method = "agglomerative"
            return cluster_questions_auto(embeddings, cfg)

        # HDBSCAN å‚æ•°ï¼šæ›´é€‚åˆâ€œç°‡æ•°æœªçŸ¥ + å«å™ªå£°â€çš„è®°å¿†/æ–‡æœ¬æ•°æ®
        min_cluster_size = int(_cfg_get(cfg.cluster, "hdbscan_min_cluster_size", 8))
        min_samples = _cfg_get(cfg.cluster, "hdbscan_min_samples", None)
        metric = _cfg_get(cfg.cluster, "hdbscan_metric", "euclidean")  # euclidean / cosine
        cluster_selection_method = _cfg_get(cfg.cluster, "hdbscan_selection_method", "eom")

        print(f"ğŸ¤– æ­£åœ¨æ‰§è¡Œ HDBSCAN èšç±» "
              f"(min_cluster_size={min_cluster_size}, min_samples={min_samples}, metric={metric})...")
        model = hdbscan.HDBSCAN(
            min_cluster_size=min_cluster_size,
            min_samples=min_samples,
            metric=metric,
            cluster_selection_method=cluster_selection_method
        )
        labels = model.fit_predict(embeddings)
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        noise = int(np.sum(labels == -1))
        print(f"âœ¨ HDBSCAN å‘ç° {n_clusters} ä¸ªç°‡ï¼›å™ªå£°ç‚¹ {noise}/{len(labels)} (label=-1)ã€‚")

    else:
        raise ValueError(f"æœªçŸ¥çš„èšç±»æ–¹æ³•: {method}")

    return labels


# =============== 4. ç»Ÿè®¡ç»˜å›¾ & å…³é”®è¯ & é™ç»´å¯è§†åŒ– ===============
def plot_cluster_stats(labels: np.ndarray, save_path: str, method_name: str):
    print(f"\nğŸ“Š æ­£åœ¨ç”Ÿæˆç»Ÿè®¡å›¾è¡¨...")
    unique_labels, counts = np.unique(labels, return_counts=True)

    # åªç»˜åˆ¶æ•°é‡ > 1 çš„èšç±»ï¼›HDBSCAN çš„å™ªå£°(-1)ä¹Ÿä¸ç»˜åˆ¶
    valid_mask = (counts > 1) & (unique_labels != -1)
    valid_labels = unique_labels[valid_mask]
    valid_counts = counts[valid_mask]

    if len(valid_counts) == 0:
        print("   âš ï¸ æ²¡æœ‰åŒ…å«å¤šä¸ªé—®é¢˜çš„èšç±»ï¼Œè·³è¿‡ç»˜å›¾ã€‚")
        return

    sorted_indices = np.argsort(valid_counts)[::-1]
    sorted_plot_labels = valid_labels[sorted_indices]
    sorted_plot_counts = valid_counts[sorted_indices]

    plt.figure(figsize=(12, 6))
    x_ticks = [str(lbl) for lbl in sorted_plot_labels]

    if len(x_ticks) > 50:
        x_ticks = x_ticks[:50]
        sorted_plot_counts = sorted_plot_counts[:50]

    plt.bar(x_ticks, sorted_plot_counts, color='steelblue', edgecolor='black', alpha=0.8)
    plt.xlabel('Cluster ID')
    plt.ylabel('Count')
    plt.title(f'Top Cluster Size Distribution ({method_name})')
    plt.xticks(rotation=90 if len(x_ticks) > 20 else 0, fontsize=8)
    plt.grid(axis='y', linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig(save_path)
    print(f"ğŸ–¼ï¸ å›¾è¡¨å·²ä¿å­˜è‡³: {save_path}")


def plot_dimensionality_reduction(embeddings: np.ndarray, labels: np.ndarray, cfg: DictConfig, save_path: str):
    """
    æ”¹è¿›ç‚¹ï¼š
    - å…ˆç”¨ PCA é™åˆ° vis_pca_dimsï¼ˆé»˜è®¤ 50ï¼‰æå‡ t-SNE/UMAP ç¨³å®šæ€§ä¸é€Ÿåº¦
    - UMAP é»˜è®¤ç”¨ cosine + æ›´å° min_distï¼ˆæ›´å®¹æ˜“â€œåŒç±»èšåœ¨ä¸€èµ·â€ï¼‰
    - å¯é€‰ supervised UMAPï¼šç”¨å·²æœ‰ labels ä½œä¸º yï¼Œè®©å›¾æ›´â€œç°‡çŠ¶â€ï¼ˆåªå½±å“å¯è§†åŒ–ï¼Œä¸æ”¹å˜èšç±»ç»“æœï¼‰
    """
    method = cfg.cluster.vis_method
    print(f"\nğŸ¨ æ­£åœ¨ç”Ÿæˆ {method.upper()} å¯è§†åŒ–...")

    n = embeddings.shape[0]
    if n < 5:
        return

    X = embeddings
    vis_pca_dims = int(_cfg_get(cfg.cluster, "vis_pca_dims", 50))
    if X.shape[1] > vis_pca_dims and vis_pca_dims > 2:
        X = PCA(n_components=vis_pca_dims, random_state=42).fit_transform(X)

    reducer = None
    if method == "tsne":
        perp = min(int(cfg.cluster.tsne_perplexity), n - 1)
        metric = _cfg_get(cfg.cluster, "tsne_metric", "cosine")
        reducer = TSNE(
            n_components=2,
            perplexity=perp,
            random_state=42,
            init='pca',
            learning_rate='auto',
            metric=metric
        )
        reduced_emb = reducer.fit_transform(X)

    elif method == "pca":
        reducer = PCA(n_components=2, random_state=42)
        reduced_emb = reducer.fit_transform(X)

    elif method == "umap":
        if umap is None:
            print("âŒ ç¼ºå°‘ umap-learnï¼Œå›é€€åˆ° t-SNEã€‚è¯·è¿è¡Œ: pip install umap-learn")
            cfg.cluster.vis_method = "tsne"
            return plot_dimensionality_reduction(embeddings, labels, cfg, save_path)

        n_neighbors = int(_cfg_get(cfg.cluster, "umap_n_neighbors", 15))
        min_dist = float(_cfg_get(cfg.cluster, "umap_min_dist", 0.05))
        metric = _cfg_get(cfg.cluster, "umap_metric", "cosine")
        supervised = bool(_cfg_get(cfg.cluster, "umap_supervised", True))
        target_weight = float(_cfg_get(cfg.cluster, "umap_target_weight", 0.5))

        reducer = umap.UMAP(
            n_components=2,
            random_state=42,
            n_neighbors=min(n_neighbors, n - 1),
            min_dist=min_dist,
            metric=metric,
            target_metric="categorical",
            target_weight=target_weight if supervised else 0.0
        )

        # supervised: UMAP(X, y=labels) ä»…ç”¨äºå›¾åƒæ›´èšç±»åŒ–ï¼›ä¸ä¼šæ”¹å˜ labels æœ¬èº«
        if supervised and labels is not None:
            reduced_emb = reducer.fit_transform(X, y=labels)
        else:
            reduced_emb = reducer.fit_transform(X)

    else:
        print(f"âŒ æœªçŸ¥å¯è§†åŒ–æ–¹æ³•: {method}")
        return

    plt.figure(figsize=(12, 10))

    # å¤„ç† HDBSCAN å™ªå£°ç‚¹ï¼šç°è‰²
    if np.any(labels == -1):
        noise_mask = (labels == -1)
        non_noise = ~noise_mask
        plt.scatter(reduced_emb[noise_mask, 0], reduced_emb[noise_mask, 1], c="lightgray", s=8, alpha=0.5, linewidths=0)
        sc = plt.scatter(reduced_emb[non_noise, 0], reduced_emb[non_noise, 1], c=labels[non_noise], cmap='tab20', s=10, alpha=0.75, linewidths=0)
        plt.colorbar(sc, label='Cluster ID (noise=-1 excluded)')
    else:
        sc = plt.scatter(reduced_emb[:, 0], reduced_emb[:, 1], c=labels, cmap='tab20', s=10, alpha=0.75, linewidths=0)
        plt.colorbar(sc, label='Cluster ID')

    plt.title(f'{method.upper()} Visualization')
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    print(f"ğŸ–¼ï¸ å¯è§†åŒ–ä¿å­˜è‡³: {save_path}")
    
    # [æ–°å¢] è¿”å›é™ç»´åçš„æ•°æ®ï¼Œä¾› KDE ä½¿ç”¨
    return reduced_emb


def plot_top_clusters_kde(reduced_emb: np.ndarray, labels: np.ndarray, save_path: str, top_k: int = 3):
    """
    æŒ‘é€‰æ•°é‡æœ€å¤šçš„ top_k ä¸ªèšç±»ï¼Œåœ¨é™ç»´åçš„ 2D å¹³é¢ä¸Šç»˜åˆ¶ KDE ç­‰é«˜çº¿å›¾ã€‚
    (ä¿®æ­£ç‰ˆï¼šä¿®å¤å›¾ä¾‹ä¸æ˜¾ç¤ºçš„é—®é¢˜)
    """
    print(f"\nğŸŒŠ æ­£åœ¨ç”Ÿæˆ Top-{top_k} èšç±»çš„ KDE å¯†åº¦å›¾...")
    
    if reduced_emb is None or len(reduced_emb) == 0:
        print("âš ï¸ é™ç»´æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡ KDE ç»˜å›¾ã€‚")
        return

    # 1. ç»Ÿè®¡ Top K èšç±» (æ’é™¤å™ªå£° -1)
    unique_labels, counts = np.unique(labels, return_counts=True)
    
    # è¿‡æ»¤æ‰ -1 å’Œæ ·æœ¬æ•°è¿‡å°‘çš„ç±»
    valid_mask = (unique_labels != -1) & (counts >= 5)
    if not np.any(valid_mask):
        print("âš ï¸ æœ‰æ•ˆèšç±»ä¸è¶³ï¼Œè·³è¿‡ KDEã€‚")
        return

    valid_labels = unique_labels[valid_mask]
    valid_counts = counts[valid_mask]
    
    # æŒ‰æ•°é‡é™åºæ’åˆ—
    sorted_indices = np.argsort(valid_counts)[::-1]
    top_labels = valid_labels[sorted_indices][:top_k]
    
    print(f"   ç›®æ ‡èšç±» ID: {top_labels}")

    # 2. ç»˜å›¾
    plt.figure(figsize=(10, 8))
    
    # ç”»èƒŒæ™¯ç°ç‚¹ (Label = Other)
    # æ³¨æ„ï¼šè¿™é‡ŒåŠ  alpha=0.3 è®©èƒŒæ™¯æ·¡ä¸€ç‚¹ï¼Œçªå‡ºå‰æ™¯
    plt.scatter(reduced_emb[:, 0], reduced_emb[:, 1], c='lightgray', s=5, alpha=0.3, label='Other')

    # å¾ªç¯ç”» Top K çš„ KDE å’Œ æ•£ç‚¹
    # ä½¿ç”¨ seaborn é»˜è®¤è°ƒè‰²ç›˜ï¼Œæˆ–è€… tab10
    colors = sns.color_palette("tab10", len(top_labels)) 
    
    for i, cid in enumerate(top_labels):
        # æå–è¯¥èšç±»çš„ç‚¹
        mask = (labels == cid)
        subset = reduced_emb[mask]
        
        # å‡†å¤‡å›¾ä¾‹æ–‡æœ¬
        label_text = f'Cluster {cid} (n={len(subset)})'
        
        try:
            # 1. ç”» KDE (æ™•æŸ“èƒŒæ™¯) - ä¹Ÿå°±æ˜¯é‚£å±‚é›¾
            # æ³¨æ„ï¼šæŠŠ label å»æ‰ï¼Œé¿å…å›¾ä¾‹æ··ä¹±æˆ–ä¸æ˜¾ç¤º
            sns.kdeplot(
                x=subset[:, 0], 
                y=subset[:, 1], 
                fill=True, 
                alpha=0.2,    # é€æ˜åº¦ä½ä¸€ç‚¹ï¼Œä¸è¦é®ä½ç‚¹
                color=colors[i], 
                warn_singular=False
            )
            
            # 2. ç”» æ•£ç‚¹ (å®å¿ƒç‚¹) - æŠŠ Label åŠ åœ¨è¿™é‡Œï¼
            # è¿™æ ·å›¾ä¾‹é‡Œå°±ä¼šå‡ºç°ä¸€ä¸ªé¢œè‰²å¯¹åº”çš„å®å¿ƒåœ†ç‚¹ï¼Œéå¸¸æ¸…æ™°
            plt.scatter(
                subset[:, 0], 
                subset[:, 1], 
                s=10, 
                color=colors[i], 
                alpha=0.8, 
                label=label_text  # <--- å…³é”®ä¿®æ”¹ï¼šLabel ç§»åˆ°è¿™é‡Œ
            )
            
        except Exception as e:
            print(f"   âš ï¸ Cluster {cid} ç”»å›¾å¤±è´¥: {e}")

    plt.title(f'KDE Density Plot for Top {len(top_labels)} Clusters')
    
    # å¼ºåˆ¶æ˜¾ç¤ºå›¾ä¾‹ï¼Œä½ç½®æ”¾åœ¨æœ€ä½³ä½ç½®
    plt.legend(loc='best')
    plt.tight_layout()
    
    # ä¿å­˜
    kde_save_path = save_path.replace(".png", "_kde.png")
    plt.savefig(kde_save_path, dpi=300)
    print(f"ğŸ–¼ï¸ KDE å›¾è¡¨å·²ä¿å­˜è‡³: {kde_save_path}")

def tfidf_keywords_per_cluster(questions, cluster_labels, max_features=5000, top_k=10):
    print("ğŸ” æå–å…³é”®è¯...")
    q_norm = [normalize_text(q) for q in questions]
    try:
        vectorizer = TfidfVectorizer(max_df=0.9, min_df=2, max_features=max_features, stop_words="english")
        X = vectorizer.fit_transform(q_norm)
        vocab = np.array(vectorizer.get_feature_names_out())

        cluster_keywords = {}
        for cid in np.unique(cluster_labels):
            if cid == -1:
                continue
            idx = np.where(cluster_labels == cid)[0]
            if len(idx) < 2:
                continue
            tfidf_mean = np.asarray(X[idx].mean(axis=0)).ravel()
            top_idx = tfidf_mean.argsort()[::-1][:top_k]
            cluster_keywords[cid] = vocab[top_idx].tolist()
        return cluster_keywords
    except ValueError:
        return {}

def llm_label_cluster(cid, questions, cluster_labels, cluster_keywords, cfg: DictConfig, max_examples=5):
    idx = np.where(cluster_labels == cid)[0]
    examples_idx = np.random.choice(idx, min(len(idx), max_examples), replace=False)
    examples = [questions[i] for i in examples_idx]
    kw = ", ".join(cluster_keywords.get(cid, []))

    prompt = f"""You are a Math Education Expert.
I have grouped similar math problems together.
Keywords: [{kw}]
Examples:
{chr(10).join(f"- {q}" for q in examples)}

Task: Provide a **very short category name** (3-6 words) for this problem type.
Output ONLY the category name. Do not explain.
"""
    callback = call_llm(prompt, cfg)
    print('æˆ‘çš„è¾“å‡ºæ˜¯è¿™æ ·çš„')
    print(callback)
    return callback.replace('"', "").strip()


# =============== Main (Hydra Integrated) ===============
@hydra.main(version_base=None, config_path="conf", config_name="config")
def cluster(cfg: DictConfig):

    # 0. åˆå§‹åŒ–
    init_llm(cfg)

    # 1. è·¯å¾„æ˜ å°„ (ä» yaml è¯»å–)
    input_file = cfg.paths.freq_file
    output_file = cfg.paths.cluster_output
    summary_file = cfg.paths.cluster_summary
    plot_file = cfg.paths.cluster_plot
    vis_plot_file = cfg.paths.cluster_vis

    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    os.makedirs(os.path.dirname(plot_file), exist_ok=True)

    ids, questions, raw_contents = load_questions(input_file)
    if not ids:
        return

    # 2. Embedding
    embeddings = build_embeddings(questions, cfg.model.embedding_name, cfg.model.device)

    # 2.1 PCA é¢„å¤„ç†ï¼ˆä¿æŒåŸå¼€å…³ï¼‰
    if cfg.cluster.enable_pca_preprocess:
        embeddings = preprocess_embeddings_pca(embeddings, n_components=int(cfg.cluster.pca_preprocess_dims))

    # 3. èšç±»ï¼ˆæ–°å¢ hdbscan / kmeans_autoï¼Œä½†ä¸å½±å“åŸæ–¹æ³•ï¼‰
    labels = cluster_questions_auto(embeddings, cfg)

    # 4. ç»Ÿè®¡ä¸å¯è§†åŒ–
    plot_cluster_stats(labels, save_path=plot_file, method_name=cfg.cluster.method)
    
    # [ä¿®æ”¹] æ¥æ”¶è¿”å›çš„ reduced_emb
    reduced_emb = plot_dimensionality_reduction(embeddings, labels, cfg, save_path=vis_plot_file)

    # [æ–°å¢] è°ƒç”¨ KDE ç»˜å›¾
    # è¿™é‡Œçš„ top_k å¯ä»¥å†™æ­»ä¸º 3ï¼Œæˆ–è€…ä» cfg è¯»å–
    if reduced_emb is not None:
        plot_top_clusters_kde(reduced_emb, labels, save_path=vis_plot_file, top_k=3)

    # 5. åˆ†æå…³é”®è¯ä¸å‘½å
    keywords_map = tfidf_keywords_per_cluster(questions, labels)

    print("\n" + "=" * 20 + " èšç±»ç»“æœåˆ†æ " + "=" * 20)
    unique, counts = np.unique(labels, return_counts=True)
    sorted_clusters = sorted(zip(unique, counts), key=lambda x: x[1], reverse=True)

    cluster_labels_text = {}
    print(f"ğŸ“Š æ€»å…±å‘ç° {len(sorted_clusters)} ä¸ªèšç±»ï¼ˆå«å™ªå£°=-1ï¼‰ã€‚")
    print("   (æ­£åœ¨ä¸º Top 10 çƒ­é—¨èšç±»ç”Ÿæˆ LLM å‘½å...)\n")

    for cid, count in sorted_clusters[:10]:
        # å¯¹å™ªå£°ç‚¹ä¸å‘½å
        if cid == -1:
            continue
        label_text = llm_label_cluster(cid, questions, labels, keywords_map, cfg)
        cluster_labels_text[int(cid)] = label_text
        print(f"   ğŸ·ï¸ Cluster {cid} ({count} é¢˜): {label_text}")
        if cfg.model.source == "gemini":
            time.sleep(1)

    # 6. ä¿å­˜è¯¦ç»†ç»“æœ
    print(f"\nğŸ’¾ ä¿å­˜è¯¦ç»†ç»“æœåˆ° {output_file}...")
    with open(output_file, "w", encoding="utf-8") as f:
        for qid, q, raw, cid in zip(ids, questions, raw_contents, labels):
            obj = {
                "id": qid,
                "contents": raw,
                "cluster_id": int(cid),
                "cluster_label": cluster_labels_text.get(int(cid), f"Cluster {cid}"),
                "cluster_keywords": keywords_map.get(int(cid), [])
            }
            f.write(json.dumps(obj, ensure_ascii=False) + "\n")

    # 7. ä¿å­˜æ‘˜è¦ç»“æœ
    print(f"ğŸ’¾ ä¿å­˜èšç±»æ‘˜è¦åˆ° {summary_file}...")
    cluster_aggregation = {}
    for qid, cid in zip(ids, labels):
        cid_int = int(cid)
        if cid_int not in cluster_aggregation:
            cluster_aggregation[cid_int] = {
                "cluster_id": cid_int,
                "cluster_label": cluster_labels_text.get(cid_int, f"Cluster {cid_int}"),
                "count": 0,
                "memory_ids": []
            }
        cluster_aggregation[cid_int]["memory_ids"].append(qid)
        cluster_aggregation[cid_int]["count"] += 1

    with open(summary_file, "w", encoding="utf-8") as f:
        for cid in sorted(cluster_aggregation.keys(), key=lambda k: cluster_aggregation[k]['count'], reverse=True):
            f.write(json.dumps(cluster_aggregation[cid], ensure_ascii=False) + "\n")

    print("âœ… å…¨éƒ¨å®Œæˆï¼")

if __name__ == "__main__":
    cluster()
