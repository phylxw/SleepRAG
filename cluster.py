import os
import json
import re
import time
import numpy as np
import torch
import google.generativeai as genai
import matplotlib.pyplot as plt 
from typing import List, Dict
from sentence_transformers import SentenceTransformer
from sklearn.cluster import AgglomerativeClustering 
from sklearn.feature_extraction.text import TfidfVectorizer
from transformers import AutoModelForCausalLM, AutoTokenizer

# ================= é…ç½®åŒºåŸŸ =================

# 1. æ ¸å¿ƒå¼€å…³: é€‰æ‹©èµ·åå­—çš„æ¨¡å‹æ¥æº
# é€‰é¡¹: 'huggingface' (æœ¬åœ°æ˜¾å¡è·‘) æˆ– 'gemini' (è°·æ­ŒAPI)
MODEL_SOURCE = "huggingface" 

# [HuggingFace é…ç½®]
HF_MODEL_NAME = "Qwen/Qwen3-4B-Instruct-2507" 

# [Gemini é…ç½®]
GEMINI_MODEL_NAME = "gemini-2.5-flash"
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")

# 2. æ–‡ä»¶é…ç½®
INPUT_FILE = "MATH-lighteval_corpus.jsonl" #å…¶ä»–çš„ç›´æ¥gsm8kæ”¹mathå°±è¡Œäº†
# INPUT_FILE = "gsm8k_corpus.jsonl"
# è¾“å‡ºæ–‡ä»¶ 1: è¯¦ç»†ç»“æœ (æ¯è¡Œä¸€é“é¢˜ï¼ŒåŒ…å«å…¶ç±»åˆ«)
OUTPUT_FILE = "MATH-lighteval_auto_clustered_result.jsonl"
# è¾“å‡ºæ–‡ä»¶ 2: èšç±»æ‘˜è¦ (æ¯è¡Œä¸€ä¸ªç±»ï¼ŒåŒ…å«è¯¥ç±»ä¸‹æ‰€æœ‰é¢˜å·) -> ğŸ”¥ æ–°å¢
SUMMARY_OUTPUT_FILE = "MATH-lighteval_cluster_summary.jsonl"
# è¾“å‡ºæ–‡ä»¶ 3: ç»Ÿè®¡å›¾è¡¨
PLOT_FILE = "MATH-lighteval_cluster_distribution.png"

# 3. èšç±»å‚æ•°
DISTANCE_THRESHOLD = 1.0  # è·ç¦»é˜ˆå€¼
EMBEDDING_MODEL = "BAAI/bge-large-en-v1.5" 
# ===========================================

# å…¨å±€å˜é‡ç”¨äºå­˜å‚¨æœ¬åœ°æ¨¡å‹ï¼Œé˜²æ­¢é‡å¤åŠ è½½
GLOBAL_MODEL = None
GLOBAL_TOKENIZER = None

# =============== 0. å·¥å…·å‡½æ•° ===============
def clean_special_chars(text: str) -> str:
    """æ¸…æ´—å¼‚å¸¸å­—ç¬¦"""
    if not isinstance(text, str): return text
    return text.replace('\u2028', ' ').replace('\u2029', ' ')

def normalize_text(x: str) -> str:
    x = x.lower()
    x = re.sub(r"\d+(\.\d+)?", " <num> ", x) 
    x = re.sub(r"\s+", " ", x).strip()
    return x

def import_torch_and_check_gpu():
    try: return torch.cuda.is_available()
    except: return False

# =============== 1. LLM åˆå§‹åŒ–ä¸è°ƒç”¨ ===============

def init_llm():
    """åˆå§‹åŒ– LLM (ä»…é’ˆå¯¹æœ¬åœ°æ¨¡å‹)"""
    global GLOBAL_MODEL, GLOBAL_TOKENIZER
    
    if MODEL_SOURCE == "gemini":
        if GEMINI_API_KEY:
            genai.configure(api_key=GEMINI_API_KEY)
            print(f"ğŸ¤– [Init] Gemini API ({GEMINI_MODEL_NAME}) å·²é…ç½®")
        else:
            print("âš ï¸ [Init] æœªæ£€æµ‹åˆ° GEMINI_API_KEYï¼ŒGemini æ¨¡å¼å¯èƒ½æ— æ³•å·¥ä½œ")
            
    elif MODEL_SOURCE == "huggingface":
        print(f"ğŸ“¥ [Init] æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹: {HF_MODEL_NAME} ...")
        try:
            GLOBAL_TOKENIZER = AutoTokenizer.from_pretrained(HF_MODEL_NAME, trust_remote_code=True)
            GLOBAL_MODEL = AutoModelForCausalLM.from_pretrained(
                HF_MODEL_NAME,
                device_map="auto",
                torch_dtype="auto",
                trust_remote_code=True
            ).eval()
            print("âœ… [Init] æœ¬åœ°æ¨¡å‹åŠ è½½å®Œæˆï¼")
        except Exception as e:
            print(f"âŒ [Init] æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ’¡ æç¤º: è¯·ç¡®ä¿å·²é€šè¿‡ `huggingface-cli login` ç™»å½•æˆ–æ£€æŸ¥ç½‘ç»œ")

def call_llm(prompt: str) -> str:
    """ç»Ÿä¸€ LLM è°ƒç”¨æ¥å£"""
    
    # --- åˆ†æ”¯ A: Gemini ---
    if MODEL_SOURCE == "gemini":
        if not GEMINI_API_KEY: return "Skipped (No Key)"
        model = genai.GenerativeModel(GEMINI_MODEL_NAME) 
        try:
            print("  ğŸ¤– [Gemini] æ­£åœ¨æ€è€ƒ...", end="", flush=True)
            resp = model.generate_content(prompt)
            print(" å®Œæˆ!")
            return clean_special_chars(resp.text.strip())
        except Exception as e:
            print(f"\nâŒ [Gemini Error]: {e}")
            time.sleep(1)
            return "Unknown Topic"

    # --- åˆ†æ”¯ B: HuggingFace (æœ¬åœ°) ---
    elif MODEL_SOURCE == "huggingface":
        if GLOBAL_MODEL is None:
            return "Skipped (Model Not Loaded)"
        
        try:
            print("  ğŸš€ [Local] æ­£åœ¨æ¨ç†...", end="", flush=True)
            
            messages = [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt}
            ]
            text = GLOBAL_TOKENIZER.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            model_inputs = GLOBAL_TOKENIZER([text], return_tensors="pt").to(GLOBAL_MODEL.device)

            with torch.no_grad():
                generated_ids = GLOBAL_MODEL.generate(
                    model_inputs.input_ids,
                    max_new_tokens=50, 
                    do_sample=False    
                )
            
            generated_ids = [
                output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
            ]
            response = GLOBAL_TOKENIZER.batch_decode(generated_ids, skip_special_tokens=True)[0]
            
            print(" å®Œæˆ!")
            return clean_special_chars(response.strip())
            
        except Exception as e:
            print(f"\nâŒ [Local Error]: {e}")
            return "Unknown Topic"
            
    return "Unknown Config"

# =============== 2. åŸºç¡€ IO ===============
def load_questions(jsonl_path: str):
    print(f"ğŸ“¥ æ­£åœ¨åŠ è½½æ–‡ä»¶: {jsonl_path}...")
    if not os.path.exists(jsonl_path):
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {jsonl_path}")
        return [], []

    ids, questions = [], []
    with open(jsonl_path, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip(): continue
            try:
                obj = json.loads(line)
            except json.JSONDecodeError: continue
            
            content = obj.get("contents", "")
            if "Question:" in content:
                q_part = content.split("Solution:")[0].replace("Question:", "").strip()
            else:
                q_part = content
            
            ids.append(str(obj["id"]))
            questions.append(clean_special_chars(q_part))
            
    print(f"âœ… åŠ è½½å®Œæˆï¼Œå…± {len(questions)} æ¡æ•°æ®")
    return ids, questions

# =============== 3. ä¸»æµç¨‹ï¼šembedding + è‡ªåŠ¨èšç±» ===============
def build_embeddings(questions: List[str], model_name: str) -> np.ndarray:
    print(f"ğŸš€ æ­£åœ¨è®¡ç®— Embeddings ({model_name})...")
    device = "cuda" if import_torch_and_check_gpu() else "cpu"
    print(f"   >>> ä½¿ç”¨è®¾å¤‡: {device}")
    
    model = SentenceTransformer(model_name, device=device)
    q_norm = [normalize_text(q) for q in questions]
    emb = model.encode(q_norm, batch_size=32, show_progress_bar=True, normalize_embeddings=True)
    return np.asarray(emb)

def cluster_questions_auto(embeddings: np.ndarray, threshold: float) -> np.ndarray:
    print(f"ğŸ¤– æ­£åœ¨æ‰§è¡Œè‡ªåŠ¨èšç±» (Distance Threshold={threshold})...")
    
    model = AgglomerativeClustering(
        n_clusters=None, 
        distance_threshold=threshold,
        metric='euclidean', 
        linkage='ward'
    )
    labels = model.fit_predict(embeddings)
    
    n_clusters_found = len(set(labels))
    print(f"âœ¨ è‡ªåŠ¨èšç±»å®Œæˆï¼æ¨¡å‹è‡ªåŠ¨å‘ç°äº† {n_clusters_found} ä¸ªé¢˜å‹ç±»åˆ«ã€‚")
    return labels

# =============== 4. ç»Ÿè®¡ç»˜å›¾ & å…³é”®è¯ ===============

def plot_cluster_stats(labels: np.ndarray, save_path: str):
    print(f"\nğŸ“Š æ­£åœ¨ç”Ÿæˆç»Ÿè®¡å›¾è¡¨...")
    unique_labels, counts = np.unique(labels, return_counts=True)
    
    singleton_mask = counts == 1
    num_singletons = np.sum(singleton_mask)
    
    valid_mask = ~singleton_mask
    valid_labels = unique_labels[valid_mask]
    valid_counts = counts[valid_mask]
    
    print(f"   - æ€»èšç±»æ•°: {len(unique_labels)}")
    print(f"   - å­¤ç«‹èšç±»æ•° (Size=1): {num_singletons} (è¿™éƒ¨åˆ†ä¸ç”»åœ¨å›¾é‡Œ)")
    print(f"   - æœ‰æ•ˆèšç±»æ•° (Size>1): {len(valid_labels)}")
    
    if len(valid_counts) == 0:
        print("   âš ï¸ æ²¡æœ‰åŒ…å«å¤šä¸ªé—®é¢˜çš„èšç±»ï¼Œè·³è¿‡ç»˜å›¾ã€‚")
        return

    sorted_indices = np.argsort(valid_counts)[::-1]
    sorted_plot_labels = valid_labels[sorted_indices]
    sorted_plot_counts = valid_counts[sorted_indices]
    
    plt.figure(figsize=(12, 6))
    x_ticks = [str(lbl) for lbl in sorted_plot_labels]
    plt.bar(x_ticks, sorted_plot_counts, color='steelblue', edgecolor='black', alpha=0.8)
    plt.xlabel('Cluster ID', fontsize=12)
    plt.ylabel('Number of Questions', fontsize=12)
    plt.title(f'Cluster Size Distribution (Descending)\n(Excluding {num_singletons} singleton clusters)', fontsize=14)
    if len(x_ticks) > 30: plt.xticks(rotation=90, fontsize=8)
    else: plt.xticks(rotation=0)
    plt.grid(axis='y', linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig(save_path)
    print(f"ğŸ–¼ï¸ å›¾è¡¨å·²ä¿å­˜è‡³: {save_path}")

def tfidf_keywords_per_cluster(questions, cluster_labels, max_features=5000, top_k=10):
    print("ğŸ” æå–å…³é”®è¯...")
    q_norm = [normalize_text(q) for q in questions]
    vectorizer = TfidfVectorizer(max_df=0.9, min_df=3, max_features=max_features, stop_words="english")
    X = vectorizer.fit_transform(q_norm)
    vocab = np.array(vectorizer.get_feature_names_out())

    cluster_keywords = {}
    for cid in np.unique(cluster_labels):
        idx = np.where(cluster_labels == cid)[0]
        if len(idx) == 0: continue
        tfidf_mean = np.asarray(X[idx].mean(axis=0)).ravel()
        top_idx = tfidf_mean.argsort()[::-1][:top_k]
        cluster_keywords[cid] = vocab[top_idx].tolist()
    return cluster_keywords

def llm_label_cluster(cid, questions, cluster_labels, cluster_keywords, max_examples=5):
    idx = np.where(cluster_labels == cid)[0]
    examples_idx = np.random.choice(idx, min(len(idx), max_examples), replace=False)
    examples = [questions[i] for i in examples_idx]
    kw = ", ".join(cluster_keywords.get(cid, []))

    prompt = f"""You are a Math Education Expert. 
I have automatically grouped similar math problems together.
Keywords: [{kw}]
Examples:
{chr(10).join(f"- {q}" for q in examples)}

Task: Provide a **very short category name** (3-6 words) for this specific math problem type.
Output ONLY the category name.
"""
    label = call_llm(prompt)
    return label.replace("\n", "").replace('"', "").strip()

# =============== Main ===============
def cluster():
    # 0. åˆå§‹åŒ–
    init_llm()

    # 1. åŠ è½½æ•°æ®
    ids, questions = load_questions(INPUT_FILE)
    if not ids: return

    # 2. Embedding
    embeddings = build_embeddings(questions, model_name=EMBEDDING_MODEL)
    
    # 3. è‡ªåŠ¨èšç±»
    labels = cluster_questions_auto(embeddings, threshold=DISTANCE_THRESHOLD)

    # 4. ç”»å›¾
    plot_cluster_stats(labels, save_path=PLOT_FILE)

    # 5. åˆ†æå…³é”®è¯
    keywords_map = tfidf_keywords_per_cluster(questions, labels)
    
    print("\n" + "="*20 + " èšç±»ç»“æœåˆ†æ " + "="*20)
    cluster_labels_text = {}
    
    unique, counts = np.unique(labels, return_counts=True)
    # æŒ‰æ•°é‡é™åºæ’åº
    sorted_clusters = sorted(zip(unique, counts), key=lambda x: x[1], reverse=True)
    
    print(f"ğŸ“Š æ€»å…±å‘ç° {len(sorted_clusters)} ä¸ªèšç±»ã€‚")
    print("   (ä»…å±•ç¤ºå¹¶å‘½ååŒ…å«é¢˜ç›®æœ€å¤šçš„å‰ 10 ä¸ªèšç±»)\n")

    for cid, count in sorted_clusters[:10]:
        print(f"\nğŸ·ï¸ åˆ†æ Cluster {cid} (åŒ…å« {count} é¢˜)...")
        label_text = llm_label_cluster(cid, questions, labels, keywords_map)
        cluster_labels_text[cid] = label_text
        print(f"   >>> é¢˜å‹: {label_text}")
        print(f"   >>> å…³é”®è¯: {keywords_map.get(cid, [])}")
        if MODEL_SOURCE == "gemini": time.sleep(2)

    # 6. ä¿å­˜è¯¦ç»†ç»“æœ (åŸåŠŸèƒ½)
    print(f"\nğŸ’¾ ä¿å­˜è¯¦ç»†ç»“æœåˆ° {OUTPUT_FILE}...")
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        for qid, q, cid in zip(ids, questions, labels):
            obj = {
                "id": qid,
                "question": q,
                "cluster_id": int(cid),
                "cluster_label": cluster_labels_text.get(int(cid), f"Cluster {cid}"),
                "cluster_keywords": keywords_map.get(int(cid), [])
            }
            f.write(json.dumps(obj, ensure_ascii=False) + "\n")
            
    # 7. ğŸ”¥ æ–°å¢ï¼šä¿å­˜èšç±»æ‘˜è¦ç´¢å¼•è¡¨
    print(f"ğŸ’¾ ä¿å­˜èšç±»æ‘˜è¦åˆ° {SUMMARY_OUTPUT_FILE}...")
    
    # æ„é€ èšåˆæ•°æ® {cluster_id: {label, ids}}
    cluster_aggregation = {}
    for qid, cid in zip(ids, labels):
        cid_int = int(cid)
        if cid_int not in cluster_aggregation:
            cluster_aggregation[cid_int] = {
                "cluster_id": cid_int,
                "cluster_label": cluster_labels_text.get(cid_int, f"Cluster {cid_int}"),
                "memory_ids": []
            }
        cluster_aggregation[cid_int]["memory_ids"].append(qid)
    
    # å†™å…¥æ–‡ä»¶
    with open(SUMMARY_OUTPUT_FILE, "w", encoding="utf-8") as f:
        # æŒ‰ cluster_id æ’åºå†™å…¥ï¼Œæ–¹ä¾¿æŸ¥çœ‹
        for cid in sorted(cluster_aggregation.keys()):
            f.write(json.dumps(cluster_aggregation[cid], ensure_ascii=False) + "\n")
            
    print("âœ… å…¨éƒ¨å®Œæˆï¼")

if __name__ == "__main__":
    cluster()