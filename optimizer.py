import os
import json
import time
from typing import Dict, List, Tuple, Set

import numpy as np
import torch
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
import google.generativeai as genai

# Hydra
import hydra
from omegaconf import DictConfig

# ================= å…¨å±€å˜é‡ (ä¿æŒåŸé€»è¾‘) =================
GLOBAL_MODEL = None
GLOBAL_TOKENIZER = None
GLOBAL_SGLANG_CLIENT = None
# =============== å·¥å…·å‡½æ•° ===============

def clean_special_chars(text: str) -> str:
    if not isinstance(text, str):
        return text
    return text.replace('\u2028', ' ').replace('\u2029', ' ')


def has_cuda() -> bool:
    try:
        return torch.cuda.is_available()
    except Exception:
        return False


# =============== LLM åˆå§‹åŒ–ä¸è°ƒç”¨ ===============

# ================= å…¨å±€å˜é‡ =================
GLOBAL_MODEL = None
GLOBAL_TOKENIZER = None
GLOBAL_SGLANG_CLIENT = None  # ğŸ”¥ æ–°å¢

def init_llm(cfg: DictConfig):
    """åˆå§‹åŒ– LLM"""
    global GLOBAL_MODEL, GLOBAL_TOKENIZER, GLOBAL_SGLANG_CLIENT
    
    model_source = cfg.model.source

    if model_source == "gemini":
        api_key = os.environ.get("GEMINI_API_KEY")
        if api_key:
            import google.generativeai as genai
            genai.configure(api_key=api_key)
            print(f"ğŸ¤– [Init] Gemini API ({cfg.model.gemini_name}) å·²é…ç½®")
        else:
            print("âš ï¸ [Init] æœªæ£€æµ‹åˆ° GEMINI_API_KEYï¼ŒGemini ç›¸å…³åŠŸèƒ½ä¼šè¢«è·³è¿‡")
            
    elif model_source == "huggingface":
        hf_name = cfg.model.hf_name
        print(f"ğŸ“¥ [Init] æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹: {hf_name} ...")
        try:
            GLOBAL_TOKENIZER = AutoTokenizer.from_pretrained(hf_name, trust_remote_code=True)
            GLOBAL_MODEL = AutoModelForCausalLM.from_pretrained(
                hf_name,
                device_map="auto",
                torch_dtype="auto",
                trust_remote_code=True
            ).eval()
            
            # ğŸ”¥ [Critical Fix] æ‰¹é‡ç”Ÿæˆå¿…é¡»è®¾ç½® left padding
            GLOBAL_TOKENIZER.padding_side = 'left'
            if GLOBAL_TOKENIZER.pad_token is None:
                GLOBAL_TOKENIZER.pad_token = GLOBAL_TOKENIZER.eos_token
                GLOBAL_TOKENIZER.pad_token_id = GLOBAL_TOKENIZER.eos_token_id
            
            print(f"âœ… [Init] æœ¬åœ°æ¨¡å‹åŠ è½½å®Œæˆï¼(Padding side set to left)")
        except Exception as e:
            print(f"âŒ [Init] æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ’¡ æç¤º: è¯·æ£€æŸ¥ HuggingFace æƒé™å’Œç½‘ç»œ")

    elif model_source == "sglang":
        try:
            from openai import OpenAI
            # ä»é…ç½®è¯»å– URLï¼Œé»˜è®¤æœ¬åœ°ç«¯å£
            api_url = cfg.model.get("sglang_api_url", "http://127.0.0.1:30000/v1")
            api_key = "EMPTY" # SGLang æœ¬åœ°éƒ¨ç½²ä¸éœ€è¦çœŸå® Key
            
            GLOBAL_SGLANG_CLIENT = OpenAI(base_url=api_url, api_key=api_key)
            print(f"âœ… [Init] SGLang Client å·²è¿æ¥è‡³ {api_url}")
        except ImportError:
            print("âŒ [Init] ç¼ºå°‘ openai åº“ï¼Œè¯·è¿è¡Œ `pip install openai`")


def call_llm(prompt: str, cfg: DictConfig, max_new_tokens: int = None) -> str:
    """ç»Ÿä¸€çš„å¤§æ¨¡å‹è°ƒç”¨æ¥å£ï¼Œå•æ¡è°ƒç”¨"""
    model_source = cfg.model.source
    # å¦‚æœæ²¡ä¼  max_new_tokensï¼Œå°±ç”¨ config é‡Œçš„é»˜è®¤å€¼
    if max_new_tokens is None:
        max_new_tokens = cfg.model.max_new_tokens

    # --- Gemini ---
    if model_source == "gemini":
        if not os.environ.get("GEMINI_API_KEY"):
            return "Skipped (No GEMINI_API_KEY)"
        try:
            import google.generativeai as genai
            model = genai.GenerativeModel(cfg.model.gemini_name)
            print("  ğŸ¤– [Gemini] æ­£åœ¨ç”Ÿæˆ...", end="", flush=True)
            resp = model.generate_content(prompt)
            print(" å®Œæˆ")
            return clean_special_chars(resp.text.strip())
        except Exception as e:
            print(f"\nâŒ [Gemini Error]: {e}")
            return ""

    # --- HuggingFace æœ¬åœ° ---
    elif model_source == "huggingface":
        if GLOBAL_MODEL is None:
            print("âš ï¸ [Local] LLM å°šæœªåˆå§‹åŒ–")
            return ""

        try:
            print("  ğŸš€ [Local] æ­£åœ¨ç”Ÿæˆ...", end="", flush=True)
            messages = [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt}
            ]
            text = GLOBAL_TOKENIZER.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            model_inputs = GLOBAL_TOKENIZER(
                [text],
                return_tensors="pt",
                truncation=True,
                max_length=cfg.model.max_input_len,
            ).to(GLOBAL_MODEL.device)

            with torch.no_grad():
                generated_ids = GLOBAL_MODEL.generate(
                    model_inputs.input_ids,
                    attention_mask=model_inputs.attention_mask,
                    max_new_tokens=max_new_tokens,
                    do_sample=False
                )
            # åªå–æ–°å¢çš„éƒ¨åˆ†
            generated_ids = [
                output_ids[len(input_ids):]
                for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
            ]
            response = GLOBAL_TOKENIZER.batch_decode(generated_ids, skip_special_tokens=True)[0]
            print(" å®Œæˆ")
            return clean_special_chars(response.strip())
        except Exception as e:
            print(f"\nâŒ [Local Error]: {e}")
            return ""

    # --- SGLang ---
    elif model_source == "sglang":
        if GLOBAL_SGLANG_CLIENT is None:
            return "Skipped (Client Not Initialized)"
        
        model_name = cfg.model.get("sglang_model_name", "Qwen/Qwen3-4B-Instruct-2507")
        try:
            print("  ğŸš€ [SGLang] æ­£åœ¨æ¨ç†...", end="", flush=True)
            resp = GLOBAL_SGLANG_CLIENT.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.0,
                max_tokens=max_new_tokens
            )
            content = resp.choices[0].message.content
            print(" å®Œæˆ")
            return clean_special_chars(content.strip())
        except Exception as e:
            print(f"\nâŒ [SGLang Error]: {e}")
            return ""

    return ""


def call_llm_batch(prompts: List[str], cfg: DictConfig, max_new_tokens: int = None) -> List[str]:
    """æ‰¹é‡è°ƒç”¨ LLM"""
    if not prompts:
        return []
    
    model_source = cfg.model.source
    if max_new_tokens is None:
        max_new_tokens = cfg.model.max_new_tokens

    # Geminiï¼šç®€å•å¾ªç¯
    if model_source == "gemini":
        results = []
        for p in prompts:
            results.append(call_llm(p, cfg, max_new_tokens=max_new_tokens))
        return results

    # SGLang: ç®€å•å¾ªç¯è°ƒç”¨ (Serverç«¯ä¼šè‡ªåŠ¨å¤„ç†å¹¶å‘)
    if model_source == "sglang":
        results = []
        # è™½ç„¶è¿™é‡Œå†™çš„æ˜¯å¾ªç¯ï¼Œä½† SGLang Server çš„ååå¾ˆé«˜ï¼Œé€Ÿåº¦é€šå¸¸æ¯”æœ¬åœ° HF Batch å¿«
        # å¦‚æœéœ€è¦æè‡´å¹¶å‘ï¼Œå¯ä»¥ä½¿ç”¨ asyncio æˆ– ThreadPoolExecutorï¼Œä½†ç®€å•å¾ªç¯é€šå¸¸è¶³å¤Ÿå¿«ä¸”ç¨³å®š
        for p in prompts:
            results.append(call_llm(p, cfg, max_new_tokens=max_new_tokens))
        return results

    # HuggingFace æœ¬åœ°
    if model_source == "huggingface":
        if GLOBAL_MODEL is None:
            print("âš ï¸ [Local] LLM å°šæœªåˆå§‹åŒ–")
            return [""] * len(prompts)

        try:
            print(f"  ğŸš€ [Local-Batch] æ­£åœ¨æ‰¹é‡ç”Ÿæˆ {len(prompts)} æ¡...", end="", flush=True)
            
            messages_list = [
                [
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": p}
                ]
                for p in prompts
            ]
            text_list = [
                GLOBAL_TOKENIZER.apply_chat_template(
                    msgs,
                    tokenize=False,
                    add_generation_prompt=True
                )
                for msgs in messages_list
            ]

            # æ‰¹é‡ Tokenize + Padding
            model_inputs = GLOBAL_TOKENIZER(
                text_list,
                return_tensors="pt",
                padding=True, # å…³é”®
                truncation=True,
                max_length=cfg.model.max_input_len,
            ).to(GLOBAL_MODEL.device)

            with torch.no_grad():
                generated_ids = GLOBAL_MODEL.generate(
                    model_inputs.input_ids,
                    attention_mask=model_inputs.attention_mask,
                    max_new_tokens=max_new_tokens,
                    do_sample=False
                )

            results = []
            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids):
                new_token_ids = output_ids[len(input_ids):]
                text = GLOBAL_TOKENIZER.decode(new_token_ids, skip_special_tokens=True)
                results.append(clean_special_chars(text.strip()))
            print(" å®Œæˆ")
            return results

        except Exception as e:
            print(f"\nâŒ [Local-Batch Error]: {e}")
            return [""] * len(prompts)

    return [""] * len(prompts)


# ===== é«˜é¢‘ & ä½é¢‘è®°å¿†çš„ LLM æ“ä½œ =====
def summarize_high_freq_prompt(group_texts: List[str], cfg: DictConfig) -> str:
    items_formatted = "\n".join(
        f"[{i+1}] {t}" for i, t in enumerate(group_texts)
    )
    template = cfg.optimizer.prompts.summarize_high_freq
    prompt = template.format(items_formatted=items_formatted)
    return prompt

def expand_low_freq_memory_prompt(text: str, cfg: DictConfig) -> str:
    """æ„é€ ä½é¢‘è®°å¿†æ‰©å†™çš„ prompt"""
    template = cfg.optimizer.prompts.expand_low_freq
    prompt = template.format(text=text)
    
    return prompt

# =============== æ•°æ®åŠ è½½ ===============

def load_clustered_memories(path: str) -> Tuple[Dict[str, dict], List[str]]:
    memories: Dict[str, dict] = {}
    order: List[str] = []
    print(f"ğŸ“¥ æ­£åœ¨åŠ è½½èšç±»åçš„è®°å¿†æ–‡ä»¶: {path}")
    if not os.path.exists(path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {path}")
        return {}, []

    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip(): continue
            obj = json.loads(line)
            mid = str(obj["id"])
            memories[mid] = obj
            order.append(mid)
    print(f"âœ… å…±åŠ è½½ {len(memories)} æ¡è®°å¿†")
    return memories, order


def load_cluster_summary(path: str) -> Dict[int, List[str]]:
    cluster_to_ids: Dict[int, List[str]] = {}
    print(f"ğŸ“¥ æ­£åœ¨åŠ è½½èšç±»æ‘˜è¦æ–‡ä»¶: {path}")
    if not os.path.exists(path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {path}")
        return {}

    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip(): continue
            obj = json.loads(line)
            cid = int(obj["cluster_id"])
            ids = [str(x) for x in obj.get("memory_ids", [])]
            cluster_to_ids[cid] = ids
    print(f"âœ… å…±åŠ è½½ {len(cluster_to_ids)} ä¸ªèšç±»")
    return cluster_to_ids


def load_memory_freq(path: str) -> Dict[str, int]:
    freq_map: Dict[str, int] = {}
    print(f"ğŸ“¥ æ­£åœ¨åŠ è½½è®°å¿†é¢‘æ¬¡æ–‡ä»¶: {path}")
    if not os.path.exists(path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {path}")
        return {}

    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip(): continue
            obj = json.loads(line)
            # å…¼å®¹ memory_id æˆ– id å­—æ®µ
            mid = str(obj.get("memory_id", obj.get("id", "")))
            if not mid: continue
            freq = int(obj.get("freq", 0))
            freq_map[mid] = freq
    print(f"âœ… é¢‘æ¬¡è®°å½•æ•°: {len(freq_map)}")
    return freq_map


# =============== Embedding & ç›¸ä¼¼åº¦ ===============

def build_embeddings_for_memories(memories: Dict[str, dict], model_name: str) -> Dict[str, np.ndarray]:
    device = "cuda" if has_cuda() else "cpu"
    print(f"ğŸš€ æ­£åœ¨è®¡ç®—è®°å¿†å‘é‡ ({model_name}) on {device}...")
    model = SentenceTransformer(model_name, device=device)

    ids = list(memories.keys())
    texts = []
    for mid in ids:
        rec = memories[mid]
        text = rec.get("question") or rec.get("contents", "")
        texts.append(text)

    embeddings = model.encode(
        texts,
        batch_size=32,
        show_progress_bar=True,
        normalize_embeddings=True
    )
    id_to_emb = {mid: embeddings[i] for i, mid in enumerate(ids)}
    print(f"âœ… å‘é‡æ„å»ºå®Œæˆï¼Œå…± {len(id_to_emb)} æ¡")
    return id_to_emb


def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
    return float(np.dot(a, b))


# =============== é«˜é¢‘/ä½é¢‘é›†åˆé€‰æ‹© ===============

def select_high_low_ids(
    freq_map: Dict[str, int],
    top_k_high: int,
    bottom_k_low: int,
    low_freq_for_low_only: int = 1
):
    items = list(freq_map.items())
    # é«˜é¢‘ï¼šæŒ‰ freq é™åº
    sorted_desc = sorted(items, key=lambda x: -x[1])
    high_ids = [mid for mid, f in sorted_desc[:top_k_high]]

    # ä½é¢‘ï¼šæŒ‰ freq å‡åº
    sorted_asc = sorted(items, key=lambda x: x[1])
    low_ids = []
    zero_ids = []
    for mid, f in sorted_asc:
        if f < 0:
            zero_ids.append(mid)
            continue
        if f == low_freq_for_low_only:
            low_ids.append(mid)
        if len(low_ids) >= bottom_k_low:
            break

    print(f"ğŸ”¥ é«˜é¢‘ anchor æ•°é‡: {len(high_ids)}")
    print(f"ğŸ§Š åˆ†æ•°å°äº-2çš„è®°å¿†æ•°é‡: {len(zero_ids)}ï¼ˆä¹‹åä¼šåˆ é™¤ï¼‰")
    print(f"ğŸ¥¶ ä½é¢‘æ‰©å†™å€™é€‰(freq={low_freq_for_low_only})æ•°é‡: {len(low_ids)} (æœ€å¤š bottom_k={bottom_k_low})")
    return set(high_ids), set(low_ids), set(zero_ids)


# =============== ä¸»ä¼˜åŒ–é€»è¾‘ (Hydra Managed) ===============

@hydra.main(version_base=None, config_path="conf", config_name="config")
def optimize_memory(cfg: DictConfig):
    # 0. åˆå§‹åŒ– LLM
    init_llm(cfg)

    # 1. è¯»å…¥åŸºç¡€æ•°æ® (ä½¿ç”¨ config ä¸­çš„è·¯å¾„)
    cluster_file = cfg.paths.cluster_output
    summary_file = cfg.paths.cluster_summary
    freq_file = cfg.paths.freq_file
    output_file = cfg.paths.optimized_memory

    memories, id_order = load_clustered_memories(cluster_file)
    cluster_to_ids = load_cluster_summary(summary_file)
    freq_map = load_memory_freq(freq_file)

    if not memories:
        print("âŒ æ— æ³•åŠ è½½è®°å¿†æ•°æ®ï¼Œç¨‹åºé€€å‡ºã€‚")
        return

    # ä¸ºæ‰€æœ‰è®°å¿†è¡¥é½é¢‘æ¬¡
    for mid in memories.keys():
        freq_map.setdefault(mid, 0)

    # 2. é€‰å‡ºé«˜é¢‘ã€ä½é¢‘ã€0 é¢‘é›†åˆ (ä½¿ç”¨ config ä¸­çš„å‚æ•°)
    high_ids, low_ids, zero_ids = select_high_low_ids(
        freq_map,
        top_k_high=cfg.optimizer.top_k_high,
        bottom_k_low=cfg.optimizer.bottom_k_low,
        low_freq_for_low_only=cfg.optimizer.low_freq_threshold
    )

    # 3. å‡†å¤‡å‘é‡
    id_to_emb = build_embeddings_for_memories(memories, cfg.model.embedding_name)

# 4. é«˜é¢‘ï¼šç±»å†…èšåˆï¼ˆmergeï¼‰
    merged_consumed_ids = set()      
    to_delete_ids = set()            

    print("\n========== é«˜é¢‘è®°å¿†èšåˆé˜¶æ®µ (Batch Optimized) ==========")
    to_delete_ids.update(zero_ids)
    high_ids_sorted = sorted(list(high_ids), key=lambda x: -freq_map.get(x, 0))
    top_n_similar = cfg.optimizer.top_n_similar
    
    # ä¸´æ—¶ç¼“å­˜é˜Ÿåˆ—
    batch_size = cfg.optimizer.llm_batch_size
    batch_prompts = []
    batch_metadata = [] # å­˜å…ƒæ•°æ®ï¼Œç”¨äºå›è°ƒæ›´æ–°: (rec_anchor, group_ids)

    for anchor_id in high_ids_sorted:
        if anchor_id not in memories: continue
        if anchor_id in merged_consumed_ids: continue

        rec_anchor = memories[anchor_id]
        cluster_id = rec_anchor.get("cluster_id")
        if cluster_id is None: continue
        cluster_id = int(cluster_id)
        
        cluster_member_ids = [str(x) for x in cluster_to_ids.get(cluster_id, [])]
        if not cluster_member_ids: continue

        candidates = [
            mid for mid in cluster_member_ids
            if mid != anchor_id and mid not in merged_consumed_ids
        ]
        if not candidates: continue

        anchor_emb = id_to_emb.get(anchor_id)
        if anchor_emb is None: continue

        sims = []
        for mid in candidates:
            emb = id_to_emb.get(mid)
            if emb is None: continue
            sims.append((mid, cosine_similarity(anchor_emb, emb)))

        if not sims: continue

        sims_sorted = sorted(sims, key=lambda x: -x[1])
        neighbors = [mid for mid, _ in sims_sorted[:top_n_similar]]
        group_ids = [anchor_id] + neighbors

        print(f"\nğŸ”¥ [Plan] Anchor {anchor_id} (freq={freq_map[anchor_id]}) å‡†å¤‡åˆå¹¶ top-{len(neighbors)} é‚»å±…")
        
        # ğŸ”¥ å…³é”®ä¿®æ”¹ 1: ç«‹å³æ ‡è®°é‚»å±…ä¸ºâ€œå·²æ¶ˆè€—â€ï¼Œé˜²æ­¢å½“å‰ Batch åé¢çš„ Anchor æŠ¢å 
        # è™½ç„¶ LLM è¿˜æ²¡è·‘å®Œï¼Œä½†æˆ‘ä»¬å…ˆå åº§ï¼Œä¿è¯è´ªå¿ƒé€»è¾‘çš„é¡ºåºæ€§
        for mid in neighbors:
            merged_consumed_ids.add(mid)
            # åªæœ‰è¢«åˆå¹¶ä¸”é¢‘æ¬¡ä½çš„æ‰åˆ é™¤
            if freq_map.get(mid, 0) < cfg.optimizer.low_freq_threshold:
                to_delete_ids.add(mid)
        
        # æ„é€  Prompt æ–‡æœ¬
        group_texts = []
        for mid in group_ids:
            rec = memories[mid]
            text = rec.get("question") or rec.get("contents", "")
            group_texts.append(f"[ID {mid}] {text}")

        # ç”Ÿæˆ Prompt å¹¶åŠ å…¥ Batch é˜Ÿåˆ—
        prompt = summarize_high_freq_prompt(group_texts, cfg)
        batch_prompts.append(prompt)
        batch_metadata.append({
            "rec_anchor": rec_anchor,
            "group_ids": group_ids,
            "anchor_id": anchor_id
        })

        # ğŸ”¥ å…³é”®ä¿®æ”¹ 2: å‡‘å¤Ÿ Batch ç«‹å³æ‰§è¡Œ
        if len(batch_prompts) >= batch_size:
            print(f"ğŸš€ [Batch Execution] å¹¶å‘æ‰§è¡Œ {len(batch_prompts)} ä¸ªé«˜é¢‘èšåˆä»»åŠ¡...")
            outputs = call_llm_batch(batch_prompts, cfg)
            
            # å›å¡«ç»“æœ
            for task_info, summary_text in zip(batch_metadata, outputs):
                if not summary_text:
                    print(f"   âš ï¸ LLM è¿”å›ä¸ºç©ºï¼Œè·³è¿‡ Anchor {task_info['anchor_id']}")
                    continue
                
                rec = task_info['rec_anchor']
                rec["original_question"] = rec.get("question") or rec.get("contents", "")
                rec["question"] = summary_text
                rec["merged_from_ids"] = task_info['group_ids']
                rec["merge_type"] = "high_freq_anchor"
            
            # æ¸…ç©ºé˜Ÿåˆ—
            batch_prompts = []
            batch_metadata = []

    # ğŸ”¥ å…³é”®ä¿®æ”¹ 3: å¤„ç†å¾ªç¯ç»“æŸåå‰©ä½™çš„ä»»åŠ¡
    if batch_prompts:
        print(f"ğŸš€ [Batch Execution] å¤„ç†å‰©ä½™çš„ {len(batch_prompts)} ä¸ªé«˜é¢‘èšåˆä»»åŠ¡...")
        outputs = call_llm_batch(batch_prompts, cfg)
        for task_info, summary_text in zip(batch_metadata, outputs):
            if not summary_text: continue
            rec = task_info['rec_anchor']
            rec["original_question"] = rec.get("question") or rec.get("contents", "")
            rec["question"] = summary_text
            rec["merged_from_ids"] = task_info['group_ids']
            rec["merge_type"] = "high_freq_anchor"

    # 5. ä½é¢‘ï¼šæ‰©å†™
    print("\n========== ä½é¢‘è®°å¿†æ‰©å†™é˜¶æ®µ ==========")

    low_expand_ids = [
        mid for mid in low_ids
        if mid in memories and mid not in to_delete_ids
    ]
    print(f"ğŸ¥¶ éœ€è¦æ‰©å†™çš„ä½é¢‘è®°å¿†æ¡ç›®æ•°: {len(low_expand_ids)}")

    low_expand_items = []
    for mid in low_expand_ids:
        rec = memories[mid]
        base_text = rec.get("question") or rec.get("contents", "")
        low_expand_items.append((mid, base_text))

    batch_size = cfg.optimizer.llm_batch_size
    total_low = len(low_expand_items)
    
    for start in range(0, total_low, batch_size):
        end = min(start + batch_size, total_low)
        batch_items = low_expand_items[start:end]
        batch_ids = [mid for (mid, _) in batch_items]

        print(f"\nğŸ¥¶ æ‰©å†™ä½é¢‘è®°å¿† Batch {start // batch_size + 1} / { (total_low + batch_size - 1) // batch_size }")
        print(f"   IDs: {batch_ids}")

        # ğŸ”¥ ä¿®æ­£: è¿™é‡Œä¹‹å‰æ¼ä¼ äº† cfg å‚æ•°ï¼Œç°åœ¨è¡¥ä¸Š
        batch_prompts = [
            expand_low_freq_memory_prompt(base_text, cfg) 
            for (_, base_text) in batch_items
        ]
        
        batch_outputs = call_llm_batch(batch_prompts, cfg)

        for (mid, base_text), expanded in zip(batch_items, batch_outputs):
            if not expanded:
                print(f"   âš ï¸ LLM è¿”å›ä¸ºç©ºï¼ŒID={mid} ä¿æŒåŸæ–‡ä¸å˜")
                continue
            rec = memories[mid]
            rec["original_question"] = base_text
            rec["question"] = expanded
            rec["opt_type"] = "low_freq_expanded"
    # 6. å†™å‡ºæ–°çš„è®°å¿†åº“
    print("\n========== å†™å‡ºä¼˜åŒ–åçš„è®°å¿†åº“ ==========")
    kept_count = 0
    with open(output_file, "w", encoding="utf-8") as f:
        for mid in id_order:
            if mid not in memories: continue
            if mid in to_delete_ids: continue
            f.write(json.dumps(memories[mid], ensure_ascii=False) + "\n")
            kept_count += 1

    print(f"âœ… æ–°è®°å¿†åº“å†™å…¥å®Œæˆ: {output_file}")
    print(f"   ä¿ç•™è®°å¿†æ¡ç›®: {kept_count}")
    print(f"   åˆ é™¤è®°å¿†æ¡ç›®: {len(to_delete_ids)}")

if __name__ == "__main__":
    optimize_memory()