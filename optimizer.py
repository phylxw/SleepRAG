import os
import json
import time
from typing import Dict, List, Tuple

import numpy as np
import torch
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
import google.generativeai as genai


# ================= é…ç½®åŒºåŸŸ =================

# 1. LLM é…ç½®ï¼šå’Œä½ èšç±»æ–‡ä»¶ä¿æŒä¸€è‡´
MODEL_SOURCE = "huggingface"   # "huggingface" æˆ– "gemini"

HF_MODEL_NAME = "Qwen/Qwen3-4B-Instruct-2507"
GEMINI_MODEL_NAME = "gemini-2.5-flash"
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")

# 2. æ–‡ä»¶è·¯å¾„ï¼ˆä½ å¯ä»¥æ ¹æ®æ•°æ®é›†æ”¹åï¼›è¿™é‡Œå…ˆä»¥ MATH ä¸ºä¾‹ï¼‰
CLUSTERED_FILE = "math_auto_clustered_result.jsonl"           # èšç±»åçš„è®°å¿†æ–‡ä»¶
CLUSTER_SUMMARY_FILE = "math_cluster_summary.jsonl"           # æ¯ä¸ªç±»æœ‰å“ªäº›è®°å¿†ID
MEM_FREQ_FILE = "MATH-lighteval_memory_freq_20251216_122715.jsonl"  # è°ƒç”¨é¢‘æ¬¡æ–‡ä»¶
OUTPUT_OPTIMIZED_FILE = "MATH_optimized_memory_k200.jsonl"   # è¾“å‡ºçš„æ–°è®°å¿†åº“

# 3. ä¼˜åŒ–é€»è¾‘å‚æ•°
TOP_K_HIGH = 30                # ä½œä¸ºâ€œé«˜é¢‘è®°å¿† anchorâ€çš„æ¡ç›®æ•°é‡ï¼ˆæŒ‰é¢‘æ¬¡æ’åºï¼‰
BOTTOM_K_LOW = 30              # ä½œä¸ºâ€œä½é¢‘è®°å¿†æ‰©å†™å¯¹è±¡â€çš„æ¡ç›®æ•°é‡ï¼ˆæŒ‰é¢‘æ¬¡ä»ä½åˆ°é«˜ï¼‰
LOW_FREQ_THRESHOLD = 2          # è¢«é«˜é¢‘åˆå¹¶æ—¶ï¼Œå¦‚æœ freq < è¿™ä¸ªé˜ˆå€¼å°±ç›´æ¥åˆ æ‰
TOP_N_SIMILAR_IN_CLUSTER = 5    # é«˜é¢‘ anchor åœ¨ç±»å†…é€‰ top-n ç›¸ä¼¼è®°å¿†æ¥åˆå¹¶

# 4. ç›¸ä¼¼åº¦ embedding æ¨¡å‹
EMBEDDING_MODEL = "BAAI/bge-large-en-v1.5"

# ===========================================

GLOBAL_MODEL = None
GLOBAL_TOKENIZER = None


# =============== å·¥å…·å‡½æ•° ===============

def clean_special_chars(text: str) -> str:
    if not isinstance(text, str):
        return text
    return text.replace('\u2028', ' ').replace('\u2029', ' ')


def has_cuda() -> bool:
    try:
        return torch.cuda.is_available()
    except Exception:
        return False


# =============== LLM åˆå§‹åŒ–ä¸è°ƒç”¨ ===============

def init_llm():
    """åˆå§‹åŒ– LLMï¼ˆå’Œä½ çš„èšç±»è„šæœ¬ä¿æŒä¸€è‡´é£æ ¼ï¼‰"""
    global GLOBAL_MODEL, GLOBAL_TOKENIZER

    if MODEL_SOURCE == "gemini":
        if GEMINI_API_KEY:
            genai.configure(api_key=GEMINI_API_KEY)
            print(f"ğŸ¤– [Init] Gemini API ({GEMINI_MODEL_NAME}) å·²é…ç½®")
        else:
            print("âš ï¸ [Init] æœªæ£€æµ‹åˆ° GEMINI_API_KEYï¼ŒGemini ç›¸å…³åŠŸèƒ½ä¼šè¢«è·³è¿‡")
    elif MODEL_SOURCE == "huggingface":
        print(f"ğŸ“¥ [Init] æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹: {HF_MODEL_NAME} ...")
        try:
            GLOBAL_TOKENIZER = AutoTokenizer.from_pretrained(HF_MODEL_NAME, trust_remote_code=True)
            GLOBAL_MODEL = AutoModelForCausalLM.from_pretrained(
                HF_MODEL_NAME,
                device_map="auto",
                torch_dtype="auto",
                trust_remote_code=True
            ).eval()
            print("âœ… [Init] æœ¬åœ°æ¨¡å‹åŠ è½½å®Œæˆï¼")
        except Exception as e:
            print(f"âŒ [Init] æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ’¡ æç¤º: è¯·æ£€æŸ¥ HuggingFace æƒé™å’Œç½‘ç»œ")


def call_llm(prompt: str, max_new_tokens: int = 256) -> str:
    """ç»Ÿä¸€çš„å¤§æ¨¡å‹è°ƒç”¨æ¥å£ï¼ˆGemini / æœ¬åœ° Qwenï¼‰"""

    # --- Gemini ---
    if MODEL_SOURCE == "gemini":
        if not GEMINI_API_KEY:
            return "Skipped (No GEMINI_API_KEY)"
        try:
            model = genai.GenerativeModel(GEMINI_MODEL_NAME)
            print("  ğŸ¤– [Gemini] æ­£åœ¨ç”Ÿæˆ...", end="", flush=True)
            resp = model.generate_content(prompt)
            print(" å®Œæˆ")
            return clean_special_chars(resp.text.strip())
        except Exception as e:
            print(f"\nâŒ [Gemini Error]: {e}")
            return ""

    # --- HuggingFace æœ¬åœ° ---
    elif MODEL_SOURCE == "huggingface":
        if GLOBAL_MODEL is None:
            print("âš ï¸ [Local] LLM å°šæœªåˆå§‹åŒ–")
            return ""

        try:
            print("  ğŸš€ [Local] æ­£åœ¨ç”Ÿæˆ...", end="", flush=True)
            messages = [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt}
            ]
            text = GLOBAL_TOKENIZER.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            model_inputs = GLOBAL_TOKENIZER([text], return_tensors="pt").to(GLOBAL_MODEL.device)
            with torch.no_grad():
                generated_ids = GLOBAL_MODEL.generate(
                    model_inputs.input_ids,
                    max_new_tokens=max_new_tokens,
                    do_sample=False
                )
            # åªå–æ–°å¢çš„éƒ¨åˆ†
            generated_ids = [
                output_ids[len(input_ids):]
                for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
            ]
            response = GLOBAL_TOKENIZER.batch_decode(generated_ids, skip_special_tokens=True)[0]
            print(" å®Œæˆ")
            return clean_special_chars(response.strip())
        except Exception as e:
            print(f"\nâŒ [Local Error]: {e}")
            return ""

    return ""


# ===== é«˜é¢‘ & ä½é¢‘è®°å¿†çš„ LLM æ“ä½œ =====

def summarize_high_freq_memory(anchor_id: str, group_texts: List[str]) -> str:
    """
    é«˜é¢‘è®°å¿†ç±»å†…èšåˆï¼šç»™å®š anchor + åŒç±»è‹¥å¹²æ¡ç›¸ä¼¼è®°å¿†ï¼ŒæŠŠå®ƒä»¬åˆå¹¶æˆä¸€ä¸ªæ›´â€œæ·±â€çš„è®°å¿†ã€‚
    """
    items_formatted = "\n".join(
        f"[{i+1}] {t}" for i, t in enumerate(group_texts)
    )
    prompt = f"""ä½ æ˜¯æ•°å­¦åŠ©æ•™ã€‚ä¸‹é¢æ˜¯ä¸€ç»„å±äºåŒä¸€é¢˜å‹çš„è®°å¿†æ¡ç›®ï¼Œå®ƒä»¬éƒ½æ¥è‡ªåŒä¸€ä¸ªèšç±»ï¼ˆåŒç±»é—®é¢˜ï¼‰ã€‚
è¯·å°†å®ƒä»¬åˆå¹¶æˆ**ä¸€æ¡æ›´å®Œæ•´ã€æ›´æŠ½è±¡çš„è®°å¿†**ï¼Œè¦æ±‚ï¼š

1. ä¸æ”¹å˜ä»»ä½•ç»“è®ºï¼Œä¹Ÿä¸è¦å¼•å…¥æ–°çš„æ•°å€¼æˆ–é¢å¤–äº‹å®ã€‚
2. ä¿ç•™æ‰€æœ‰å…³é”®æ¡ä»¶ã€å…¬å¼ä¸è§£é¢˜ç»“è®ºã€‚
3. é€‚å½“æ€»ç»“å…±åŒçš„è§£é¢˜æ€è·¯ï¼Œå¯ä»¥åˆå¹¶é‡å¤ä¿¡æ¯ã€‚
4. ç”¨Englishå†™æˆä¸€æ®µæˆ–ä¸¤æ®µè¿ç»­æ–‡æœ¬ï¼Œä¸è¦åˆ†æ¡åˆ—å‡ºåŸé¢˜å·ã€‚

å¾…åˆå¹¶çš„è®°å¿†æ¡ç›®å¦‚ä¸‹ï¼š
{items_formatted}
"""
    return call_llm(prompt)


def expand_low_freq_memory(text: str) -> str:
    """
    ä½é¢‘è®°å¿†æ‰©å†™ï¼šä¸æ”¹å˜æ ¸å¿ƒè¯­ä¹‰ã€ä¸æ–°å¢äº‹å®ï¼Œåªåšè§£é‡Š & åŒä¹‰æ‰©å†™ã€‚
    """
    prompt = f"""ä½ æ˜¯æ•°å­¦åŠ©æ•™ã€‚ä¸‹é¢æ˜¯ä¸€æ¡æ•°å­¦é¢˜ç›®çš„è®°å¿†å†…å®¹ã€‚

è¯·åœ¨ **ä¸æ”¹å˜é¢˜ç›®æ¡ä»¶å’Œç­”æ¡ˆã€ä¸æ·»åŠ ä»»ä½•æ–°æ•°å€¼æˆ–äº‹å®** çš„å‰æä¸‹ï¼Œå¯¹å®ƒè¿›è¡Œè¯­ä¹‰æ‰©å†™ï¼š
1. å¯ä»¥å¢åŠ å¯¹é¢˜ç›®è€ƒå¯Ÿç‚¹çš„è§£é‡Šå’ŒèƒŒæ™¯è¯´æ˜ã€‚
2. å¯ä»¥åŠ å…¥åŒä¹‰æ”¹å†™ã€æ›´å¤šè‡ªç„¶è¯­è¨€è¡¨è¿°ï¼Œä»¥ä¾¿æœªæ¥æ›´å®¹æ˜“è¢«æ£€ç´¢åˆ°ã€‚
3. è¾“å‡ºä¸€æ®µæˆ–ä¸¤æ®µEnglishæ–‡æœ¬ï¼Œä¸è¦ä¸¢å¤±åŸå§‹ä¿¡æ¯ã€‚

åŸå§‹è®°å¿†ï¼š
{text}
"""
    return call_llm(prompt)


# =============== æ•°æ®åŠ è½½ ===============

def load_clustered_memories(path: str) -> Tuple[Dict[str, dict], List[str]]:
    """
    è¯»å– *_auto_clustered_result.jsonl
    è¿”å›ï¼š
      - id -> è®°å½• dict
      - id_list: ä¿ç•™åŸå§‹é¡ºåºçš„ id åˆ—è¡¨ï¼ˆæ–¹ä¾¿æœ€åå†™å›ï¼‰
    """
    memories: Dict[str, dict] = {}
    order: List[str] = []
    print(f"ğŸ“¥ æ­£åœ¨åŠ è½½èšç±»åçš„è®°å¿†æ–‡ä»¶: {path}")
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
            obj = json.loads(line)
            mid = str(obj["id"])
            memories[mid] = obj
            order.append(mid)
    print(f"âœ… å…±åŠ è½½ {len(memories)} æ¡è®°å¿†")
    return memories, order


def load_cluster_summary(path: str) -> Dict[int, List[str]]:
    """
    è¯»å– *_cluster_summary.jsonl
    è¿”å›ï¼šcluster_id -> [memory_ids...]
    """
    cluster_to_ids: Dict[int, List[str]] = {}
    print(f"ğŸ“¥ æ­£åœ¨åŠ è½½èšç±»æ‘˜è¦æ–‡ä»¶: {path}")
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
            obj = json.loads(line)
            cid = int(obj["cluster_id"])
            ids = [str(x) for x in obj.get("memory_ids", [])]
            cluster_to_ids[cid] = ids
    print(f"âœ… å…±åŠ è½½ {len(cluster_to_ids)} ä¸ªèšç±»")
    return cluster_to_ids


def load_memory_freq(path: str) -> Dict[str, int]:
    """
    è¯»å–è°ƒç”¨é¢‘æ¬¡æ–‡ä»¶ MATH-lighteval_memory_freq_*.jsonl
    é¢„æœŸæ¯è¡ŒåŒ…å« memory_id / id, freq å­—æ®µã€‚
    """
    freq_map: Dict[str, int] = {}
    print(f"ğŸ“¥ æ­£åœ¨åŠ è½½è®°å¿†é¢‘æ¬¡æ–‡ä»¶: {path}")
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
            obj = json.loads(line)
            mid = str(obj.get("memory_id", obj.get("id", "")))
            if not mid:
                continue
            freq = int(obj.get("freq", 0))
            freq_map[mid] = freq
    print(f"âœ… é¢‘æ¬¡è®°å½•æ•°: {len(freq_map)}")
    return freq_map


# =============== Embedding & ç›¸ä¼¼åº¦ ===============

def build_embeddings_for_memories(memories: Dict[str, dict]) -> Dict[str, np.ndarray]:
    """
    å¯¹æ‰€æœ‰è®°å¿†æ„å»ºå‘é‡ï¼Œç”¨äºç±»å†…ç›¸ä¼¼åº¦è®¡ç®—ã€‚
    é»˜è®¤ä¸ºä½¿ç”¨è®°å½•ä¸­çš„ "question" å­—æ®µï¼›å¦‚æœä½ æƒ³æ”¹æˆ "contents" å°±è‡ªå·±æ¢ä¸€ä¸‹ã€‚
    """
    device = "cuda" if has_cuda() else "cpu"
    print(f"ğŸš€ æ­£åœ¨è®¡ç®—è®°å¿†å‘é‡ ({EMBEDDING_MODEL}) on {device}...")
    model = SentenceTransformer(EMBEDDING_MODEL, device=device)

    ids = list(memories.keys())
    texts = []
    for mid in ids:
        rec = memories[mid]
        text = rec.get("question") or rec.get("contents", "")
        texts.append(text)

    embeddings = model.encode(
        texts,
        batch_size=32,
        show_progress_bar=True,
        normalize_embeddings=True
    )
    id_to_emb = {mid: embeddings[i] for i, mid in enumerate(ids)}
    print(f"âœ… å‘é‡æ„å»ºå®Œæˆï¼Œå…± {len(id_to_emb)} æ¡")
    return id_to_emb


def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
    return float(np.dot(a, b))


# =============== é«˜é¢‘/ä½é¢‘é›†åˆé€‰æ‹© ===============

def select_high_low_ids(
    freq_map: Dict[str, int],
    top_k_high: int,
    bottom_k_low: int,
    low_freq_for_low_only: int = 1
):
    """
    ä» freq_map ä¸­é€‰ï¼š
      - top_k_high ä¸ªæœ€é«˜é¢‘ä½œä¸ºé«˜é¢‘ anchor
      - bottom_k_low ä¸ªä½é¢‘å€™é€‰ï¼ˆä½†åªä¿ç•™ freq == low_freq_for_low_only çš„ï¼‰
      - åŒæ—¶è®°å½•æ‰€æœ‰ freq == 0 çš„ id æ–¹ä¾¿ä¹‹ååˆ é™¤
    """
    # å…ˆè¡¥å…¨ 0 é¢‘æ¬¡ï¼ˆå¦‚æœæœ‰ä¸€äº› id åœ¨ freq_map ä¸­æ²¡å‡ºç°ï¼‰
    # ï¼ˆè¿™ä¸€è¡¥å…¨åº”è¯¥åœ¨å¤–éƒ¨å¯¹æ‰€æœ‰ IDs åšä¸€æ¬¡ï¼Œè¿™é‡Œå‡è®¾å·²ç»è¡¥ï¼‰
    items = list(freq_map.items())
    # é«˜é¢‘ï¼šæŒ‰ freq é™åº
    sorted_desc = sorted(items, key=lambda x: -x[1])
    high_ids = [mid for mid, f in sorted_desc[:top_k_high]]

    # ä½é¢‘ï¼šæŒ‰ freq å‡åº
    sorted_asc = sorted(items, key=lambda x: x[1])
    low_ids = []
    zero_ids = []
    for mid, f in sorted_asc:
        if f == 0:
            zero_ids.append(mid)
            continue
        if f == low_freq_for_low_only:
            low_ids.append(mid)
        if len(low_ids) >= bottom_k_low:
            break

    print(f"ğŸ”¥ é«˜é¢‘ anchor æ•°é‡: {len(high_ids)}")
    print(f"ğŸ§Š 0 æ¬¡è°ƒç”¨çš„è®°å¿†æ•°é‡: {len(zero_ids)}ï¼ˆä¹‹åä¼šåˆ é™¤ï¼‰")
    print(f"ğŸ¥¶ ä½é¢‘æ‰©å†™å€™é€‰(freq={low_freq_for_low_only})æ•°é‡: {len(low_ids)} (æœ€å¤š bottom_k={bottom_k_low})")
    return set(high_ids), set(low_ids), set(zero_ids)


# =============== ä¸»ä¼˜åŒ–é€»è¾‘ ===============

def optimize_memory():
    # 0. åˆå§‹åŒ– LLM
    init_llm()

    # 1. è¯»å…¥åŸºç¡€æ•°æ®
    memories, id_order = load_clustered_memories(CLUSTERED_FILE)
    cluster_to_ids = load_cluster_summary(CLUSTER_SUMMARY_FILE)
    freq_map = load_memory_freq(MEM_FREQ_FILE)

    # ä¸ºæ‰€æœ‰è®°å¿†è¡¥é½é¢‘æ¬¡
    for mid in memories.keys():
        freq_map.setdefault(mid, 0)

    # 2. é€‰å‡ºé«˜é¢‘ã€ä½é¢‘ã€0 é¢‘é›†åˆ
    high_ids, low_ids, zero_ids = select_high_low_ids(
        freq_map,
        TOP_K_HIGH,
        BOTTOM_K_LOW,
        low_freq_for_low_only=LOW_FREQ_THRESHOLD
    )

    # 3. å‡†å¤‡å‘é‡ï¼Œç”¨äºç±»å†…ç›¸ä¼¼åº¦
    id_to_emb = build_embeddings_for_memories(memories)

    # 4. é«˜é¢‘ï¼šç±»å†…èšåˆï¼ˆmergeï¼‰
    merged_consumed_ids = set()      # è¢«å½“ä½œâ€œé‚»å±…â€å‚ä¸ merge çš„è®°å¿† id
    to_delete_ids = set()            # æœ€ç»ˆè¦å½»åº•åˆ é™¤çš„ idï¼ˆä½é¢‘è¢« merge / é¢‘æ¬¡ä¸º0 ç­‰ï¼‰

    print("\n========== é«˜é¢‘è®°å¿†èšåˆé˜¶æ®µ ==========")
    # æŒ‰é¢‘æ¬¡ä»é«˜åˆ°ä½é¡ºåºå¤„ç† anchorï¼Œé¿å… rank ä½çš„ anchor æŠ¢èµ°é«˜é¢‘é‚»å±…
    high_ids_sorted = sorted(list(high_ids), key=lambda x: -freq_map.get(x, 0))

    for anchor_id in high_ids_sorted:
        if anchor_id not in memories:
            continue
        if anchor_id in merged_consumed_ids:
            # è¯´æ˜å·²ç»ä½œä¸ºåˆ«äºº group çš„æˆå‘˜äº†ï¼Œå°±ä¸å†å½“ anchor
            continue

        rec_anchor = memories[anchor_id]
        cluster_id = rec_anchor.get("cluster_id")
        if cluster_id is None:
            continue

        cluster_id = int(cluster_id)
        cluster_member_ids = [str(x) for x in cluster_to_ids.get(cluster_id, [])]
        if not cluster_member_ids:
            continue

        # å€™é€‰é‚»å±…ï¼šåŒç±»ã€ä¸æ˜¯è‡ªå·±ã€æ²¡è¢« merge è¿‡
        candidates = [
            mid for mid in cluster_member_ids
            if mid != anchor_id and mid not in merged_consumed_ids
        ]
        if not candidates:
            continue

        anchor_emb = id_to_emb.get(anchor_id)
        if anchor_emb is None:
            continue

        sims = []
        for mid in candidates:
            emb = id_to_emb.get(mid)
            if emb is None:
                continue
            sims.append((mid, cosine_similarity(anchor_emb, emb)))

        if not sims:
            continue

        # å–ç±»å†… top-n ç›¸ä¼¼
        sims_sorted = sorted(sims, key=lambda x: -x[1])
        neighbors = [mid for mid, _ in sims_sorted[:TOP_N_SIMILAR_IN_CLUSTER]]
        group_ids = [anchor_id] + neighbors

        print(f"\nğŸ”¥ Anchor {anchor_id} (freq={freq_map[anchor_id]}, cluster={cluster_id})")
        print(f"   åˆå¹¶åŒç±» top-{len(neighbors)}: {neighbors}")

        # æ„é€ è¦ç»™ LLM çš„æ–‡æœ¬
        group_texts = []
        for mid in group_ids:
            rec = memories[mid]
            text = rec.get("question") or rec.get("contents", "")
            group_texts.append(f"[ID {mid}] {text}")

        summary_text = summarize_high_freq_memory(anchor_id, group_texts)
        if not summary_text:
            print("   âš ï¸ LLM è¿”å›ä¸ºç©ºï¼Œè·³è¿‡è¿™ç»„åˆå¹¶")
            continue

        # æ›´æ–° anchor çš„å†…å®¹ï¼šç”¨ summary æ›¿æ¢ questionï¼Œå¹¶ä¿ç•™åŸå§‹ä¿¡æ¯
        original_text = rec_anchor.get("question") or rec_anchor.get("contents", "")
        rec_anchor["original_question"] = original_text
        rec_anchor["question"] = summary_text
        rec_anchor["merged_from_ids"] = group_ids
        rec_anchor["merge_type"] = "high_freq_anchor"

        # é‚»å±…æ ‡è®°ä¸ºå·²å‚ä¸ mergeï¼›å…¶ä¸­ä½é¢‘çš„æ ‡è®°ä¸ºåˆ é™¤
        for mid in neighbors:
            merged_consumed_ids.add(mid)
            if freq_map.get(mid, 0) < LOW_FREQ_THRESHOLD:
                to_delete_ids.add(mid)

    # 5. ä½é¢‘ï¼šä¸è¢«åˆå¹¶æ¶ˆæ‰ã€freq=1 çš„è®°å¿†åšæ‰©å†™
    print("\n========== ä½é¢‘è®°å¿†æ‰©å†™é˜¶æ®µ ==========")
    # å…ˆæŠŠæ‰€æœ‰ freq=0 çš„ç›´æ¥åŠ å…¥åˆ é™¤é›†åˆ
    to_delete_ids.update(zero_ids)

    # çœŸæ­£è¦æ‰©å†™çš„ä½é¢‘è®°å¿†ï¼šfreq==1ï¼Œä¸”æ²¡æœ‰è¢« merge æ¶ˆè€—æ‰
    low_expand_ids = [
        mid for mid in low_ids
        if mid in memories and mid not in to_delete_ids
    ]

    print(f"ğŸ¥¶ éœ€è¦æ‰©å†™çš„ä½é¢‘è®°å¿†æ¡ç›®æ•°: {len(low_expand_ids)}")

    for mid in low_expand_ids:
        rec = memories[mid]
        base_text = rec.get("question") or rec.get("contents", "")
        print(f"\nğŸ¥¶ æ‰©å†™ä½é¢‘è®°å¿† ID={mid}, freq={freq_map[mid]}")
        expanded = expand_low_freq_memory(base_text)
        if not expanded:
            print("   âš ï¸ LLM è¿”å›ä¸ºç©ºï¼Œä¿æŒåŸæ–‡ä¸å˜")
            continue

        rec["original_question"] = base_text
        rec["question"] = expanded
        rec["opt_type"] = "low_freq_expanded"

    # 6. å†™å‡ºæ–°çš„è®°å¿†åº“ï¼šè·³è¿‡ to_delete_ids
    print("\n========== å†™å‡ºä¼˜åŒ–åçš„è®°å¿†åº“ ==========")
    kept_count = 0
    with open(OUTPUT_OPTIMIZED_FILE, "w", encoding="utf-8") as f:
        for mid in id_order:
            if mid not in memories:
                continue
            if mid in to_delete_ids:
                continue
            f.write(json.dumps(memories[mid], ensure_ascii=False) + "\n")
            kept_count += 1

    print(f"âœ… æ–°è®°å¿†åº“å†™å…¥å®Œæˆ: {OUTPUT_OPTIMIZED_FILE}")
    print(f"   ä¿ç•™è®°å¿†æ¡ç›®: {kept_count}")
    print(f"   åˆ é™¤è®°å¿†æ¡ç›®: {len(to_delete_ids)}")
    print("   ï¼ˆæ³¨æ„ï¼šåŸå§‹ *_auto_clustered_result.jsonl æ–‡ä»¶æ²¡æœ‰è¢«ä¿®æ”¹ï¼‰")


if __name__ == "__main__":
    optimize_memory()