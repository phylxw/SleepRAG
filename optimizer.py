import os
import json
import time
from typing import Dict, List, Tuple, Any
from concurrent.futures import ThreadPoolExecutor

import numpy as np
import torch
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
import google.generativeai as genai


# ================= é…ç½®åŒºåŸŸ =================

# 1. LLM é…ç½®
MODEL_SOURCE = "huggingface"   # "huggingface" æˆ– "gemini"

HF_MODEL_NAME = "Qwen/Qwen3-4B-Instruct-2507"
GEMINI_MODEL_NAME = "gemini-2.5-flash"
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")

# 2. æ–‡ä»¶è·¯å¾„
CLUSTERED_FILE = "AMATH-lighteval_auto_clustered_result.jsonl"
CLUSTER_SUMMARY_FILE = "AMATH-lighteval_cluster_summary.jsonl"
MEM_FREQ_FILE = "MATH-lighteval_memory_freq_20251218_150403.jsonl"
OUTPUT_OPTIMIZED_FILE = "AMATH-lighteval_optimized_memory_k50.jsonl"

# 3. ä¼˜åŒ–é€»è¾‘å‚æ•°
TOP_K_HIGH = 50                # é«˜é¢‘ anchor æ•°é‡
BOTTOM_K_LOW = 50               # ä½Žé¢‘æ‰©å†™æ•°é‡
LOW_FREQ_THRESHOLD = 2          # é¢‘æ¬¡é˜ˆå€¼
TOP_N_SIMILAR_IN_CLUSTER = 5    # ç±»å†…åˆå¹¶é‚»å±…æ•°

# 4. ç›¸ä¼¼åº¦ embedding æ¨¡åž‹
EMBEDDING_MODEL = "BAAI/bge-large-en-v1.5"

# 5. LLM å¹¶è¡Œä¸Žæ‰¹é‡æŽ§åˆ¶
# å¯¹äºŽå¤šå¡çŽ¯å¢ƒï¼Œå¢žåŠ  LLM_BATCH_SIZE å¯ä»¥æé«˜æ˜¾å¡åˆ©ç”¨çŽ‡
LLM_BATCH_SIZE = 8          # æ‰¹é‡å¤„ç†å¤§å°ï¼ˆæ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼Œå¤šå¡å¯è°ƒå¤§ï¼‰
MAX_NEW_TOKENS = 512        # è¾“å‡ºé•¿åº¦
MAX_INPUT_TOKENS = 2048     # è¾“å…¥é•¿åº¦
MAX_WORKERS = 4             # Gemini å¹¶è¡Œè¯·æ±‚æ•°ï¼ˆä»…åœ¨ MODEL_SOURCE="gemini" æ—¶æœ‰æ•ˆï¼‰

# ===========================================

GLOBAL_MODEL = None
GLOBAL_TOKENIZER = None


# =============== å·¥å…·å‡½æ•° ===============

def clean_special_chars(text: str) -> str:
    if not isinstance(text, str):
        return text
    return text.replace('\u2028', ' ').replace('\u2029', ' ')


def has_cuda() -> bool:
    try:
        return torch.cuda.is_available()
    except Exception:
        return False


# =============== LLM åˆå§‹åŒ–ä¸Žè°ƒç”¨ ===============

def init_llm():
    """åˆå§‹åŒ– LLM"""
    global GLOBAL_MODEL, GLOBAL_TOKENIZER

    if MODEL_SOURCE == "gemini":
        if GEMINI_API_KEY:
            genai.configure(api_key=GEMINI_API_KEY)
            print(f"ðŸ¤– [Init] Gemini API ({GEMINI_MODEL_NAME}) å·²é…ç½®")
        else:
            print("âš ï¸ [Init] æœªæ£€æµ‹åˆ° GEMINI_API_KEY")
    elif MODEL_SOURCE == "huggingface":
        print(f"ðŸ“¥ [Init] æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡åž‹: {HF_MODEL_NAME} ...")
        try:
            GLOBAL_TOKENIZER = AutoTokenizer.from_pretrained(HF_MODEL_NAME, trust_remote_code=True)
            # å¿…é¡»è®¾ç½® padding_side='left' ä»¥æ”¯æŒæ‰¹é‡æŽ¨ç†
            GLOBAL_TOKENIZER.padding_side = 'left'
            if GLOBAL_TOKENIZER.pad_token is None:
                GLOBAL_TOKENIZER.pad_token = GLOBAL_TOKENIZER.eos_token
            
            # device_map="auto" ä¼šè‡ªåŠ¨å°†æ¨¡åž‹åˆ†å¸ƒåœ¨å¤šå¼ æ˜¾å¡ä¸Š
            GLOBAL_MODEL = AutoModelForCausalLM.from_pretrained(
                HF_MODEL_NAME,
                device_map="auto",
                torch_dtype="auto",
                trust_remote_code=True
            ).eval()
            print("âœ… [Init] æœ¬åœ°æ¨¡åž‹å¤šå¡åˆ†å‘åŠ è½½å®Œæˆï¼")
        except Exception as e:
            print(f"âŒ [Init] æœ¬åœ°æ¨¡åž‹åŠ è½½å¤±è´¥: {e}")


def call_llm(prompt: str, max_new_tokens: int = MAX_NEW_TOKENS) -> str:
    """å•æ¡è°ƒç”¨æŽ¥å£"""
    # åŒ…è£…æ‰¹é‡æŽ¥å£
    res = call_llm_batch([prompt], max_new_tokens=max_new_tokens)
    return res[0] if res else ""


def call_llm_batch(prompts: List[str], max_new_tokens: int = MAX_NEW_TOKENS) -> List[str]:
    """æ‰¹é‡æŽ¨ç†æŽ¥å£ï¼šå®žçŽ°å¤šå¡å¹¶è¡Œ/å¹¶å‘"""
    if not prompts:
        return []

    # --- Geminiï¼šä½¿ç”¨çº¿ç¨‹æ± æ¨¡æ‹Ÿå¹¶è¡ŒæŽ¨ç† ---
    if MODEL_SOURCE == "gemini":
        def single_gemini_call(p):
            try:
                model = genai.GenerativeModel(GEMINI_MODEL_NAME)
                resp = model.generate_content(p)
                return clean_special_chars(resp.text.strip())
            except Exception:
                return ""
        
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            print(f" ðŸ¤– [Gemini-Parallel] æ­£åœ¨å¹¶å‘å¤„ç† {len(prompts)} æ¡è¯·æ±‚...")
            results = list(executor.map(single_gemini_call, prompts))
        return results

    # --- HuggingFaceï¼šåˆ©ç”¨ batching å’Œ device_map è¿›è¡Œç¡¬ä»¶å¹¶è¡Œ ---
    if MODEL_SOURCE == "huggingface":
        if GLOBAL_MODEL is None:
            return [""] * len(prompts)

        try:
            print(f" ðŸš€ [Local-Batch] æ­£åœ¨å¹¶è¡Œç”Ÿæˆ {len(prompts)} æ¡ (Batch Size={LLM_BATCH_SIZE})...", end="", flush=True)
            
            text_list = []
            for p in prompts:
                messages = [{"role": "system", "content": "You are a helpful assistant."}, {"role": "user", "content": p}]
                text = GLOBAL_TOKENIZER.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
                text_list.append(text)

            model_inputs = GLOBAL_TOKENIZER(
                text_list,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=MAX_INPUT_TOKENS,
            ).to(GLOBAL_MODEL.device)

            with torch.no_grad():
                generated_ids = GLOBAL_MODEL.generate(
                    model_inputs.input_ids,
                    attention_mask=model_inputs.attention_mask,
                    max_new_tokens=max_new_tokens,
                    do_sample=False,
                    pad_token_id=GLOBAL_TOKENIZER.pad_token_id
                )

            results = []
            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids):
                new_token_ids = output_ids[len(input_ids):]
                text = GLOBAL_TOKENIZER.decode(new_token_ids, skip_special_tokens=True)
                results.append(clean_special_chars(text.strip()))
            print(" å®Œæˆ")
            return results
        except Exception as e:
            print(f"\nâŒ [Local Error]: {e}")
            return [""] * len(prompts)

    return [""] * len(prompts)


# =============== ä»»åŠ¡ Prompt æž„é€  ===============

def get_summarize_prompt(group_texts: List[str]) -> str:
    """æž„é€ é«˜é¢‘åˆå¹¶ Prompt"""
    items_formatted = "\n".join(f"[{i+1}] {t}" for i, t in enumerate(group_texts))
    return f"""ä½ æ˜¯æ•°å­¦åŠ©æ•™ã€‚ä¸‹é¢æ˜¯ä¸€ç»„å±žäºŽåŒä¸€é¢˜åž‹çš„è®°å¿†æ¡ç›®ï¼Œå®ƒä»¬éƒ½æ¥è‡ªåŒä¸€ä¸ªèšç±»ï¼ˆåŒç±»é—®é¢˜ï¼‰ã€‚
è¯·å°†å®ƒä»¬åˆå¹¶æˆ**ä¸€æ¡æ›´å®Œæ•´ã€æ›´æŠ½è±¡çš„è®°å¿†**ï¼Œè¦æ±‚ï¼š
1. ä¸æ”¹å˜ä»»ä½•ç»“è®ºï¼Œä¹Ÿä¸è¦å¼•å…¥æ–°çš„æ•°å€¼æˆ–é¢å¤–äº‹å®žã€‚
2. ä¿ç•™æ‰€æœ‰å…³é”®æ¡ä»¶ã€å…¬å¼ä¸Žè§£é¢˜ç»“è®ºã€‚
3. é€‚å½“æ€»ç»“å…±åŒçš„è§£é¢˜æ€è·¯ï¼Œå¯ä»¥åˆå¹¶é‡å¤ä¿¡æ¯ã€‚
4. ç”¨Englishå†™æˆä¸€æ®µæˆ–ä¸¤æ®µè¿žç»­æ–‡æœ¬ï¼Œä¸è¦åˆ†æ¡åˆ—å‡ºåŽŸé¢˜å·ã€‚

å¾…åˆå¹¶çš„è®°å¿†æ¡ç›®å¦‚ä¸‹ï¼š
{items_formatted}
"""

def get_expand_prompt(text: str) -> str:
    """æž„é€ ä½Žé¢‘æ‰©å†™ Prompt"""
    return f"""ä½ æ˜¯æ•°å­¦åŠ©æ•™ã€‚ä¸‹é¢æ˜¯ä¸€æ¡æ•°å­¦é¢˜ç›®çš„è®°å¿†å†…å®¹ã€‚
è¯·åœ¨ **ä¸æ”¹å˜é¢˜ç›®æ¡ä»¶å’Œç­”æ¡ˆã€ä¸æ·»åŠ ä»»ä½•æ–°æ•°å€¼æˆ–äº‹å®ž** çš„å‰æä¸‹ï¼Œå¯¹å®ƒè¿›è¡Œè¯­ä¹‰æ‰©å†™ï¼š
1. å¯ä»¥å¢žåŠ å¯¹é¢˜ç›®è€ƒå¯Ÿç‚¹çš„è§£é‡Šå’ŒèƒŒæ™¯è¯´æ˜Žã€‚
2. å¯ä»¥åŠ å…¥åŒä¹‰æ”¹å†™ã€æ›´å¤šè‡ªç„¶è¯­è¨€è¡¨è¿°ï¼Œä»¥ä¾¿æœªæ¥æ›´å®¹æ˜“è¢«æ£€ç´¢åˆ°ã€‚
3. è¾“å‡ºä¸€æ®µæˆ–ä¸¤æ®µEnglishæ–‡æœ¬ï¼Œä¸è¦ä¸¢å¤±åŽŸå§‹ä¿¡æ¯ã€‚

åŽŸå§‹è®°å¿†ï¼š
{text}
"""


# =============== æ•°æ®åŠ è½½ä¸Žå‘é‡è®¡ç®— (ä¿æŒåŽŸé€»è¾‘) ===============

def load_clustered_memories(path: str) -> Tuple[Dict[str, dict], List[str]]:
    memories, order = {}, []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip(): continue
            obj = json.loads(line)
            mid = str(obj["id"])
            memories[mid] = obj
            order.append(mid)
    return memories, order

def load_cluster_summary(path: str) -> Dict[int, List[str]]:
    cluster_to_ids = {}
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip(): continue
            obj = json.loads(line)
            cluster_to_ids[int(obj["cluster_id"])] = [str(x) for x in obj.get("memory_ids", [])]
    return cluster_to_ids

def load_memory_freq(path: str) -> Dict[str, int]:
    freq_map = {}
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip(): continue
            obj = json.loads(line)
            mid = str(obj.get("memory_id", obj.get("id", "")))
            if mid: freq_map[mid] = int(obj.get("freq", 0))
    return freq_map

def build_embeddings_for_memories(memories: Dict[str, dict]) -> Dict[str, np.ndarray]:
    device = "cuda" if has_cuda() else "cpu"
    model = SentenceTransformer(EMBEDDING_MODEL, device=device)
    ids = list(memories.keys())
    texts = [memories[mid].get("question") or memories[mid].get("contents", "") for mid in ids]
    embeddings = model.encode(texts, batch_size=32, show_progress_bar=True, normalize_embeddings=True)
    return {mid: embeddings[i] for i, mid in enumerate(ids)}

def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
    return float(np.dot(a, b))

def select_high_low_ids(freq_map: Dict[str, int], top_k_high: int, bottom_k_low: int, low_freq: int):
    items = sorted(freq_map.items(), key=lambda x: -x[1])
    high_ids = [mid for mid, f in items[:top_k_high]]
    items_asc = sorted(freq_map.items(), key=lambda x: x[1])
    low_ids, zero_ids = [], []
    for mid, f in items_asc:
        if f == 0: zero_ids.append(mid)
        elif f == low_freq and len(low_ids) < bottom_k_low: low_ids.append(mid)
    return set(high_ids), set(low_ids), set(zero_ids)


# =============== ä¸»ä¼˜åŒ–é€»è¾‘ (é‡ç‚¹ä¿®æ”¹ï¼šæ‰¹é‡å¹¶è¡Œ) ===============

def optimize_memory():
    init_llm()
    memories, id_order = load_clustered_memories(CLUSTERED_FILE)
    cluster_to_ids = load_cluster_summary(CLUSTER_SUMMARY_FILE)
    freq_map = load_memory_freq(MEM_FREQ_FILE)
    for mid in memories: freq_map.setdefault(mid, 0)

    high_ids, low_ids, zero_ids = select_high_low_ids(freq_map, TOP_K_HIGH, BOTTOM_K_LOW, LOW_FREQ_THRESHOLD)
    id_to_emb = build_embeddings_for_memories(memories)

    to_delete_ids = set(zero_ids)
    merged_consumed_ids = set()

    # --- é˜¶æ®µ 4ï¼šé«˜é¢‘èšåˆ (æ‰¹é‡æ”¶é›†æ¨¡å¼) ---
    print("\n========== é«˜é¢‘è®°å¿†èšåˆé˜¶æ®µ (å¤šå¡å¹¶è¡Œå‡†å¤‡) ==========")
    high_ids_sorted = sorted(list(high_ids), key=lambda x: -freq_map.get(x, 0))
    
    aggregation_tasks = [] # å­˜å‚¨ (anchor_id, neighbors, prompt)

    for anchor_id in high_ids_sorted:
        if anchor_id not in memories or anchor_id in merged_consumed_ids: continue
        
        rec_anchor = memories[anchor_id]
        cid = rec_anchor.get("cluster_id")
        if cid is None: continue
        
        members = [str(x) for x in cluster_to_ids.get(int(cid), [])]
        candidates = [m for m in members if m != anchor_id and m not in merged_consumed_ids]
        if not candidates: continue

        anchor_emb = id_to_emb.get(anchor_id)
        sims = [(m, cosine_similarity(anchor_emb, id_to_emb[m])) for m in candidates if m in id_to_emb]
        if not sims: continue

        neighbors = [m for m, _ in sorted(sims, key=lambda x: -x[1])[:TOP_N_SIMILAR_IN_CLUSTER]]
        
        # é¢„å¤‡æ–‡æœ¬
        group_ids = [anchor_id] + neighbors
        group_texts = [f"[ID {mid}] {memories[mid].get('question') or memories[mid].get('contents', '')}" for mid in group_ids]
        
        # è®°å½•ä»»åŠ¡
        aggregation_tasks.append({
            "anchor_id": anchor_id,
            "neighbors": neighbors,
            "prompt": get_summarize_prompt(group_texts)
        })

        # æ ‡è®°æ¶ˆè€—
        for mid in neighbors: merged_consumed_ids.add(mid)

    # æ‰¹é‡æ‰§è¡Œé«˜é¢‘èšåˆ
    if aggregation_tasks:
        prompts = [t["prompt"] for t in aggregation_tasks]
        results = []
        for i in range(0, len(prompts), LLM_BATCH_SIZE):
            batch = prompts[i : i + LLM_BATCH_SIZE]
            results.extend(call_llm_batch(batch))

        for task, summary in zip(aggregation_tasks, results):
            if not summary: continue
            aid = task["anchor_id"]
            neighbors = task["neighbors"]
            
            rec = memories[aid]
            rec["original_question"] = rec.get("question") or rec.get("contents", "")
            rec["question"] = summary
            rec["merged_from_ids"] = [aid] + neighbors
            rec["merge_type"] = "high_freq_anchor"
            
            for mid in neighbors:
                if freq_map.get(mid, 0) < LOW_FREQ_THRESHOLD:
                    to_delete_ids.add(mid)

    # --- é˜¶æ®µ 5ï¼šä½Žé¢‘æ‰©å†™ (æ‰¹é‡æ”¶é›†æ¨¡å¼) ---
    print("\n========== ä½Žé¢‘è®°å¿†æ‰©å†™é˜¶æ®µ (å¤šå¡å¹¶è¡Œå‡†å¤‡) ==========")
    low_expand_ids = [mid for mid in low_ids if mid in memories and mid not in to_delete_ids]
    
    if low_expand_ids:
        expand_prompts = [get_expand_prompt(memories[mid].get("question") or memories[mid].get("contents", "")) for mid in low_expand_ids]
        expand_results = []
        for i in range(0, len(expand_prompts), LLM_BATCH_SIZE):
            batch = expand_prompts[i : i + LLM_BATCH_SIZE]
            expand_results.extend(call_llm_batch(batch))

        for mid, expanded in zip(low_expand_ids, expand_results):
            if not expanded: continue
            rec = memories[mid]
            rec["original_question"] = rec.get("question") or rec.get("contents", "")
            rec["question"] = expanded
            rec["opt_type"] = "low_freq_expanded"

    # --- é˜¶æ®µ 6ï¼šå†™å‡ºç»“æžœ ---
    print("\n========== å†™å‡ºä¼˜åŒ–åŽçš„è®°å¿†åº“ ==========")
    kept_count = 0
    with open(OUTPUT_OPTIMIZED_FILE, "w", encoding="utf-8") as f:
        for mid in id_order:
            if mid in memories and mid not in to_delete_ids:
                f.write(json.dumps(memories[mid], ensure_ascii=False) + "\n")
                kept_count += 1

    print(f"âœ… å®Œæˆï¼ä¿ç•™: {kept_count}, åˆ é™¤: {len(to_delete_ids)}")


if __name__ == "__main__":
    optimize_memory()