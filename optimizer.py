import os
import json
import time
from typing import Dict, List, Tuple, Set
import numpy as np
import torch
from sentence_transformers import SentenceTransformer
# Hydra
import hydra
from omegaconf import DictConfig
from utils.toolfunction import clean_special_chars,has_cuda
from tools.optimize.callllm import init_llm,call_llm,call_llm_batch
from tools.optimize.memoryload import load_clustered_memories,load_cluster_summary,load_memory_freq
# ================= å…¨å±€å˜é‡ (ä¿æŒåŸé€»è¾‘) =================
GLOBAL_MODEL = None
GLOBAL_TOKENIZER = None
GLOBAL_SGLANG_CLIENT = None

# ===== é«˜é¢‘ & ä½é¢‘è®°å¿†çš„ LLM æ“ä½œ =====
def summarize_high_freq_prompt(group_texts: List[str], cfg: DictConfig) -> str:
    items_formatted = "\n".join(
        f"[{i+1}] {t}" for i, t in enumerate(group_texts)
    )
    template = cfg.optimizer.prompts.summarize_high_freq
    prompt = template.format(items_formatted=items_formatted)
    return prompt

def expand_low_freq_memory_prompt(text: str, cfg: DictConfig) -> str:
    """æ„é€ ä½é¢‘è®°å¿†æ‰©å†™çš„ prompt"""
    template = cfg.optimizer.prompts.expand_low_freq
    prompt = template.format(text=text)
    
    return prompt


# =============== Embedding & ç›¸ä¼¼åº¦ ===============

def build_embeddings_for_memories(memories: Dict[str, dict], model_name: str) -> Dict[str, np.ndarray]:
    device = "cuda" if has_cuda() else "cpu"
    print(f"ğŸš€ æ­£åœ¨è®¡ç®—è®°å¿†å‘é‡ ({model_name}) on {device}...")
    model = SentenceTransformer(model_name, device=device)

    ids = list(memories.keys())
    texts = []
    for mid in ids:
        rec = memories[mid]
        text = rec.get("contents", "")
        texts.append(text)

    embeddings = model.encode(
        texts,
        batch_size=32,
        show_progress_bar=True,
        normalize_embeddings=True
    )
    id_to_emb = {mid: embeddings[i] for i, mid in enumerate(ids)}
    print(f"âœ… å‘é‡æ„å»ºå®Œæˆï¼Œå…± {len(id_to_emb)} æ¡")
    return id_to_emb


def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
    return float(np.dot(a, b))


# =============== é«˜é¢‘/ä½é¢‘é›†åˆé€‰æ‹© ===============

def select_high_low_ids(
    freq_map: Dict[str, int],
    top_k_high: int,
    bottom_k_low: int,
    low_freq_for_low_only: int = 1
):
    items = list(freq_map.items())
    # é«˜é¢‘ï¼šæŒ‰ freq é™åº
    sorted_desc = sorted(items, key=lambda x: -x[1])
    high_ids = [mid for mid, f in sorted_desc[:top_k_high]]

    # ä½é¢‘ï¼šæŒ‰ freq å‡åº
    sorted_asc = sorted(items, key=lambda x: x[1])
    low_ids = []
    zero_ids = []
    for mid, f in sorted_asc:
        if f < 0:
            zero_ids.append(mid)
            continue
        if f == low_freq_for_low_only:
            low_ids.append(mid)
        if len(low_ids) >= bottom_k_low:
            break

    print(f"ğŸ”¥ é«˜é¢‘ anchor æ•°é‡: {len(high_ids)}")
    print(f"ğŸ§Š åˆ†æ•°å°äº-2çš„è®°å¿†æ•°é‡: {len(zero_ids)}ï¼ˆä¹‹åä¼šåˆ é™¤ï¼‰")
    print(f"ğŸ¥¶ ä½é¢‘æ‰©å†™å€™é€‰(freq={low_freq_for_low_only})æ•°é‡: {len(low_ids)} (æœ€å¤š bottom_k={bottom_k_low})")
    return set(high_ids), set(low_ids), set(zero_ids)


# =============== ä¸»ä¼˜åŒ–é€»è¾‘ (Hydra Managed) ===============

@hydra.main(version_base=None, config_path="conf", config_name="config")
def optimize_memory(cfg: DictConfig):
    # 0. åˆå§‹åŒ– LLM
    init_llm(cfg)

    # 1. è¯»å…¥åŸºç¡€æ•°æ® (ä½¿ç”¨ config ä¸­çš„è·¯å¾„)
    cluster_file = cfg.paths.cluster_output
    summary_file = cfg.paths.cluster_summary
    freq_file = cfg.paths.freq_file
    output_file = cfg.paths.optimized_memory

    memories, id_order = load_clustered_memories(cluster_file)
    cluster_to_ids = load_cluster_summary(summary_file)
    freq_map = load_memory_freq(freq_file)

    if not memories:
        print("âŒ æ— æ³•åŠ è½½è®°å¿†æ•°æ®ï¼Œç¨‹åºé€€å‡ºã€‚")
        return

    # ä¸ºæ‰€æœ‰è®°å¿†è¡¥é½é¢‘æ¬¡
    for mid in memories.keys():
        freq_map.setdefault(mid, 0)

    # 2. é€‰å‡ºé«˜é¢‘ã€ä½é¢‘ã€0 é¢‘é›†åˆ (ä½¿ç”¨ config ä¸­çš„å‚æ•°)
    high_ids, low_ids, zero_ids = select_high_low_ids(
        freq_map,
        top_k_high=cfg.optimizer.top_k_high,
        bottom_k_low=cfg.optimizer.bottom_k_low,
        low_freq_for_low_only=cfg.optimizer.low_freq_threshold
    )

    # 3. å‡†å¤‡å‘é‡
    id_to_emb = build_embeddings_for_memories(memories, cfg.model.embedding_name)

# 4. é«˜é¢‘ï¼šç±»å†…èšåˆï¼ˆmergeï¼‰
    merged_consumed_ids = set()      
    to_delete_ids = set()            

    print("\n========== é«˜é¢‘è®°å¿†èšåˆé˜¶æ®µ (Batch Optimized) ==========")
    to_delete_ids.update(zero_ids)
    high_ids_sorted = sorted(list(high_ids), key=lambda x: -freq_map.get(x, 0))
    top_n_similar = cfg.optimizer.top_n_similar
    
    # ä¸´æ—¶ç¼“å­˜é˜Ÿåˆ—
    batch_size = cfg.optimizer.llm_batch_size
    batch_prompts = []
    batch_metadata = [] # å­˜å…ƒæ•°æ®ï¼Œç”¨äºå›è°ƒæ›´æ–°: (rec_anchor, group_ids)

    for anchor_id in high_ids_sorted:
        if anchor_id not in memories: continue
        if anchor_id in merged_consumed_ids: continue

        rec_anchor = memories[anchor_id]
        cluster_id = rec_anchor.get("cluster_id")
        if cluster_id is None: continue
        cluster_id = int(cluster_id)
        
        cluster_member_ids = [str(x) for x in cluster_to_ids.get(cluster_id, [])]
        if not cluster_member_ids: continue

        candidates = [
            mid for mid in cluster_member_ids
            if mid != anchor_id and mid not in merged_consumed_ids
        ]
        if not candidates: continue

        anchor_emb = id_to_emb.get(anchor_id)
        if anchor_emb is None: continue

        sims = []
        for mid in candidates:
            emb = id_to_emb.get(mid)
            if emb is None: continue
            sims.append((mid, cosine_similarity(anchor_emb, emb)))

        if not sims: continue

        sims_sorted = sorted(sims, key=lambda x: -x[1])
        neighbors = [mid for mid, _ in sims_sorted[:top_n_similar]]
        group_ids = [anchor_id] + neighbors

        print(f"\nğŸ”¥ [Plan] Anchor {anchor_id} (freq={freq_map[anchor_id]}) å‡†å¤‡åˆå¹¶ top-{len(neighbors)} é‚»å±…")
        print("   >>> é‚»å±…è¯¦æƒ… (ID | Similarity):")
        for mid, score in sims_sorted[:top_n_similar]:
             print(f"       - ID: {mid:<6} | Sim: {score:.4f}")
        # ğŸ”¥ å…³é”®ä¿®æ”¹ 1: ç«‹å³æ ‡è®°é‚»å±…ä¸ºâ€œå·²æ¶ˆè€—â€ï¼Œé˜²æ­¢å½“å‰ Batch åé¢çš„ Anchor æŠ¢å 
        # è™½ç„¶ LLM è¿˜æ²¡è·‘å®Œï¼Œä½†æˆ‘ä»¬å…ˆå åº§ï¼Œä¿è¯è´ªå¿ƒé€»è¾‘çš„é¡ºåºæ€§
        for mid in neighbors:
            merged_consumed_ids.add(mid)
            # åªæœ‰è¢«åˆå¹¶ä¸”é¢‘æ¬¡ä½çš„æ‰åˆ é™¤
            if freq_map.get(mid, 0) < cfg.optimizer.low_freq_threshold:
                to_delete_ids.add(mid)
        
        # æ„é€  Prompt æ–‡æœ¬
        group_texts = []
        for mid in group_ids:
            rec = memories[mid]
            text = rec.get("contents", "")
            group_texts.append(f"[ID {mid}] {text}")

        # ç”Ÿæˆ Prompt å¹¶åŠ å…¥ Batch é˜Ÿåˆ—
        prompt = summarize_high_freq_prompt(group_texts, cfg)
        batch_prompts.append(prompt)
        batch_metadata.append({
            "rec_anchor": rec_anchor,
            "group_ids": group_ids,
            "anchor_id": anchor_id
        })

        # ğŸ”¥ å…³é”®ä¿®æ”¹ 2: å‡‘å¤Ÿ Batch ç«‹å³æ‰§è¡Œ
        if len(batch_prompts) >= batch_size:
            print(f"ğŸš€ [Batch Execution] å¹¶å‘æ‰§è¡Œ {len(batch_prompts)} ä¸ªé«˜é¢‘èšåˆä»»åŠ¡...")
            outputs = call_llm_batch(batch_prompts, cfg)
            
            # å›å¡«ç»“æœ
            for task_info, summary_text in zip(batch_metadata, outputs):
                if not summary_text:
                    print(f"   âš ï¸ LLM è¿”å›ä¸ºç©ºï¼Œè·³è¿‡ Anchor {task_info['anchor_id']}")
                    continue
                
                rec = task_info['rec_anchor']
                rec["contents"] = summary_text
                rec["merged_from_ids"] = task_info['group_ids']
                rec["merge_type"] = "high_freq_anchor"
            
            # æ¸…ç©ºé˜Ÿåˆ—
            batch_prompts = []
            batch_metadata = []

    # ğŸ”¥ å…³é”®ä¿®æ”¹ 3: å¤„ç†å¾ªç¯ç»“æŸåå‰©ä½™çš„ä»»åŠ¡
    if batch_prompts:
        print(f"ğŸš€ [Batch Execution] å¤„ç†å‰©ä½™çš„ {len(batch_prompts)} ä¸ªé«˜é¢‘èšåˆä»»åŠ¡...")
        outputs = call_llm_batch(batch_prompts, cfg)
        for task_info, summary_text in zip(batch_metadata, outputs):
            if not summary_text: continue
            rec = task_info['rec_anchor']
            rec["contents"] = summary_text
            rec["merged_from_ids"] = task_info['group_ids']
            rec["merge_type"] = "high_freq_anchor"

    # 5. ä½é¢‘ï¼šæ‰©å†™
    print("\n========== ä½é¢‘è®°å¿†æ‰©å†™é˜¶æ®µ ==========")

    low_expand_ids = [
        mid for mid in low_ids
        if mid in memories and mid not in to_delete_ids
    ]
    print(f"ğŸ¥¶ éœ€è¦æ‰©å†™çš„ä½é¢‘è®°å¿†æ¡ç›®æ•°: {len(low_expand_ids)}")

    low_expand_items = []
    for mid in low_expand_ids:
        rec = memories[mid]
        base_text = rec.get("contents", "")
        low_expand_items.append((mid, base_text))

    batch_size = cfg.optimizer.llm_batch_size
    total_low = len(low_expand_items)
    
    for start in range(0, total_low, batch_size):
        end = min(start + batch_size, total_low)
        batch_items = low_expand_items[start:end]
        batch_ids = [mid for (mid, _) in batch_items]

        print(f"\nğŸ¥¶ æ‰©å†™ä½é¢‘è®°å¿† Batch {start // batch_size + 1} / { (total_low + batch_size - 1) // batch_size }")
        print(f"   IDs: {batch_ids}")

        # ğŸ”¥ ä¿®æ­£: è¿™é‡Œä¹‹å‰æ¼ä¼ äº† cfg å‚æ•°ï¼Œç°åœ¨è¡¥ä¸Š
        batch_prompts = [
            expand_low_freq_memory_prompt(base_text, cfg) 
            for (_, base_text) in batch_items
        ]
        
        batch_outputs = call_llm_batch(batch_prompts, cfg)

        for (mid, base_text), expanded in zip(batch_items, batch_outputs):
            if not expanded:
                print(f"   âš ï¸ LLM è¿”å›ä¸ºç©ºï¼ŒID={mid} ä¿æŒåŸæ–‡ä¸å˜")
                continue
            rec = memories[mid]
            rec["contents"] = expanded
            rec["opt_type"] = "low_freq_expanded"
    # 6. å†™å‡ºæ–°çš„è®°å¿†åº“
    print("\n========== å†™å‡ºä¼˜åŒ–åçš„è®°å¿†åº“ ==========")
    kept_count = 0
    with open(output_file, "w", encoding="utf-8") as f:
        for mid in id_order:
            if mid not in memories: continue
            if mid in to_delete_ids: continue
            f.write(json.dumps(memories[mid], ensure_ascii=False) + "\n")
            kept_count += 1

    print(f"âœ… æ–°è®°å¿†åº“å†™å…¥å®Œæˆ: {output_file}")
    print(f"   ä¿ç•™è®°å¿†æ¡ç›®: {kept_count}")
    print(f"   åˆ é™¤è®°å¿†æ¡ç›®: {len(to_delete_ids)}")

if __name__ == "__main__":
    optimize_memory()