import os
import json
import re
import time
import torch
import bm25s
import logging
import ast
import collections # æ–°å¢: ç”¨äºè®¡æ•°
from datasets import load_dataset
from tqdm import tqdm
from huggingface_hub import snapshot_download
from flashrag.config import Config
from flashrag.pipeline import SequentialPipeline
from flashrag.utils import get_retriever, get_generator, Dataset
from flashrag.prompt import PromptTemplate
import matplotlib.pyplot as plt
import json
# å±è”½ transformers çš„å†—ä½™è­¦å‘Š
import transformers
transformers.logging.set_verbosity_error()

# ==========================================
# ğŸ› ï¸ æ ¸å¿ƒé…ç½®åŒºåŸŸ (å·²ä¿®æ”¹ä¸º GSM8K)
# ==========================================
os.environ["CUDA_VISIBLE_DEVICES"] = "3,4,5,6,7"
# 1. å®éªŒæ§åˆ¶å¼€å…³
# é€‰é¡¹: 'baseline' (åªæµ‹åŸæ¨¡å‹), 'rag' (åªæµ‹FlashRAG), 'all' (å¯¹æ¯”æµ‹è¯•)
EXPERIMENT_MODE = "all" 

# ğŸ”¥ [æ–°å¢] è®°å¿†çƒ­åº¦ç»Ÿè®¡å¼€å…³
# True: ç”»å‡ºè®°å¿†è°ƒç”¨é¢‘æ¬¡åˆ†å¸ƒå›¾ (ä¿å­˜ä¸ºpng)
# False: ä»…åœ¨ç»ˆç«¯è¾“å‡ºé¢‘æ¬¡æœ€é«˜çš„ Top 30 è®°å¿†ID
VISUALIZE_MEMORY_DISTRIBUTION = True

# 2. è°ƒè¯•æ ·æœ¬æ•°
# é€‰é¡¹: 10 (å¿«é€Ÿæµ‹è¯•), None (è·‘å…¨é‡)
# MATH æµ‹è¯•é›†æœ‰ 1319 æ¡ï¼Œå»ºè®®å…ˆè®¾ä¸º 20-50 æ¡è·‘é€šæµç¨‹
DEBUG_NUM = None

# 3. æ¨¡å‹è®¾ç½®
# é€‰é¡¹: 'huggingface' (æœ¬åœ°/HFæ¨¡å‹) æˆ– 'gemini' (Google API)
MODEL_SOURCE = "huggingface" 

# é€‰é¡¹ï¼šQwen/Qwen3-4B-Instruct-2507 Qwen/Qwen2-1.5B-Instruct
# å»ºè®®ä½¿ç”¨ Qwen2.5-7B-Instructï¼Œæ•ˆæœæ›´ç¨³å®š
HF_MODEL_NAME = "Qwen/Qwen3-4B-Instruct-2507" 

# [Gemini é…ç½®]
GEMINI_MODEL_NAME = "gemini-2.5-flash"
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY") 

# ==========================================
# 4. æ•°æ®é›†é…ç½® (MATH / LightEval ä¸“ç”¨é…ç½®)
# ==========================================
# lighteval/MATH å…¶å®é€šå¸¸æŒ‡å‘ hendrycks/competition_math
# å»ºè®®ç›´æ¥ä½¿ç”¨åŸå§‹æºï¼Œæˆ–è€…ç¡®ä¿ä½ å¼•ç”¨çš„åº“å­˜åœ¨
DATASET_NAME = "DigitalLearningGmbH/MATH-lighteval" 
# MATH æ•°æ®é›†éœ€è¦æŒ‡å®šå­é›†ï¼Œ"all" è¡¨ç¤ºåŠ è½½ä»£æ•°ã€å‡ ä½•ç­‰æ‰€æœ‰ç±»åˆ«
DATASET_CONFIG = "algebra" 

SPLIT_TRAIN = "train" 
SPLIT_TEST = "test"   

# ==========================================
# 5. å­—æ®µæ˜ å°„ (âš ï¸ å…³é”®ä¿®æ”¹)
# ==========================================
# GSM8K çš„åˆ—åæ˜¯ 'question' å’Œ 'answer'
# MATH  çš„åˆ—åé€šå¸¸æ˜¯ 'problem'  å’Œ 'solution'
FIELD_MAP = {
    'question': 'problem',        
    'answer': 'solution' 
}

# ==========================================
# âš™ï¸ è‡ªåŠ¨è·¯å¾„ç”Ÿæˆ (å‹¿åŠ¨)
# ==========================================
dataset_tag = DATASET_NAME.split('/')[-1]
corpus_file = f"{dataset_tag}_corpus.jsonl"
test_file = f"{dataset_tag}_test_data.jsonl"
index_dir = f"{dataset_tag}_bm25_index"

timestamp = time.strftime("%Y%m%d_%H%M%S")
RESULT_LOG_FILE = f"{dataset_tag}_{MODEL_SOURCE}_{EXPERIMENT_MODE}_{timestamp}.txt"
VIS_IMAGE_FILE = f"memory_distribution_{timestamp}.png"

# ğŸ”¥ æ–°å¢ï¼šè®°å¿†è°ƒç”¨é¢‘æ¬¡æ’åºç»“æœï¼ˆjsonlï¼‰
MEM_FREQ_JSONL_FILE = f"{dataset_tag}_memory_freq_{timestamp}.jsonl"
# ==========================================
# 1. æ•°æ®å‡†å¤‡æ¨¡å— (é€‚é… GSM8K)
# ==========================================
def prepare_data():
    print(f"ğŸ“¥ [Step 1] æ­£åœ¨åŠ è½½æ•°æ®é›†: {DATASET_NAME} (Config: {DATASET_CONFIG})...")
    try:
        if DATASET_CONFIG:
            dataset = load_dataset(DATASET_NAME, DATASET_CONFIG)
        else:
            dataset = load_dataset(DATASET_NAME)
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return False

    q_col = FIELD_MAP['question']
    a_col = FIELD_MAP['answer']

    # --- A. æ„å»ºè®°å¿†åº“ (ä½¿ç”¨ Train é›†) ---
    if not os.path.exists(corpus_file):
        print(f"ğŸ”¨ [Memory] æ­£åœ¨å°† {SPLIT_TRAIN} é›†è½¬æ¢ä¸ºè®°å¿†åº“: {corpus_file}...")
        with open(corpus_file, "w", encoding="utf-8") as f:
            if SPLIT_TRAIN not in dataset:
                print(f"âš ï¸ è­¦å‘Š: æ•°æ®é›†æ²¡æœ‰ {SPLIT_TRAIN} åˆ’åˆ†ï¼")
                return False
                
            for i, item in enumerate(tqdm(dataset[SPLIT_TRAIN])):
                q_text = item.get(q_col, "")
                a_text = item.get(a_col, "") # GSM8K ç›´æ¥æ˜¯å­—ç¬¦ä¸²ï¼Œä¸éœ€è¦ eval
                
                # æ„å»ºæ£€ç´¢å†…å®¹ï¼šé€šå¸¸æ£€ç´¢ç›¸ä¼¼çš„é—®é¢˜å’Œè§£é¢˜æ€è·¯
                content = f"Question: {q_text}\nAnswer: {a_text}"
                f.write(json.dumps({"id": str(i), "contents": content}) + "\n")
    else:
        print(f"âœ… [Memory] æ£€æµ‹åˆ°ç°æœ‰è®°å¿†åº“: {corpus_file}ï¼Œè·³è¿‡æ„å»ºã€‚")
    
    # --- B. å‡†å¤‡æµ‹è¯•é›† ---
    print(f"ğŸ”¨ [Test] æ­£åœ¨æå–æµ‹è¯•é›† (æ ·æœ¬æ•°: {DEBUG_NUM if DEBUG_NUM else 'ALL'})...")
    with open(test_file, "w", encoding="utf-8") as f:
        if SPLIT_TEST not in dataset:
             print(f"âŒ é”™è¯¯: æ•°æ®é›†æ²¡æœ‰ {SPLIT_TEST} åˆ’åˆ†ï¼")
             return False
             
        test_data = dataset[SPLIT_TEST]
        if DEBUG_NUM:
            limit = min(DEBUG_NUM, len(test_data))
            test_data = test_data.select(range(limit))
            
        for i, item in enumerate(test_data):
            q_text = item.get(q_col, "")
            raw_ans = item.get(a_col, "") # è¿™é‡Œæ˜¯åŒ…å« reasoning + #### result çš„å®Œæ•´ç­”æ¡ˆ
            
            f.write(json.dumps({
                "id": str(i),
                "question": q_text,
                "golden_answers": [str(raw_ans)] # å­˜å…¥åˆ—è¡¨ä¿æŒæ ¼å¼ä¸€è‡´
            }) + "\n")
    return True

# ==========================================
# 2. ç´¢å¼•æ„å»ºæ¨¡å— (BM25)
# ==========================================
def build_index():
    if os.path.exists(index_dir) and os.path.exists(os.path.join(index_dir, "vocab.tokenizer.json")):
        print(f"âœ… [Index] ç´¢å¼•å·²å­˜åœ¨: {index_dir}ï¼Œè·³è¿‡æ„å»ºã€‚")
        return

    print(f"ğŸ”¨ [Index] æ­£åœ¨ä¸º {corpus_file} æ„å»º BM25 ç´¢å¼•...")
    corpus_texts = []
    with open(corpus_file, "r", encoding="utf-8") as f:
        for line in f:
            corpus_texts.append(json.loads(line)['contents'])
    
    corpus_tokens = bm25s.tokenize(corpus_texts)
    retriever_builder = bm25s.BM25()
    retriever_builder.index(corpus_tokens)
    retriever_builder.save(index_dir)
    
    with open(os.path.join(index_dir, "stopwords.tokenizer.json"), "w") as f:
        json.dump([], f)
    with open(os.path.join(index_dir, "vocab.tokenizer.json"), "w") as f:
        vocab = corpus_tokens.vocab
        json.dump({"word_to_id": vocab, "stem_to_sid": vocab, "word_to_stem": {k: k for k in vocab}}, f)
    print("âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼")

# ==========================================
# 3. Gemini ç”Ÿæˆå™¨ç±»
# ==========================================
class GeminiGenerator:
    def __init__(self, model_name, api_key):
        import google.generativeai as genai
        if not api_key:
            raise ValueError("âŒ æœªæ£€æµ‹åˆ° API Keyï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ GEMINI_API_KEY")
        
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model_name)
        print(f"ğŸ¤– Gemini Generator ({model_name}) å·²åŠ è½½")
        
        self.max_input_len = 30000 

    def generate(self, input_list, **kwargs):
        responses = []
        for prompt in input_list:
            try:
                if isinstance(prompt, list): prompt = " ".join(prompt)
                clean_prompt = str(prompt)
                result = self.model.generate_content(clean_prompt)
                if result.parts:
                    responses.append(result.text)
                else:
                    responses.append("Error: Empty Response (Safety Block)")
                import time
                time.sleep(2) # ç¨å¾®å‡å°‘ä¸€ç‚¹ sleepï¼ŒåŠ å¿«é€Ÿåº¦
            except Exception as e:
                print(f"âš ï¸ Gemini API Error: {e}")
                import time
                time.sleep(5)
                responses.append("Error")
        return responses

# ==========================================
# 4. è¯„ä¼°å·¥å…· (ä¸“ä¸º Mathlighteval æ”¹é€ )
# ==========================================

import re

def extract_math_answer(text):
    """
    ä¸“é—¨ç”¨äº MATH/LightEval æ•°æ®é›†çš„ç­”æ¡ˆæå–é€»è¾‘ã€‚
    ç›®æ ‡ï¼šæå– \boxed{...} ä¸­çš„å†…å®¹ã€‚
    """
    if not text:
        return None
    text = str(text)

    # --- ç­–ç•¥ 1: æ ‡å‡† \boxed{...} æå– (æ”¯æŒåµŒå¥—æ‹¬å·) ---
    # ç®€å•çš„æ­£åˆ™ r'\\boxed\{(.*?)\}' æ— æ³•å¤„ç† \boxed{\frac{1}{2}} è¿™ç§åµŒå¥—æƒ…å†µ
    # æ‰€ä»¥æˆ‘ä»¬éœ€è¦ä»åå¾€å‰æ‰¾ \boxed{ï¼Œç„¶åç”¨æ ˆé€»è¾‘åŒ¹é…å³æ‹¬å·
    idx = text.rfind("\\boxed{")
    if idx != -1:
        # ä» "boxed{" åé¢å¼€å§‹æ‰¾
        content_start = idx + 7 
        balance = 0
        for i in range(content_start, len(text)):
            char = text[i]
            if char == '{':
                balance += 1
            elif char == '}':
                if balance == 0:
                    return text[content_start:i] # æ‰¾åˆ°é—­åˆç‚¹ï¼Œè¿”å›å†…å®¹
                balance -= 1
    
    # --- ç­–ç•¥ 2: å¦‚æœæ²¡æ‰¾åˆ° boxedï¼Œå°è¯•æå–æœ€åä¸€è¡Œ (ä¿åº•ç­–ç•¥) ---
    # å¾ˆå¤šæ¨¡å‹å¦‚æœæ²¡æœ‰éµå¾ªæŒ‡ä»¤è¾“å‡º boxedï¼Œç­”æ¡ˆé€šå¸¸åœ¨æœ€å
    lines = text.strip().split('\n')
    if lines:
        last_line = lines[-1].strip()
        # ç®€å•çš„æ¸…ç†ï¼šå»æ‰ "Answer:" "The answer is" ç­‰å‰ç¼€
        last_line = re.sub(r'^(The )?Answer( is)?:?', '', last_line, flags=re.IGNORECASE).strip()
        # å¦‚æœå‰©ä¸‹çš„æ˜¯ä¸ªå¾ˆçŸ­çš„å­—ç¬¦ä¸²ï¼ˆæ¯”å¦‚æ•°å­—æˆ–çŸ­å…¬å¼ï¼‰ï¼Œå°±å½“åšç­”æ¡ˆ
        if len(last_line) < 50: 
            return last_line

    return None

def normalize_latex(s):
    """
    å¯¹ LaTeX ç­”æ¡ˆè¿›è¡Œç®€å•çš„å½’ä¸€åŒ–ï¼Œä»¥ä¾¿è¿›è¡Œå­—ç¬¦ä¸²æ¯”è¾ƒã€‚
    """
    if not s: return ""
    s = str(s)
    # 1. å»é™¤æ‰€æœ‰ç©ºç™½å­—ç¬¦ (ç©ºæ ¼ã€æ¢è¡Œ) -> "x + y" å˜æˆ "x+y"
    s = "".join(s.split())
    # 2. ç»Ÿä¸€éƒ¨åˆ† LaTeX å†™æ³• (å¯é€‰ï¼Œæ ¹æ®éœ€è¦æ‰©å±•)
    # æ¯”å¦‚æŠŠ \dfrac å˜æˆ \frac
    s = s.replace("\\dfrac", "\\frac")
    # 3. å»æ‰æ–‡æœ¬æ¨¡å¼æ ‡è®° (å¯é€‰)
    s = s.replace("\\text", "")
    return s.strip()

def evaluate_results(results, experiment_name):
    correct = 0
    total = len(results)
    
    with open(RESULT_LOG_FILE, "a", encoding="utf-8") as f:
        header = f"\n{'='*20} {experiment_name} {'='*20}\n"
        print(header.strip())
        f.write(header)
        
        for i, item in enumerate(results):
            pred = item.pred if hasattr(item, 'pred') else item['pred']
            gold_raw = item.golden_answers[0] if hasattr(item, 'golden_answers') else item['golden_answers'][0]
            question = item.question if hasattr(item, 'question') else item['question']

            # --- 1. æå– Gold Answer ---
            # MATH æ•°æ®é›†çš„ gold_raw é€šå¸¸æœ¬èº«å°±æ˜¯ solutionï¼Œæœ€åä¸€æ®µå«æœ‰ \boxed{}
            gold_val = extract_math_answer(gold_raw)
            if gold_val is None:
                # å¦‚æœæ ‡å‡†ç­”æ¡ˆé‡Œç«Ÿç„¶æ²¡æœ‰ boxed (æå°‘è§)ï¼Œåˆ™å–æœ€åä¸€éƒ¨åˆ†
                gold_val = str(gold_raw).strip()

            # --- 2. æå– Prediction Answer ---
            pred_val = extract_math_answer(pred)
            
            # --- 3. å¯¹æ¯”åˆ¤æ–­ (æ ¸å¿ƒä¿®æ”¹ï¼šå­—ç¬¦ä¸²å½’ä¸€åŒ–å¯¹æ¯”) ---
            is_right = False
            
            # å¿…é¡»ä¸¤è€…éƒ½æœ‰å€¼æ‰èƒ½æ¯”
            if gold_val and pred_val:
                # å½’ä¸€åŒ–å¤„ç†
                norm_gold = normalize_latex(gold_val)
                norm_pred = normalize_latex(pred_val)
                
                # å­—ç¬¦ä¸²å…¨ç­‰å¯¹æ¯” (Exact Match)
                if norm_gold == norm_pred:
                    is_right = True
                
                # [å¯é€‰] å°è¯•æ•°å€¼å¯¹æ¯” (é˜²æ­¢ 1/2 != 0.5 çš„æƒ…å†µ)
                # åªæœ‰å½“ä¸¤è€…çœ‹èµ·æ¥éƒ½åƒçº¯æ•°å­—æ—¶æ‰å°è¯•
                # try:
                #     if abs(float(norm_gold) - float(norm_pred)) < 1e-6:
                #         is_right = True
                # except:
                #     pass

            if is_right:
                correct += 1

            # æ‰“å°æ—¥å¿—
            log_entry = (
                f"\n[ID]: {i}\n"
                f"[Question]: {str(question)[:100]}...\n" # é¢˜ç›®å¤ªé•¿æˆªæ–­ä¸€ä¸‹
                f"[Gold Raw]: ... => [Extracted]: {gold_val}\n"
                f"[Pred Raw]: ...{str(pred)[-50:].replace(chr(10), ' ')} => [Extracted]: {pred_val}\n"
                f"[Result]: {'âœ… Correct' if is_right else 'âŒ Wrong'}\n"
                f"{'-'*30}\n"
            )
            f.write(log_entry)
            if i < 10: print(log_entry.strip())

        acc = correct / total * 100
        summary = (
            f"\nğŸ“Š ç»Ÿè®¡ ({experiment_name}):\n"
            f"Total: {total}, Correct: {correct}, Accuracy: {acc:.2f}%\n"
            f"{'='*50}\n"
        )
        print(summary)
        f.write(summary)
    return acc
# ==========================================
# ğŸ”¥ [é‡æ„ç‰ˆ] è®°å¿†è°ƒç”¨é¢‘æ¬¡åˆ†æ (å«å…¨é‡ç»Ÿè®¡ & å ä½ç¬¦)
# ==========================================
def analyze_memory_usage(rag_results):
    print("\nğŸ” [Analysis] æ­£åœ¨è¿›è¡Œå…¨é‡è®°å¿†çƒ­åº¦ç»Ÿè®¡...")
    
    # -------------------------------------------------------
    # 1. å»ºç«‹å…¨é‡è®°å¿†è´¦æœ¬ (åˆå§‹åŒ–æ‰€æœ‰ ID ä¸º 0)ï¼ŒåŒæ—¶ä¿å­˜å†…å®¹
    # -------------------------------------------------------
    all_memory_ids = set()
    id_to_content = {}  # æ–°å¢ï¼šè®°å½•æ¯æ¡è®°å¿†çš„åŸå§‹æ–‡æœ¬

    try:
        with open(corpus_file, "r", encoding="utf-8") as f:
            for line in f:
                item = json.loads(line)
                mid = str(item['id'])
                all_memory_ids.add(mid)
                # è®°ä½è¿™æ¡è®°å¿†çš„å†…å®¹ï¼Œæ–¹ä¾¿åé¢å†™å…¥ jsonl
                id_to_content[mid] = item.get("contents", "")
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è¯»å–è®°å¿†åº“æ–‡ä»¶ {corpus_file}ï¼Œå°†ä»…ç»Ÿè®¡è¢«æ£€ç´¢åˆ°çš„è®°å¿†ã€‚é”™è¯¯: {e}")
    
    # åˆå§‹åŒ–è®¡æ•°å™¨ï¼Œæ‰€æœ‰å·²çŸ¥ ID é»˜è®¤ä¸º 0
    memory_counter = collections.Counter({mid: 0 for mid in all_memory_ids})
    
    # -------------------------------------------------------
    # 2. ç»Ÿè®¡å®é™…æ£€ç´¢å‘½ä¸­
    # -------------------------------------------------------
    for item in rag_results:
        retrieved_docs = getattr(item, 'retrieval_result', [])
        
        for doc in retrieved_docs:
            if isinstance(doc, dict):
                doc_id = str(doc.get('id'))
            else:
                doc_id = str(getattr(doc, 'id', None))
                
            if doc_id:
                memory_counter[doc_id] += 1

    # -------------------------------------------------------
    # 3. æ’åº (æŒ‰é¢‘æ¬¡é™åº -> ID å‡åº)
    # -------------------------------------------------------
    sorted_memories = sorted(memory_counter.items(), key=lambda x: (-x[1], x[0]))
    
    total_memories = len(sorted_memories)
    used_memories = sum(1 for _, v in sorted_memories if v > 0)
    unused_memories = total_memories - used_memories
    
    print(f"ğŸ“Š è®°å¿†åº“æ€»é‡: {total_memories}")
    print(f"ğŸ”¥ è¢«æ¿€æ´»çš„è®°å¿†: {used_memories} ({(used_memories/total_memories)*100:.2f}%)")
    print(f"ğŸ§Š æ²‰ç¡çš„è®°å¿†(0æ¬¡): {unused_memories}")

    # -------------------------------------------------------
    # 4. âœ… æ–°å¢ï¼šå¯¼å‡ºæŒ‰é¢‘æ¬¡æ’åºçš„ jsonl
    # -------------------------------------------------------
    try:
        print(f"ğŸ’¾ [Save] æ­£åœ¨å¯¼å‡ºè®°å¿†è°ƒç”¨é¢‘æ¬¡æ’åºç»“æœåˆ°: {MEM_FREQ_JSONL_FILE}")
        with open(MEM_FREQ_JSONL_FILE, "w", encoding="utf-8") as f:
            for rank, (mid, freq) in enumerate(sorted_memories, start=1):
                record = {
                    "rank": rank,
                    "memory_id": mid,
                    "freq": int(freq),
                    "contents": id_to_content.get(mid, "")
                }
                f.write(json.dumps(record, ensure_ascii=False) + "\n")
        print("âœ… è°ƒç”¨é¢‘æ¬¡ jsonl å¯¼å‡ºå®Œæˆï¼")
    except Exception as e:
        print(f"âŒ å¯¼å‡º {MEM_FREQ_JSONL_FILE} å¤±è´¥: {e}")

    # -------------------------------------------------------
    # 5. å¯è§†åŒ–é€»è¾‘ (Top 30 ... Bottom 30)
    # -------------------------------------------------------
    if VISUALIZE_MEMORY_DISTRIBUTION:
        print(f"ğŸ¨ [Visual] æ­£åœ¨ç”Ÿæˆé¢‘æ¬¡åˆ†å¸ƒå›¾: {VIS_IMAGE_FILE}")
        try:
            ids = [m[0] for m in sorted_memories]
            counts = [m[1] for m in sorted_memories]
            
            display_limit = 50
            
            if len(ids) > display_limit * 2:
                print(f"â„¹ï¸ å±•ç¤ºç­–ç•¥: Top {display_limit} + å ä½ç¬¦ + Bottom {display_limit}")
                
                head_ids = ids[:display_limit]
                head_counts = counts[:display_limit]
                
                tail_ids = ids[-display_limit:]
                tail_counts = counts[-display_limit:]
                
                plot_ids = head_ids + ["..."] + tail_ids
                plot_counts = head_counts + [0] + tail_counts
                
                colors = ['skyblue'] * len(head_ids) + ['white'] + ['salmon'] * len(tail_ids)
                edge_colors = ['navy'] * len(head_ids) + ['white'] + ['darkred'] * len(tail_ids)
            else:
                plot_ids = ids
                plot_counts = counts
                colors = 'skyblue'
                edge_colors = 'navy'

            plt.figure(figsize=(15, 6))
            bars = plt.bar(plot_ids, plot_counts, color=colors, edgecolor=edge_colors)
            
            plt.title(f'Memory Usage Distribution (Top {display_limit} vs Bottom {display_limit})', fontsize=14)
            plt.xlabel('Memory ID', fontsize=12)
            plt.ylabel('Frequency', fontsize=12)
            
            plt.xticks(rotation=90, fontsize=8) 
            plt.grid(axis='y', linestyle='--', alpha=0.5)
            
            for i, bar in enumerate(bars):
                height = bar.get_height()
                if plot_ids[i] != "...": 
                    plt.text(
                        bar.get_x() + bar.get_width()/2., height,
                        f'{int(height)}',
                        ha='center', va='bottom', fontsize=8
                    )
            
            plt.tight_layout()
            plt.savefig(VIS_IMAGE_FILE, dpi=300)
            print("âœ… å›¾ç‰‡ä¿å­˜æˆåŠŸï¼")
            
        except ImportError:
            print("âŒ ç¼ºå°‘ matplotlib")
            
    else:
        print("\nğŸ† [Top 10 Hot Memories]")
        for mid, count in sorted_memories[:10]:
            print(f"   ID: {mid:<5} | Count: {count}")
            
        print("\nğŸ§Š [Bottom 10 Cold Memories]")
        for mid, count in sorted_memories[-10:]:
             print(f"   ID: {mid:<5} | Count: {count}")

# ==========================================
# 5. ä¸»ç¨‹åº
# ==========================================
def main():

    print("Visible GPU count:", torch.cuda.device_count())
    for i in range(torch.cuda.device_count()):
        print(f"  GPU {i}:", torch.cuda.get_device_name(i))


    if os.path.exists(RESULT_LOG_FILE): os.remove(RESULT_LOG_FILE)
    print(f"ğŸ“ ç»“æœå°†ä¿å­˜è‡³: {RESULT_LOG_FILE}")
    print(f"ğŸ› ï¸ æ¨¡å¼: {EXPERIMENT_MODE} | æº: {MODEL_SOURCE} | æ•°æ®é›†: {DATASET_NAME}")

    if not prepare_data(): return
    if EXPERIMENT_MODE in ['rag', 'all']: build_index()
    
    generator = None
    config = None
    
    if MODEL_SOURCE == "gemini":
        print(f"ğŸ¤– [Init] åˆå§‹åŒ– Gemini: {GEMINI_MODEL_NAME}...")
        gemini_config_dict = {
            "device": "cpu",
            "retrieval_method": "bm25",
            "corpus_path": corpus_file,
            "index_path": index_dir,
            "retriever_model_path": index_dir,
            "generator_model": "huggingface", 
            "generator_model_path": "gpt2",   
            "generation_method": "custom",  
            "save_dir": "rag_result_cache"
        }
        config = Config(config_dict=gemini_config_dict)
        generator = GeminiGenerator(GEMINI_MODEL_NAME, GEMINI_API_KEY)
        
    elif MODEL_SOURCE == "huggingface":
        print(f"ğŸ“¥ [Init] æ£€æŸ¥/ä¸‹è½½ HF æ¨¡å‹: {HF_MODEL_NAME}...")
        try:
            model_path = snapshot_download(repo_id=HF_MODEL_NAME)
        except:
            print("âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ– HF_ENDPOINT è®¾ç½®")
            return

        hf_config_dict = {
            "device": "cuda" if torch.cuda.is_available() else "cpu",
            "gpu_num": torch.cuda.device_count(),
            "generator_model": "huggingface",
            "generator_model_path": model_path,
            "generation_method": "huggingface",
            "batch_size":10, # ä¿å®ˆä¸€ç‚¹ï¼Œé˜²æ­¢æ˜¾å­˜çˆ†ç‚¸æˆ– padding é—®é¢˜
            "max_input_len": 4096, 
            "max_new_tokens": 1024,
            "save_dir": "rag_result_cache"
        }
        print("ğŸš€ åŠ è½½ HF ç”Ÿæˆå™¨...")
        config = Config(config_dict=hf_config_dict)
        generator = get_generator(config)
        
        # ğŸ”¥ğŸ”¥ğŸ”¥ Critical Fix: Tokenizer & Padding ğŸ”¥ğŸ”¥ğŸ”¥
        # å¾ˆå¤š Instruct æ¨¡å‹åœ¨æ‰¹é‡ç”Ÿæˆæ—¶ï¼Œå¦‚æœ padding è®¾ç½®ä¸å¯¹ï¼Œä¼šç›´æ¥è¾“å‡ºä¹±ç æˆ–åœæ­¢ç¬¦
        if hasattr(generator, 'tokenizer'):
            # ç¡®ä¿ä½¿ç”¨ Left Padding (ç”Ÿæˆä»»åŠ¡å¿…é¡»)
            generator.tokenizer.padding_side = 'left' 
            # ç¡®ä¿ pad_token å­˜åœ¨
            if generator.tokenizer.pad_token is None:
                generator.tokenizer.pad_token = generator.tokenizer.eos_token
                generator.tokenizer.pad_token_id = generator.tokenizer.eos_token_id
            
            # å¼ºåˆ¶æ›´æ–° max length
            generator.tokenizer.model_max_length = 4096
        
        if hasattr(generator, 'model'):
            # ç¡®ä¿æ¨¡å‹ config ä¹Ÿæœ‰ pad token id
            if hasattr(generator.model.config, 'pad_token_id') and generator.model.config.pad_token_id is None:
                generator.model.config.pad_token_id = generator.tokenizer.pad_token_id

        generator.max_input_len = 4096
        print(f"âœ… Tokenizer ä¿®æ­£: padding_side='left', pad_token={generator.tokenizer.pad_token}")
    
    else:
        print(f"âŒ æœªçŸ¥çš„ MODEL_SOURCE: {MODEL_SOURCE}")
        return

    with open(test_file, "r") as f:
        test_dataset_raw = [json.loads(line) for line in f]

    acc_baseline = 0
    acc_rag = 0

    # ==========================================
    # ğŸ”¥ Prompt æ ¼å¼åŒ– (ä¿®å¤: ä½¿ç”¨çº¯æ–‡æœ¬æŒ‡ä»¤æ ¼å¼ï¼Œæ”¾å¼ƒå®¹æ˜“å‡ºé”™çš„ ChatML)
    # ==========================================
    def format_base_prompt(system_text, user_text):
        """
        ä½¿ç”¨æœ€ç¨³å¥çš„ Alpaca é£æ ¼æˆ– Question/Answer é£æ ¼ã€‚
        å¯¹äº Qwen-Instructï¼Œ### Question: ... è¿™ç§æ ¼å¼é€šå¸¸æ¯”é”™è¯¯çš„ ChatML æ ‡ç­¾æ›´å¥½ç”¨ã€‚
        """
        if MODEL_SOURCE == "gemini":
            return f"{system_text}\n\n{user_text}" if system_text else user_text
            
        # HuggingFace æ¨¡å‹é€šç”¨æ ¼å¼
        prompt = ""
        if system_text:
            prompt += f"{system_text}\n\n"
        
        # æ„é€ æ¸…æ™°çš„ Q&A ç»“æ„
        # é‡ç‚¹ï¼šæœ«å°¾åŠ ä¸Š "Let's think step by step." ä½œä¸º Priming (å¯åŠ¨å­)
        prompt += f"### Question:\n{user_text}\n\n### Answer:\nLet's think step by step."
        return prompt

    if EXPERIMENT_MODE in ['baseline', 'all']:
        print("\nâš”ï¸ [Task A] æ­£åœ¨è¿è¡Œ Baseline ...")
        
        baseline_inputs = []
        for item in test_dataset_raw:
            # åœ¨ User Text é‡Œä¸è¦åŠ  instructionï¼Œæ”¾åˆ° format å‡½æ•°é‡Œç»Ÿä¸€åŠ ï¼Œé¿å…æ··ä¹±
            user_content = item['question']
            
            # æ„é€ æç¤ºè¯
            sys_msg = "You are a math expert. Solve the problem in a brief. Don't answer more than 50 words.End your answer with \boxed\{number\}."
            formatted_prompt = format_base_prompt(sys_msg, user_content)
            baseline_inputs.append(formatted_prompt)

        baseline_preds = generator.generate(baseline_inputs)
        
        baseline_results = []
        for item, pred in zip(test_dataset_raw, baseline_preds):
            baseline_results.append({
                "question": item['question'],
                "golden_answers": item['golden_answers'],
                "pred": pred
            })
        acc_baseline = evaluate_results(baseline_results, "Baseline (No RAG)")

    if EXPERIMENT_MODE in ['rag', 'all']:
        print("\nâš”ï¸ [Task B] æ­£åœ¨è¿è¡Œ FlashRAG (Few-shot Retrieval)...")
        
        rag_config_dict = config.config_dict.copy() if hasattr(config, 'config_dict') else {}
        if not rag_config_dict:
             rag_config_dict = gemini_config_dict if MODEL_SOURCE == "gemini" else hf_config_dict
             
        rag_config_dict.update({
            "retrieval_method": "bm25",
            "corpus_path": corpus_file,
            "index_path": index_dir,
            "retriever_model_path": index_dir,
            "topk": 3 
        })
        
        rag_config = Config(config_dict=rag_config_dict)
        retriever = get_retriever(rag_config)
        
        # --- FlashRAG Prompt Template (ä¿®å¤ç‰ˆ) ---
        # 1. æ”¾å¼ƒå¤æ‚çš„ ChatML
        # 2. ç»Ÿä¸€ä½¿ç”¨ ### æ ‡è®°
        # 3. å¼ºåˆ¶ Priming ("Let's think step by step.")
        
        rag_system_part = (
            "You are a math expert. You can solve math problems in one second to give the correct answer. Below are some similar solved problems. "
            "Refer to the logic in these examples to solve the new question.\n\n"
            "Solve the problem in a very brief. Don't answer more than 80 tokens. If the problem is easy, You can also just give the final answer."
            "Do not perform unit conversion."
            "{reference}\n\n"
            "### Question:\n{question}\n\n"
            "### Answer:\n"
            "Let's think step by step." 
        )
        
        # System Prompt åŒ…å«äº†æ‰€æœ‰ç»“æ„ï¼ŒUser Prompt ç•™ç©ºï¼ˆæˆ–è€…ç”± FlashRAG å†…éƒ¨å¤„ç†ï¼‰
        # FlashRAG çš„ Dataset é»˜è®¤æŠŠ query å¡«å…¥ {question}ï¼Œretrieval å¡«å…¥ {reference}
        prompt_tpl = PromptTemplate(rag_config, system_prompt=rag_system_part, user_prompt="")
        
        pipeline = SequentialPipeline(rag_config, prompt_template=prompt_tpl, retriever=retriever, generator=generator)
        
        dataset_obj = Dataset(rag_config, test_file)
        
        # è¿è¡Œ Pipeline
        rag_results = pipeline.run(dataset_obj)
        
        # 1. è¯„ä¼°å‡†ç¡®ç‡
        acc_rag = evaluate_results(rag_results, f"FlashRAG ({dataset_tag} Memory)")
        
        # 2. ğŸ”¥ ç»Ÿè®¡è®°å¿†çƒ­åº¦
        analyze_memory_usage(rag_results)

    if EXPERIMENT_MODE == 'all':
        summary = (
            f"\n{'='*20} æœ€ç»ˆå¯¹æ¯”ç»“æœ {'='*20}\n"
            f"ğŸ“Š æ•°æ®é›†: {DATASET_NAME}\n"
            f"ğŸ¤– æ¨¡å‹: {MODEL_SOURCE} / {GEMINI_MODEL_NAME if MODEL_SOURCE=='gemini' else HF_MODEL_NAME}\n"
            f"ğŸ“‰ Baseline: {acc_baseline:.2f}%\n"
            f"ğŸ“ˆ FlashRAG: {acc_rag:.2f}%\n"
            f"ğŸš€ æå‡: {acc_rag - acc_baseline:+.2f}%\n"
            f"{'='*50}\n"
        )
        print(summary)
        with open(RESULT_LOG_FILE, "a", encoding="utf-8") as f:
            f.write(summary)



import os
import json
import re
import time
import numpy as np
import torch
import google.generativeai as genai
import matplotlib.pyplot as plt 
from typing import List, Dict
from sentence_transformers import SentenceTransformer
from sklearn.cluster import AgglomerativeClustering 
from sklearn.feature_extraction.text import TfidfVectorizer
from transformers import AutoModelForCausalLM, AutoTokenizer

# ================= é…ç½®åŒºåŸŸ =================

# 1. æ ¸å¿ƒå¼€å…³: é€‰æ‹©èµ·åå­—çš„æ¨¡å‹æ¥æº
# é€‰é¡¹: 'huggingface' (æœ¬åœ°æ˜¾å¡è·‘) æˆ– 'gemini' (è°·æ­ŒAPI)
MODEL_SOURCE = "huggingface" 

# [HuggingFace é…ç½®]
HF_MODEL_NAME = "Qwen/Qwen3-4B-Instruct-2507" 

# [Gemini é…ç½®]
GEMINI_MODEL_NAME = "gemini-2.5-flash"
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")

# 2. æ–‡ä»¶é…ç½®
INPUT_FILE = "MATH-lighteval_corpus.jsonl" #å…¶ä»–çš„ç›´æ¥gsm8kæ”¹mathå°±è¡Œäº†
# INPUT_FILE = "gsm8k_corpus.jsonl"
# è¾“å‡ºæ–‡ä»¶ 1: è¯¦ç»†ç»“æœ (æ¯è¡Œä¸€é“é¢˜ï¼ŒåŒ…å«å…¶ç±»åˆ«)
OUTPUT_FILE = "MATH-lighteval_auto_clustered_result.jsonl"
# è¾“å‡ºæ–‡ä»¶ 2: èšç±»æ‘˜è¦ (æ¯è¡Œä¸€ä¸ªç±»ï¼ŒåŒ…å«è¯¥ç±»ä¸‹æ‰€æœ‰é¢˜å·) -> ğŸ”¥ æ–°å¢
SUMMARY_OUTPUT_FILE = "MATH-lighteval_cluster_summary.jsonl"
# è¾“å‡ºæ–‡ä»¶ 3: ç»Ÿè®¡å›¾è¡¨
PLOT_FILE = "MATH-lighteval_cluster_distribution.png"

# 3. èšç±»å‚æ•°
DISTANCE_THRESHOLD = 1.0  # è·ç¦»é˜ˆå€¼
EMBEDDING_MODEL = "BAAI/bge-large-en-v1.5" 
# ===========================================

# å…¨å±€å˜é‡ç”¨äºå­˜å‚¨æœ¬åœ°æ¨¡å‹ï¼Œé˜²æ­¢é‡å¤åŠ è½½
GLOBAL_MODEL = None
GLOBAL_TOKENIZER = None

# =============== 0. å·¥å…·å‡½æ•° ===============
def clean_special_chars(text: str) -> str:
    """æ¸…æ´—å¼‚å¸¸å­—ç¬¦"""
    if not isinstance(text, str): return text
    return text.replace('\u2028', ' ').replace('\u2029', ' ')

def normalize_text(x: str) -> str:
    x = x.lower()
    x = re.sub(r"\d+(\.\d+)?", " <num> ", x) 
    x = re.sub(r"\s+", " ", x).strip()
    return x

def import_torch_and_check_gpu():
    try: return torch.cuda.is_available()
    except: return False

# =============== 1. LLM åˆå§‹åŒ–ä¸è°ƒç”¨ ===============

def init_llm():
    """åˆå§‹åŒ– LLM (ä»…é’ˆå¯¹æœ¬åœ°æ¨¡å‹)"""
    global GLOBAL_MODEL, GLOBAL_TOKENIZER
    
    if MODEL_SOURCE == "gemini":
        if GEMINI_API_KEY:
            genai.configure(api_key=GEMINI_API_KEY)
            print(f"ğŸ¤– [Init] Gemini API ({GEMINI_MODEL_NAME}) å·²é…ç½®")
        else:
            print("âš ï¸ [Init] æœªæ£€æµ‹åˆ° GEMINI_API_KEYï¼ŒGemini æ¨¡å¼å¯èƒ½æ— æ³•å·¥ä½œ")
            
    elif MODEL_SOURCE == "huggingface":
        print(f"ğŸ“¥ [Init] æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹: {HF_MODEL_NAME} ...")
        try:
            GLOBAL_TOKENIZER = AutoTokenizer.from_pretrained(HF_MODEL_NAME, trust_remote_code=True)
            GLOBAL_MODEL = AutoModelForCausalLM.from_pretrained(
                HF_MODEL_NAME,
                device_map="auto",
                torch_dtype="auto",
                trust_remote_code=True
            ).eval()
            print("âœ… [Init] æœ¬åœ°æ¨¡å‹åŠ è½½å®Œæˆï¼")
        except Exception as e:
            print(f"âŒ [Init] æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ’¡ æç¤º: è¯·ç¡®ä¿å·²é€šè¿‡ `huggingface-cli login` ç™»å½•æˆ–æ£€æŸ¥ç½‘ç»œ")

def call_llm(prompt: str) -> str:
    """ç»Ÿä¸€ LLM è°ƒç”¨æ¥å£"""
    
    # --- åˆ†æ”¯ A: Gemini ---
    if MODEL_SOURCE == "gemini":
        if not GEMINI_API_KEY: return "Skipped (No Key)"
        model = genai.GenerativeModel(GEMINI_MODEL_NAME) 
        try:
            print("  ğŸ¤– [Gemini] æ­£åœ¨æ€è€ƒ...", end="", flush=True)
            resp = model.generate_content(prompt)
            print(" å®Œæˆ!")
            return clean_special_chars(resp.text.strip())
        except Exception as e:
            print(f"\nâŒ [Gemini Error]: {e}")
            time.sleep(1)
            return "Unknown Topic"

    # --- åˆ†æ”¯ B: HuggingFace (æœ¬åœ°) ---
    elif MODEL_SOURCE == "huggingface":
        if GLOBAL_MODEL is None:
            return "Skipped (Model Not Loaded)"
        
        try:
            print("  ğŸš€ [Local] æ­£åœ¨æ¨ç†...", end="", flush=True)
            
            messages = [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt}
            ]
            text = GLOBAL_TOKENIZER.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            model_inputs = GLOBAL_TOKENIZER([text], return_tensors="pt").to(GLOBAL_MODEL.device)

            with torch.no_grad():
                generated_ids = GLOBAL_MODEL.generate(
                    model_inputs.input_ids,
                    max_new_tokens=50, 
                    do_sample=False    
                )
            
            generated_ids = [
                output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
            ]
            response = GLOBAL_TOKENIZER.batch_decode(generated_ids, skip_special_tokens=True)[0]
            
            print(" å®Œæˆ!")
            return clean_special_chars(response.strip())
            
        except Exception as e:
            print(f"\nâŒ [Local Error]: {e}")
            return "Unknown Topic"
            
    return "Unknown Config"

# =============== 2. åŸºç¡€ IO ===============
def load_questions(jsonl_path: str):
    print(f"ğŸ“¥ æ­£åœ¨åŠ è½½æ–‡ä»¶: {jsonl_path}...")
    if not os.path.exists(jsonl_path):
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {jsonl_path}")
        return [], []

    ids, questions = [], []
    with open(jsonl_path, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip(): continue
            try:
                obj = json.loads(line)
            except json.JSONDecodeError: continue
            
            content = obj.get("contents", "")
            if "Question:" in content:
                q_part = content.split("Solution:")[0].replace("Question:", "").strip()
            else:
                q_part = content
            
            ids.append(str(obj["id"]))
            questions.append(clean_special_chars(q_part))
            
    print(f"âœ… åŠ è½½å®Œæˆï¼Œå…± {len(questions)} æ¡æ•°æ®")
    return ids, questions

# =============== 3. ä¸»æµç¨‹ï¼šembedding + è‡ªåŠ¨èšç±» ===============
def build_embeddings(questions: List[str], model_name: str) -> np.ndarray:
    print(f"ğŸš€ æ­£åœ¨è®¡ç®— Embeddings ({model_name})...")
    device = "cuda" if import_torch_and_check_gpu() else "cpu"
    print(f"   >>> ä½¿ç”¨è®¾å¤‡: {device}")
    
    model = SentenceTransformer(model_name, device=device)
    q_norm = [normalize_text(q) for q in questions]
    emb = model.encode(q_norm, batch_size=32, show_progress_bar=True, normalize_embeddings=True)
    return np.asarray(emb)

def cluster_questions_auto(embeddings: np.ndarray, threshold: float) -> np.ndarray:
    print(f"ğŸ¤– æ­£åœ¨æ‰§è¡Œè‡ªåŠ¨èšç±» (Distance Threshold={threshold})...")
    
    model = AgglomerativeClustering(
        n_clusters=None, 
        distance_threshold=threshold,
        metric='euclidean', 
        linkage='ward'
    )
    labels = model.fit_predict(embeddings)
    
    n_clusters_found = len(set(labels))
    print(f"âœ¨ è‡ªåŠ¨èšç±»å®Œæˆï¼æ¨¡å‹è‡ªåŠ¨å‘ç°äº† {n_clusters_found} ä¸ªé¢˜å‹ç±»åˆ«ã€‚")
    return labels

# =============== 4. ç»Ÿè®¡ç»˜å›¾ & å…³é”®è¯ ===============

def plot_cluster_stats(labels: np.ndarray, save_path: str):
    print(f"\nğŸ“Š æ­£åœ¨ç”Ÿæˆç»Ÿè®¡å›¾è¡¨...")
    unique_labels, counts = np.unique(labels, return_counts=True)
    
    singleton_mask = counts == 1
    num_singletons = np.sum(singleton_mask)
    
    valid_mask = ~singleton_mask
    valid_labels = unique_labels[valid_mask]
    valid_counts = counts[valid_mask]
    
    print(f"   - æ€»èšç±»æ•°: {len(unique_labels)}")
    print(f"   - å­¤ç«‹èšç±»æ•° (Size=1): {num_singletons} (è¿™éƒ¨åˆ†ä¸ç”»åœ¨å›¾é‡Œ)")
    print(f"   - æœ‰æ•ˆèšç±»æ•° (Size>1): {len(valid_labels)}")
    
    if len(valid_counts) == 0:
        print("   âš ï¸ æ²¡æœ‰åŒ…å«å¤šä¸ªé—®é¢˜çš„èšç±»ï¼Œè·³è¿‡ç»˜å›¾ã€‚")
        return

    sorted_indices = np.argsort(valid_counts)[::-1]
    sorted_plot_labels = valid_labels[sorted_indices]
    sorted_plot_counts = valid_counts[sorted_indices]
    
    plt.figure(figsize=(12, 6))
    x_ticks = [str(lbl) for lbl in sorted_plot_labels]
    plt.bar(x_ticks, sorted_plot_counts, color='steelblue', edgecolor='black', alpha=0.8)
    plt.xlabel('Cluster ID', fontsize=12)
    plt.ylabel('Number of Questions', fontsize=12)
    plt.title(f'Cluster Size Distribution (Descending)\n(Excluding {num_singletons} singleton clusters)', fontsize=14)
    if len(x_ticks) > 30: plt.xticks(rotation=90, fontsize=8)
    else: plt.xticks(rotation=0)
    plt.grid(axis='y', linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig(save_path)
    print(f"ğŸ–¼ï¸ å›¾è¡¨å·²ä¿å­˜è‡³: {save_path}")

def tfidf_keywords_per_cluster(questions, cluster_labels, max_features=5000, top_k=10):
    print("ğŸ” æå–å…³é”®è¯...")
    q_norm = [normalize_text(q) for q in questions]
    vectorizer = TfidfVectorizer(max_df=0.9, min_df=3, max_features=max_features, stop_words="english")
    X = vectorizer.fit_transform(q_norm)
    vocab = np.array(vectorizer.get_feature_names_out())

    cluster_keywords = {}
    for cid in np.unique(cluster_labels):
        idx = np.where(cluster_labels == cid)[0]
        if len(idx) == 0: continue
        tfidf_mean = np.asarray(X[idx].mean(axis=0)).ravel()
        top_idx = tfidf_mean.argsort()[::-1][:top_k]
        cluster_keywords[cid] = vocab[top_idx].tolist()
    return cluster_keywords

def llm_label_cluster(cid, questions, cluster_labels, cluster_keywords, max_examples=5):
    idx = np.where(cluster_labels == cid)[0]
    examples_idx = np.random.choice(idx, min(len(idx), max_examples), replace=False)
    examples = [questions[i] for i in examples_idx]
    kw = ", ".join(cluster_keywords.get(cid, []))

    prompt = f"""You are a Math Education Expert. 
I have automatically grouped similar math problems together.
Keywords: [{kw}]
Examples:
{chr(10).join(f"- {q}" for q in examples)}

Task: Provide a **very short category name** (3-6 words) for this specific math problem type.
Output ONLY the category name.
"""
    label = call_llm(prompt)
    return label.replace("\n", "").replace('"', "").strip()

# =============== Main ===============
def cluster():
    # 0. åˆå§‹åŒ–
    init_llm()

    # 1. åŠ è½½æ•°æ®
    ids, questions = load_questions(INPUT_FILE)
    if not ids: return

    # 2. Embedding
    embeddings = build_embeddings(questions, model_name=EMBEDDING_MODEL)
    
    # 3. è‡ªåŠ¨èšç±»
    labels = cluster_questions_auto(embeddings, threshold=DISTANCE_THRESHOLD)

    # 4. ç”»å›¾
    plot_cluster_stats(labels, save_path=PLOT_FILE)

    # 5. åˆ†æå…³é”®è¯
    keywords_map = tfidf_keywords_per_cluster(questions, labels)
    
    print("\n" + "="*20 + " èšç±»ç»“æœåˆ†æ " + "="*20)
    cluster_labels_text = {}
    
    unique, counts = np.unique(labels, return_counts=True)
    # æŒ‰æ•°é‡é™åºæ’åº
    sorted_clusters = sorted(zip(unique, counts), key=lambda x: x[1], reverse=True)
    
    print(f"ğŸ“Š æ€»å…±å‘ç° {len(sorted_clusters)} ä¸ªèšç±»ã€‚")
    print("   (ä»…å±•ç¤ºå¹¶å‘½ååŒ…å«é¢˜ç›®æœ€å¤šçš„å‰ 10 ä¸ªèšç±»)\n")

    for cid, count in sorted_clusters[:10]:
        print(f"\nğŸ·ï¸ åˆ†æ Cluster {cid} (åŒ…å« {count} é¢˜)...")
        label_text = llm_label_cluster(cid, questions, labels, keywords_map)
        cluster_labels_text[cid] = label_text
        print(f"   >>> é¢˜å‹: {label_text}")
        print(f"   >>> å…³é”®è¯: {keywords_map.get(cid, [])}")
        if MODEL_SOURCE == "gemini": time.sleep(2)

    # 6. ä¿å­˜è¯¦ç»†ç»“æœ (åŸåŠŸèƒ½)
    print(f"\nğŸ’¾ ä¿å­˜è¯¦ç»†ç»“æœåˆ° {OUTPUT_FILE}...")
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        for qid, q, cid in zip(ids, questions, labels):
            obj = {
                "id": qid,
                "question": q,
                "cluster_id": int(cid),
                "cluster_label": cluster_labels_text.get(int(cid), f"Cluster {cid}"),
                "cluster_keywords": keywords_map.get(int(cid), [])
            }
            f.write(json.dumps(obj, ensure_ascii=False) + "\n")
            
    # 7. ğŸ”¥ æ–°å¢ï¼šä¿å­˜èšç±»æ‘˜è¦ç´¢å¼•è¡¨
    print(f"ğŸ’¾ ä¿å­˜èšç±»æ‘˜è¦åˆ° {SUMMARY_OUTPUT_FILE}...")
    
    # æ„é€ èšåˆæ•°æ® {cluster_id: {label, ids}}
    cluster_aggregation = {}
    for qid, cid in zip(ids, labels):
        cid_int = int(cid)
        if cid_int not in cluster_aggregation:
            cluster_aggregation[cid_int] = {
                "cluster_id": cid_int,
                "cluster_label": cluster_labels_text.get(cid_int, f"Cluster {cid_int}"),
                "memory_ids": []
            }
        cluster_aggregation[cid_int]["memory_ids"].append(qid)
    
    # å†™å…¥æ–‡ä»¶
    with open(SUMMARY_OUTPUT_FILE, "w", encoding="utf-8") as f:
        # æŒ‰ cluster_id æ’åºå†™å…¥ï¼Œæ–¹ä¾¿æŸ¥çœ‹
        for cid in sorted(cluster_aggregation.keys()):
            f.write(json.dumps(cluster_aggregation[cid], ensure_ascii=False) + "\n")
            
    print("âœ… å…¨éƒ¨å®Œæˆï¼")



import os
import json
import time
from typing import Dict, List, Tuple

import numpy as np
import torch
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
import google.generativeai as genai


# ================= é…ç½®åŒºåŸŸ =================

# 1. LLM é…ç½®ï¼šå’Œä½ èšç±»æ–‡ä»¶ä¿æŒä¸€è‡´
MODEL_SOURCE = "huggingface"   # "huggingface" æˆ– "gemini"

HF_MODEL_NAME = "Qwen/Qwen3-4B-Instruct-2507"
GEMINI_MODEL_NAME = "gemini-2.5-flash"
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")


# 2. æ–‡ä»¶è·¯å¾„ï¼ˆä½ å¯ä»¥æ ¹æ®æ•°æ®é›†æ”¹åï¼›è¿™é‡Œå…ˆä»¥ MATH ä¸ºä¾‹ï¼‰
CLUSTERED_FILE = OUTPUT_FILE           # èšç±»åçš„è®°å¿†æ–‡ä»¶
CLUSTER_SUMMARY_FILE = SUMMARY_OUTPUT_FILE           # æ¯ä¸ªç±»æœ‰å“ªäº›è®°å¿†ID
MEM_FREQ_FILE = MEM_FREQ_JSONL_FILE  # è°ƒç”¨é¢‘æ¬¡æ–‡ä»¶
OUTPUT_OPTIMIZED_FILE = "MATH-lighteval_optimized_memory_k50.jsonl"   # è¾“å‡ºçš„æ–°è®°å¿†åº“

# 3. ä¼˜åŒ–é€»è¾‘å‚æ•°
TOP_K_HIGH = 50                # ä½œä¸ºâ€œé«˜é¢‘è®°å¿† anchorâ€çš„æ¡ç›®æ•°é‡ï¼ˆæŒ‰é¢‘æ¬¡æ’åºï¼‰
BOTTOM_K_LOW = 50              # ä½œä¸ºâ€œä½é¢‘è®°å¿†æ‰©å†™å¯¹è±¡â€çš„æ¡ç›®æ•°é‡ï¼ˆæŒ‰é¢‘æ¬¡ä»ä½åˆ°é«˜ï¼‰
LOW_FREQ_THRESHOLD = 2          # è¢«é«˜é¢‘åˆå¹¶æ—¶ï¼Œå¦‚æœ freq < è¿™ä¸ªé˜ˆå€¼å°±ç›´æ¥åˆ æ‰
TOP_N_SIMILAR_IN_CLUSTER = 5    # é«˜é¢‘ anchor åœ¨ç±»å†…é€‰ top-n ç›¸ä¼¼è®°å¿†æ¥åˆå¹¶

# 4. ç›¸ä¼¼åº¦ embedding æ¨¡å‹
EMBEDDING_MODEL = "BAAI/bge-large-en-v1.5"

# ==== æ–°å¢ï¼šLLM æ‰¹é‡ä¸é•¿åº¦æ§åˆ¶ ====
LLM_BATCH_SIZE = 4          # ä½é¢‘æ‰©å†™æ—¶ï¼Œä¸€æ‰¹å¤„ç†å¤šå°‘æ¡
MAX_NEW_TOKENS = 512        # ç”Ÿæˆçš„æœ€å¤§ token æ•°ï¼ˆè¾“å‡ºé•¿åº¦ï¼‰
MAX_INPUT_TOKENS = 2048     # è¾“å…¥çš„æœ€å¤§ token æ•°ï¼Œè¶…è¿‡ä¼šè¢«æˆªæ–­

# ===========================================

GLOBAL_MODEL = None
GLOBAL_TOKENIZER = None


# =============== å·¥å…·å‡½æ•° ===============

def clean_special_chars(text: str) -> str:
    if not isinstance(text, str):
        return text
    return text.replace('\u2028', ' ').replace('\u2029', ' ')


def has_cuda() -> bool:
    try:
        return torch.cuda.is_available()
    except Exception:
        return False


# =============== LLM åˆå§‹åŒ–ä¸è°ƒç”¨ ===============

def init_llm():
    """åˆå§‹åŒ– LLMï¼ˆå’Œä½ çš„èšç±»è„šæœ¬ä¿æŒä¸€è‡´é£æ ¼ï¼‰"""
    global GLOBAL_MODEL, GLOBAL_TOKENIZER

    if MODEL_SOURCE == "gemini":
        if GEMINI_API_KEY:
            genai.configure(api_key=GEMINI_API_KEY)
            print(f"ğŸ¤– [Init] Gemini API ({GEMINI_MODEL_NAME}) å·²é…ç½®")
        else:
            print("âš ï¸ [Init] æœªæ£€æµ‹åˆ° GEMINI_API_KEYï¼ŒGemini ç›¸å…³åŠŸèƒ½ä¼šè¢«è·³è¿‡")
    elif MODEL_SOURCE == "huggingface":
        print(f"ğŸ“¥ [Init] æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹: {HF_MODEL_NAME} ...")
        try:
            GLOBAL_TOKENIZER = AutoTokenizer.from_pretrained(HF_MODEL_NAME, trust_remote_code=True)
            GLOBAL_MODEL = AutoModelForCausalLM.from_pretrained(
                HF_MODEL_NAME,
                device_map="auto",
                torch_dtype="auto",
                trust_remote_code=True
            ).eval()
            print("âœ… [Init] æœ¬åœ°æ¨¡å‹åŠ è½½å®Œæˆï¼")
        except Exception as e:
            print(f"âŒ [Init] æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ’¡ æç¤º: è¯·æ£€æŸ¥ HuggingFace æƒé™å’Œç½‘ç»œ")


def call_llm(prompt: str, max_new_tokens: int = MAX_NEW_TOKENS) -> str:
    """ç»Ÿä¸€çš„å¤§æ¨¡å‹è°ƒç”¨æ¥å£ï¼ˆGemini / æœ¬åœ° Qwenï¼‰ï¼Œå•æ¡è°ƒç”¨"""

    # --- Gemini ---
    if MODEL_SOURCE == "gemini":
        if not GEMINI_API_KEY:
            return "Skipped (No GEMINI_API_KEY)"
        try:
            model = genai.GenerativeModel(GEMINI_MODEL_NAME)
            print("  ğŸ¤– [Gemini] æ­£åœ¨ç”Ÿæˆ...", end="", flush=True)
            resp = model.generate_content(prompt)
            print(" å®Œæˆ")
            return clean_special_chars(resp.text.strip())
        except Exception as e:
            print(f"\nâŒ [Gemini Error]: {e}")
            return ""

    # --- HuggingFace æœ¬åœ° ---
    elif MODEL_SOURCE == "huggingface":
        if GLOBAL_MODEL is None:
            print("âš ï¸ [Local] LLM å°šæœªåˆå§‹åŒ–")
            return ""

        try:
            print("  ğŸš€ [Local] æ­£åœ¨ç”Ÿæˆ...", end="", flush=True)
            messages = [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt}
            ]
            text = GLOBAL_TOKENIZER.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            model_inputs = GLOBAL_TOKENIZER(
                [text],
                return_tensors="pt",
                truncation=True,
                max_length=MAX_INPUT_TOKENS,
            ).to(GLOBAL_MODEL.device)

            with torch.no_grad():
                generated_ids = GLOBAL_MODEL.generate(
                    model_inputs.input_ids,
                    attention_mask=model_inputs.attention_mask,
                    max_new_tokens=max_new_tokens,
                    do_sample=False
                )
            # åªå–æ–°å¢çš„éƒ¨åˆ†
            generated_ids = [
                output_ids[len(input_ids):]
                for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
            ]
            response = GLOBAL_TOKENIZER.batch_decode(generated_ids, skip_special_tokens=True)[0]
            print(" å®Œæˆ")
            return clean_special_chars(response.strip())
        except Exception as e:
            print(f"\nâŒ [Local Error]: {e}")
            return ""

    return ""


# ==== æ–°å¢ï¼šæ‰¹é‡è°ƒç”¨æ¥å£ ====
def call_llm_batch(prompts: List[str], max_new_tokens: int = MAX_NEW_TOKENS) -> List[str]:
    """
    æ‰¹é‡è°ƒç”¨ LLMï¼š
    - HuggingFaceï¼šçœŸæ­£ batch generate
    - Geminiï¼šå†…éƒ¨å¾ªç¯ call_llmï¼ˆAPI ä¸æ”¯æŒ batchï¼‰
    """
    if not prompts:
        return []

    # Geminiï¼šç®€å•å¾ªç¯
    if MODEL_SOURCE == "gemini":
        results = []
        for p in prompts:
            results.append(call_llm(p, max_new_tokens=max_new_tokens))
        return results

    # HuggingFace æœ¬åœ°
    if MODEL_SOURCE == "huggingface":
        if GLOBAL_MODEL is None:
            print("âš ï¸ [Local] LLM å°šæœªåˆå§‹åŒ–")
            return [""] * len(prompts)

        try:
            print(f"  ğŸš€ [Local-Batch] æ­£åœ¨æ‰¹é‡ç”Ÿæˆ {len(prompts)} æ¡...", end="", flush=True)
            messages_list = [
                [
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": p}
                ]
                for p in prompts
            ]
            text_list = [
                GLOBAL_TOKENIZER.apply_chat_template(
                    msgs,
                    tokenize=False,
                    add_generation_prompt=True
                )
                for msgs in messages_list
            ]

            model_inputs = GLOBAL_TOKENIZER(
                text_list,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=MAX_INPUT_TOKENS,
            ).to(GLOBAL_MODEL.device)

            with torch.no_grad():
                generated_ids = GLOBAL_MODEL.generate(
                    model_inputs.input_ids,
                    attention_mask=model_inputs.attention_mask,
                    max_new_tokens=max_new_tokens,
                    do_sample=False
                )

            results = []
            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids):
                new_token_ids = output_ids[len(input_ids):]
                text = GLOBAL_TOKENIZER.decode(new_token_ids, skip_special_tokens=True)
                results.append(clean_special_chars(text.strip()))
            print(" å®Œæˆ")
            return results

        except Exception as e:
            print(f"\nâŒ [Local-Batch Error]: {e}")
            return [""] * len(prompts)

    return [""] * len(prompts)


# ===== é«˜é¢‘ & ä½é¢‘è®°å¿†çš„ LLM æ“ä½œ =====

def summarize_high_freq_memory(anchor_id: str, group_texts: List[str]) -> str:
    """
    é«˜é¢‘è®°å¿†ç±»å†…èšåˆï¼šç»™å®š anchor + åŒç±»è‹¥å¹²æ¡ç›¸ä¼¼è®°å¿†ï¼ŒæŠŠå®ƒä»¬åˆå¹¶æˆä¸€ä¸ªæ›´â€œæ·±â€çš„è®°å¿†ã€‚
    """
    items_formatted = "\n".join(
        f"[{i+1}] {t}" for i, t in enumerate(group_texts)
    )
    prompt = f"""ä½ æ˜¯æ•°å­¦åŠ©æ•™ã€‚ä¸‹é¢æ˜¯ä¸€ç»„å±äºåŒä¸€é¢˜å‹çš„è®°å¿†æ¡ç›®ï¼Œå®ƒä»¬éƒ½æ¥è‡ªåŒä¸€ä¸ªèšç±»ï¼ˆåŒç±»é—®é¢˜ï¼‰ã€‚
è¯·å°†å®ƒä»¬åˆå¹¶æˆ**ä¸€æ¡æ›´å®Œæ•´ã€æ›´æŠ½è±¡çš„è®°å¿†**ï¼Œè¦æ±‚ï¼š

1. ä¸æ”¹å˜ä»»ä½•ç»“è®ºï¼Œä¹Ÿä¸è¦å¼•å…¥æ–°çš„æ•°å€¼æˆ–é¢å¤–äº‹å®ã€‚
2. ä¿ç•™æ‰€æœ‰å…³é”®æ¡ä»¶ã€å…¬å¼ä¸è§£é¢˜ç»“è®ºã€‚
3. é€‚å½“æ€»ç»“å…±åŒçš„è§£é¢˜æ€è·¯ï¼Œå¯ä»¥åˆå¹¶é‡å¤ä¿¡æ¯ã€‚
4. ç”¨Englishå†™æˆä¸€æ®µæˆ–ä¸¤æ®µè¿ç»­æ–‡æœ¬ï¼Œä¸è¦åˆ†æ¡åˆ—å‡ºåŸé¢˜å·ã€‚

å¾…åˆå¹¶çš„è®°å¿†æ¡ç›®å¦‚ä¸‹ï¼š
{items_formatted}
"""
    return call_llm(prompt)


# ==== ä¿®æ”¹ï¼šæ‹†å‡ºä¸€ä¸ªåªæ„é€  prompt çš„å‡½æ•°ï¼Œæ–¹ä¾¿æ‰¹é‡ ====
def expand_low_freq_memory_prompt(text: str) -> str:
    """
    æ„é€ ä½é¢‘è®°å¿†æ‰©å†™çš„ promptï¼ˆä¸ç›´æ¥è°ƒ LLMï¼‰
    """
    prompt = f"""ä½ æ˜¯æ•°å­¦åŠ©æ•™ã€‚ä¸‹é¢æ˜¯ä¸€æ¡æ•°å­¦é¢˜ç›®çš„è®°å¿†å†…å®¹ã€‚

è¯·åœ¨ **ä¸æ”¹å˜é¢˜ç›®æ¡ä»¶å’Œç­”æ¡ˆã€ä¸æ·»åŠ ä»»ä½•æ–°æ•°å€¼æˆ–äº‹å®** çš„å‰æä¸‹ï¼Œå¯¹å®ƒè¿›è¡Œè¯­ä¹‰æ‰©å†™ï¼š
1. å¯ä»¥å¢åŠ å¯¹é¢˜ç›®è€ƒå¯Ÿç‚¹çš„è§£é‡Šå’ŒèƒŒæ™¯è¯´æ˜ã€‚
2. å¯ä»¥åŠ å…¥åŒä¹‰æ”¹å†™ã€æ›´å¤šè‡ªç„¶è¯­è¨€è¡¨è¿°ï¼Œä»¥ä¾¿æœªæ¥æ›´å®¹æ˜“è¢«æ£€ç´¢åˆ°ã€‚
3. è¾“å‡ºä¸€æ®µæˆ–ä¸¤æ®µEnglishæ–‡æœ¬ï¼Œä¸è¦ä¸¢å¤±åŸå§‹ä¿¡æ¯ã€‚

åŸå§‹è®°å¿†ï¼š
{text}
"""
    return prompt


def expand_low_freq_memory(text: str) -> str:
    """
    å•æ¡ä½é¢‘è®°å¿†æ‰©å†™ï¼ˆä¿æŒåŸæ¥å£ï¼Œå†…éƒ¨è°ƒç”¨å•æ¡ LLMï¼‰
    """
    prompt = expand_low_freq_memory_prompt(text)
    return call_llm(prompt)


# =============== æ•°æ®åŠ è½½ ===============

def load_clustered_memories(path: str) -> Tuple[Dict[str, dict], List[str]]:
    """
    è¯»å– *_auto_clustered_result.jsonl
    è¿”å›ï¼š
      - id -> è®°å½• dict
      - id_list: ä¿ç•™åŸå§‹é¡ºåºçš„ id åˆ—è¡¨ï¼ˆæ–¹ä¾¿æœ€åå†™å›ï¼‰
    """
    memories: Dict[str, dict] = {}
    order: List[str] = []
    print(f"ğŸ“¥ æ­£åœ¨åŠ è½½èšç±»åçš„è®°å¿†æ–‡ä»¶: {path}")
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
            obj = json.loads(line)
            mid = str(obj["id"])
            memories[mid] = obj
            order.append(mid)
    print(f"âœ… å…±åŠ è½½ {len(memories)} æ¡è®°å¿†")
    return memories, order


def load_cluster_summary(path: str) -> Dict[int, List[str]]:
    """
    è¯»å– *_cluster_summary.jsonl
    è¿”å›ï¼šcluster_id -> [memory_ids...]
    """
    cluster_to_ids: Dict[int, List[str]] = {}
    print(f"ğŸ“¥ æ­£åœ¨åŠ è½½èšç±»æ‘˜è¦æ–‡ä»¶: {path}")
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
            obj = json.loads(line)
            cid = int(obj["cluster_id"])
            ids = [str(x) for x in obj.get("memory_ids", [])]
            cluster_to_ids[cid] = ids
    print(f"âœ… å…±åŠ è½½ {len(cluster_to_ids)} ä¸ªèšç±»")
    return cluster_to_ids


def load_memory_freq(path: str) -> Dict[str, int]:
    """
    è¯»å–è°ƒç”¨é¢‘æ¬¡æ–‡ä»¶ MATH-lighteval_memory_freq_*.jsonl
    é¢„æœŸæ¯è¡ŒåŒ…å« memory_id / id, freq å­—æ®µã€‚
    """
    freq_map: Dict[str, int] = {}
    print(f"ğŸ“¥ æ­£åœ¨åŠ è½½è®°å¿†é¢‘æ¬¡æ–‡ä»¶: {path}")
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue
            obj = json.loads(line)
            mid = str(obj.get("memory_id", obj.get("id", "")))
            if not mid:
                continue
            freq = int(obj.get("freq", 0))
            freq_map[mid] = freq
    print(f"âœ… é¢‘æ¬¡è®°å½•æ•°: {len(freq_map)}")
    return freq_map


# =============== Embedding & ç›¸ä¼¼åº¦ ===============

def build_embeddings_for_memories(memories: Dict[str, dict]) -> Dict[str, np.ndarray]:
    """
    å¯¹æ‰€æœ‰è®°å¿†æ„å»ºå‘é‡ï¼Œç”¨äºç±»å†…ç›¸ä¼¼åº¦è®¡ç®—ã€‚
    é»˜è®¤ä¸ºä½¿ç”¨è®°å½•ä¸­çš„ "question" å­—æ®µï¼›å¦‚æœä½ æƒ³æ”¹æˆ "contents" å°±è‡ªå·±æ¢ä¸€ä¸‹ã€‚
    """
    device = "cuda" if has_cuda() else "cpu"
    print(f"ğŸš€ æ­£åœ¨è®¡ç®—è®°å¿†å‘é‡ ({EMBEDDING_MODEL}) on {device}...")
    model = SentenceTransformer(EMBEDDING_MODEL, device=device)

    ids = list(memories.keys())
    texts = []
    for mid in ids:
        rec = memories[mid]
        text = rec.get("question") or rec.get("contents", "")
        texts.append(text)

    embeddings = model.encode(
        texts,
        batch_size=32,
        show_progress_bar=True,
        normalize_embeddings=True
    )
    id_to_emb = {mid: embeddings[i] for i, mid in enumerate(ids)}
    print(f"âœ… å‘é‡æ„å»ºå®Œæˆï¼Œå…± {len(id_to_emb)} æ¡")
    return id_to_emb


def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
    return float(np.dot(a, b))


# =============== é«˜é¢‘/ä½é¢‘é›†åˆé€‰æ‹© ===============

def select_high_low_ids(
    freq_map: Dict[str, int],
    top_k_high: int,
    bottom_k_low: int,
    low_freq_for_low_only: int = 1
):
    """
    ä» freq_map ä¸­é€‰ï¼š
      - top_k_high ä¸ªæœ€é«˜é¢‘ä½œä¸ºé«˜é¢‘ anchor
      - bottom_k_low ä¸ªä½é¢‘å€™é€‰ï¼ˆä½†åªä¿ç•™ freq == low_freq_for_low_only çš„ï¼‰
      - åŒæ—¶è®°å½•æ‰€æœ‰ freq == 0 çš„ id æ–¹ä¾¿ä¹‹ååˆ é™¤
    """
    items = list(freq_map.items())
    # é«˜é¢‘ï¼šæŒ‰ freq é™åº
    sorted_desc = sorted(items, key=lambda x: -x[1])
    high_ids = [mid for mid, f in sorted_desc[:top_k_high]]

    # ä½é¢‘ï¼šæŒ‰ freq å‡åº
    sorted_asc = sorted(items, key=lambda x: x[1])
    low_ids = []
    zero_ids = []
    for mid, f in sorted_asc:
        if f == 0:
            zero_ids.append(mid)
            continue
        if f == low_freq_for_low_only:
            low_ids.append(mid)
        if len(low_ids) >= bottom_k_low:
            break

    print(f"ğŸ”¥ é«˜é¢‘ anchor æ•°é‡: {len(high_ids)}")
    print(f"ğŸ§Š 0 æ¬¡è°ƒç”¨çš„è®°å¿†æ•°é‡: {len(zero_ids)}ï¼ˆä¹‹åä¼šåˆ é™¤ï¼‰")
    print(f"ğŸ¥¶ ä½é¢‘æ‰©å†™å€™é€‰(freq={low_freq_for_low_only})æ•°é‡: {len(low_ids)} (æœ€å¤š bottom_k={bottom_k_low})")
    return set(high_ids), set(low_ids), set(zero_ids)


# =============== ä¸»ä¼˜åŒ–é€»è¾‘ ===============

def optimize_memory():
    # 0. åˆå§‹åŒ– LLM
    init_llm()

    # 1. è¯»å…¥åŸºç¡€æ•°æ®
    memories, id_order = load_clustered_memories(CLUSTERED_FILE)
    cluster_to_ids = load_cluster_summary(CLUSTER_SUMMARY_FILE)
    freq_map = load_memory_freq(MEM_FREQ_FILE)

    # ä¸ºæ‰€æœ‰è®°å¿†è¡¥é½é¢‘æ¬¡
    for mid in memories.keys():
        freq_map.setdefault(mid, 0)

    # 2. é€‰å‡ºé«˜é¢‘ã€ä½é¢‘ã€0 é¢‘é›†åˆ
    high_ids, low_ids, zero_ids = select_high_low_ids(
        freq_map,
        TOP_K_HIGH,
        BOTTOM_K_LOW,
        low_freq_for_low_only=LOW_FREQ_THRESHOLD
    )

    # 3. å‡†å¤‡å‘é‡ï¼Œç”¨äºç±»å†…ç›¸ä¼¼åº¦
    id_to_emb = build_embeddings_for_memories(memories)

    # 4. é«˜é¢‘ï¼šç±»å†…èšåˆï¼ˆmergeï¼‰
    merged_consumed_ids = set()      # è¢«å½“ä½œâ€œé‚»å±…â€å‚ä¸ merge çš„è®°å¿† id
    to_delete_ids = set()            # æœ€ç»ˆè¦å½»åº•åˆ é™¤çš„ idï¼ˆä½é¢‘è¢« merge / é¢‘æ¬¡ä¸º0 ç­‰ï¼‰

    print("\n========== é«˜é¢‘è®°å¿†èšåˆé˜¶æ®µ ==========")
    # æŒ‰é¢‘æ¬¡ä»é«˜åˆ°ä½é¡ºåºå¤„ç† anchorï¼Œé¿å… rank ä½çš„ anchor æŠ¢èµ°é«˜é¢‘é‚»å±…
    high_ids_sorted = sorted(list(high_ids), key=lambda x: -freq_map.get(x, 0))

    for anchor_id in high_ids_sorted:
        if anchor_id not in memories:
            continue
        if anchor_id in merged_consumed_ids:
            # è¯´æ˜å·²ç»ä½œä¸ºåˆ«äºº group çš„æˆå‘˜äº†ï¼Œå°±ä¸å†å½“ anchor
            continue

        rec_anchor = memories[anchor_id]
        cluster_id = rec_anchor.get("cluster_id")
        if cluster_id is None:
            continue

        cluster_id = int(cluster_id)
        cluster_member_ids = [str(x) for x in cluster_to_ids.get(cluster_id, [])]
        if not cluster_member_ids:
            continue

        # å€™é€‰é‚»å±…ï¼šåŒç±»ã€ä¸æ˜¯è‡ªå·±ã€æ²¡è¢« merge è¿‡
        candidates = [
            mid for mid in cluster_member_ids
            if mid != anchor_id and mid not in merged_consumed_ids
        ]
        if not candidates:
            continue

        anchor_emb = id_to_emb.get(anchor_id)
        if anchor_emb is None:
            continue

        sims = []
        for mid in candidates:
            emb = id_to_emb.get(mid)
            if emb is None:
                continue
            sims.append((mid, cosine_similarity(anchor_emb, emb)))

        if not sims:
            continue

        # å–ç±»å†… top-n ç›¸ä¼¼
        sims_sorted = sorted(sims, key=lambda x: -x[1])
        neighbors = [mid for mid, _ in sims_sorted[:TOP_N_SIMILAR_IN_CLUSTER]]
        group_ids = [anchor_id] + neighbors

        print(f"\nğŸ”¥ Anchor {anchor_id} (freq={freq_map[anchor_id]}, cluster={cluster_id})")
        print(f"   åˆå¹¶åŒç±» top-{len(neighbors)}: {neighbors}")

        # æ„é€ è¦ç»™ LLM çš„æ–‡æœ¬
        group_texts = []
        for mid in group_ids:
            rec = memories[mid]
            text = rec.get("question") or rec.get("contents", "")
            group_texts.append(f"[ID {mid}] {text}")

        summary_text = summarize_high_freq_memory(anchor_id, group_texts)
        if not summary_text:
            print("   âš ï¸ LLM è¿”å›ä¸ºç©ºï¼Œè·³è¿‡è¿™ç»„åˆå¹¶")
            continue

        # æ›´æ–° anchor çš„å†…å®¹ï¼šç”¨ summary æ›¿æ¢ questionï¼Œå¹¶ä¿ç•™åŸå§‹ä¿¡æ¯
        original_text = rec_anchor.get("question") or rec_anchor.get("contents", "")
        rec_anchor["original_question"] = original_text
        rec_anchor["question"] = summary_text
        rec_anchor["merged_from_ids"] = group_ids
        rec_anchor["merge_type"] = "high_freq_anchor"

        # é‚»å±…æ ‡è®°ä¸ºå·²å‚ä¸ mergeï¼›å…¶ä¸­ä½é¢‘çš„æ ‡è®°ä¸ºåˆ é™¤
        for mid in neighbors:
            merged_consumed_ids.add(mid)
            if freq_map.get(mid, 0) < LOW_FREQ_THRESHOLD:
                to_delete_ids.add(mid)

    # 5. ä½é¢‘ï¼šä¸è¢«åˆå¹¶æ¶ˆæ‰ã€freq=1 çš„è®°å¿†åšæ‰©å†™
    print("\n========== ä½é¢‘è®°å¿†æ‰©å†™é˜¶æ®µ ==========")
    # å…ˆæŠŠæ‰€æœ‰ freq=0 çš„ç›´æ¥åŠ å…¥åˆ é™¤é›†åˆ
    to_delete_ids.update(zero_ids)

    # çœŸæ­£è¦æ‰©å†™çš„ä½é¢‘è®°å¿†ï¼šfreq==LOW_FREQ_THRESHOLDï¼Œä¸”æ²¡æœ‰è¢« merge æ¶ˆè€—æ‰
    low_expand_ids = [
        mid for mid in low_ids
        if mid in memories and mid not in to_delete_ids
    ]
    print(f"ğŸ¥¶ éœ€è¦æ‰©å†™çš„ä½é¢‘è®°å¿†æ¡ç›®æ•°: {len(low_expand_ids)}")

    # ==== ä¿®æ”¹ï¼šè¿™ä¸€æ®µæ”¹æˆæ‰¹é‡è°ƒç”¨ LLMï¼Œè€Œä¸æ˜¯ä¸€æ¡ä¸€æ¡ ====
    low_expand_items = []
    for mid in low_expand_ids:
        rec = memories[mid]
        base_text = rec.get("question") or rec.get("contents", "")
        low_expand_items.append((mid, base_text))

    total_low = len(low_expand_items)
    for start in range(0, total_low, LLM_BATCH_SIZE):
        end = min(start + LLM_BATCH_SIZE, total_low)
        batch_items = low_expand_items[start:end]
        batch_ids = [mid for (mid, _) in batch_items]

        print(f"\nğŸ¥¶ æ‰©å†™ä½é¢‘è®°å¿† Batch {start // LLM_BATCH_SIZE + 1} / { (total_low + LLM_BATCH_SIZE - 1) // LLM_BATCH_SIZE }")
        print(f"   IDs: {batch_ids}")

        batch_prompts = [expand_low_freq_memory_prompt(base_text) for (_, base_text) in batch_items]
        batch_outputs = call_llm_batch(batch_prompts, max_new_tokens=MAX_NEW_TOKENS)

        for (mid, base_text), expanded in zip(batch_items, batch_outputs):
            if not expanded:
                print(f"   âš ï¸ LLM è¿”å›ä¸ºç©ºï¼ŒID={mid} ä¿æŒåŸæ–‡ä¸å˜")
                continue
            rec = memories[mid]
            rec["original_question"] = base_text
            rec["question"] = expanded
            rec["opt_type"] = "low_freq_expanded"

    # 6. å†™å‡ºæ–°çš„è®°å¿†åº“ï¼šè·³è¿‡ to_delete_ids
    print("\n========== å†™å‡ºä¼˜åŒ–åçš„è®°å¿†åº“ ==========")
    kept_count = 0
    with open(OUTPUT_OPTIMIZED_FILE, "w", encoding="utf-8") as f:
        for mid in id_order:
            if mid not in memories:
                continue
            if mid in to_delete_ids:
                continue
            f.write(json.dumps(memories[mid], ensure_ascii=False) + "\n")
            kept_count += 1

    print(f"âœ… æ–°è®°å¿†åº“å†™å…¥å®Œæˆ: {OUTPUT_OPTIMIZED_FILE}")
    print(f"   ä¿ç•™è®°å¿†æ¡ç›®: {kept_count}")
    print(f"   åˆ é™¤è®°å¿†æ¡ç›®: {len(to_delete_ids)}")
    print("   ï¼ˆæ³¨æ„ï¼šåŸå§‹ *_auto_clustered_result.jsonl æ–‡ä»¶æ²¡æœ‰è¢«ä¿®æ”¹ï¼‰")



if __name__ == "__main__":
    main()
    cluster()
    optimize_memory()
