import os
import json
import time
import torch
from huggingface_hub import snapshot_download
# Hydra & OmegaConf
import hydra
from omegaconf import DictConfig, OmegaConf
import random # [æ–°å¢] GPQA æ‰“ä¹±é€‰é¡¹éœ€è¦
# FlashRAG
from flashrag.config import Config
from flashrag.pipeline import SequentialPipeline
from flashrag.utils import get_retriever, get_generator, Dataset
from flashrag.prompt import PromptTemplate

# å±è”½ transformers çš„å†—ä½™è­¦å‘Š å’Œ httpx çš„ INFO æ—¥å¿— 
import transformers
transformers.logging.set_verbosity_error()

# ==========================================
# 1. ä¸€æ³¢å¼•ç”¨
# ==========================================
from utils.prepare_data import prepare_data
from utils.build_index import build_index
from utils.generator.gemini import GeminiGenerator
from utils.generator.sglang import SGLangGenerator
from tools.evaluate import judge_math_item,evaluate_results

# ==========================================
# 2. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° (Hydra é€‚é…)
# ==========================================
from tools.memoryscore import _load_memory_corpus,_calculate_scores,_print_stats_and_save,_visualize_results

def analyze_memory_usage(rag_results, cfg: DictConfig, corpus_file: str, vis_image_file: str):
    """
    è®°å¿†çƒ­åº¦/æ•ˆç”¨ç»Ÿè®¡ä¸å¯¼å‡º (å¼ºåŒ–å­¦ä¹ ç‰ˆ) - ä¸»å…¥å£
    é€»è¾‘ï¼š
    - æ£€ç´¢å‘½ä¸­ & é¢˜ç›®åšå¯¹: freq += 2 (å¥–åŠ±)
    - æ£€ç´¢å‘½ä¸­ & é¢˜ç›®åšé”™: freq -= 1 (æƒ©ç½š)
    """
    freq_file = cfg.paths.freq_file
    print("\nğŸ” [Analysis] æ­£åœ¨è¿›è¡Œå…¨é‡è®°å¿†æ•ˆç”¨è¯„åˆ† (RL Scoring)...")

    # 1. åŠ è½½æ•°æ®
    all_memory_ids, id_to_content = _load_memory_corpus(corpus_file)

    # 2. è®¡ç®—åˆ†æ•°
    memory_scores, correct_count = _calculate_scores(rag_results, all_memory_ids, cfg)

    # 3. æ‰“å°ç»Ÿè®¡å¹¶ä¿å­˜æ–‡ä»¶ (éœ€è¦è¿”å›æ’åºåçš„åˆ—è¡¨ä¾›å¯è§†åŒ–ä½¿ç”¨)
    sorted_memories = _print_stats_and_save(
        memory_scores, 
        id_to_content, 
        len(rag_results), 
        correct_count, 
        freq_file
    )

    # 4. å¯è§†åŒ–æˆ–æ‰“å° Top 10
    _visualize_results(cfg, sorted_memories, vis_image_file)
# ==========================================
# 4. ä¸»ç¨‹åº (Hydra Managed)
# ==========================================

@hydra.main(version_base=None, config_path="conf", config_name="config")
def main(cfg: DictConfig):
    
    # 0. åŸºç¡€è®¾ç½®ä¸è·¯å¾„æ„é€ 
    print("Visible GPU count:", torch.cuda.device_count())
    
    root_dir = cfg.paths.root
    
    # =================================================================
    # ğŸ”¥ [ä¿®æ”¹ç‚¹ 1] åˆ†åˆ«æå–â€œè®°å¿†åº“æ ‡ç­¾â€å’Œâ€œæµ‹è¯•é›†æ ‡ç­¾â€
    # =================================================================
    # ä¼˜å…ˆè¯»å– xxx_dataset_nameï¼Œå¦‚æœ yaml é‡Œæ²¡å†™ï¼Œå›é€€åˆ° dataset_name
    
    # 1. è®°å¿†åº“ Tag (ç”¨äºå‘½å corpus å’Œ index)
    corpus_name = cfg.experiment.get("corpus_dataset_name") or cfg.experiment.dataset_name
    corpus_tag = corpus_name.split('/')[-1] 
    
    # 2. æµ‹è¯•é›† Tag (ç”¨äºå‘½å test_data å’Œ log)
    test_name = cfg.experiment.get("test_dataset_name") or cfg.experiment.dataset_name
    test_tag = test_name.split('/')[-1]

    print(f"ğŸ·ï¸  Corpus Tag: {corpus_tag} | Test Tag: {test_tag}")

    # =================================================================
    # ğŸ”¥ [ä¿®æ”¹ç‚¹ 2] æ–‡ä»¶ååˆ†ç¦»
    # =================================================================
    
    # è®°å¿†åº“æ–‡ä»¶ & ç´¢å¼•ç›®å½• -> è·Ÿéš corpus_tag (æ¯”å¦‚ MATH)
    corpus_file = os.path.join(root_dir, f"{corpus_tag}_corpus.jsonl")
    index_dir = os.path.join(root_dir, f"{corpus_tag}_bm25_index")
    
    # æµ‹è¯•é›†æ•°æ®æ–‡ä»¶ -> è·Ÿéš test_tag (æ¯”å¦‚ hmmt)
    # è¿™æ ·ä½ å°±ä¸ä¼šæŠŠ MATH çš„æµ‹è¯•é›†è¦†ç›–æ‰äº†
    test_file = os.path.join(root_dir, f"{test_tag}_test_data.jsonl")
    
    # ç»“æœæ—¥å¿— -> æœ€å¥½åŒæ—¶ä½“ç° "ç”¨ä»€ä¹ˆåº“æµ‹ä»€ä¹ˆé¢˜"
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    # æ ¼å¼: HMMT_on_MATH_sglang_rag_2025...
    result_log_file = os.path.join(root_dir, f"{test_tag}_on_{corpus_tag}_{cfg.model.source}_{cfg.experiment.mode}_{timestamp}.txt")
    
    # å¯è§†åŒ–å›¾ç‰‡ -> è·Ÿéšæ—¥å¿—å
    vis_image_file = os.path.join(root_dir, f"{test_tag}_on_{corpus_tag}_dist_{timestamp}.png")

    if os.path.exists(result_log_file): os.remove(result_log_file)
    print(f"ğŸ“ ç»“æœå°†ä¿å­˜è‡³: {result_log_file}")
    print(f"ğŸ› ï¸ æ¨¡å¼: {cfg.experiment.mode} | æº: {cfg.model.source}")
    print(f"ğŸ“š Memory: {corpus_name} | ğŸ¯ Test: {test_name}")

    # 1. æ•°æ®å‡†å¤‡
    if not prepare_data(cfg, corpus_file, test_file): return
    
    # 2. ç´¢å¼•æ„å»º (å¦‚æœæ˜¯ rag æˆ– all æ¨¡å¼)
    if cfg.experiment.mode in ['rag', 'all']:
        build_index(corpus_file, index_dir)
    
    # 3. åˆå§‹åŒ– Generator
    generator = None
    config = None # FlashRAG config
    
    model_source = cfg.model.source
    
    if model_source == "gemini":
        print(f"ğŸ¤– [Init] åˆå§‹åŒ– Gemini: {cfg.model.gemini_name}...")
        api_key = os.environ.get("GEMINI_API_KEY") 
        generator = GeminiGenerator(cfg.model.gemini_name, api_key)
        
        # æ„é€  FlashRAG é…ç½®å­—å…¸
        gemini_config_dict = {
            "data_dir": root_dir,
            "save_dir": cfg.paths.rag_cache_dir,
            "device": "cpu",
            "retrieval_method": cfg.experiment.retrieval_method,
            "corpus_path": corpus_file,
            "index_path": index_dir,
            "retriever_model_path": index_dir,
            "generator_model": "huggingface", # å ä½
            "generator_model_path": "gpt2",   # å ä½
        }
        config = Config(config_dict=gemini_config_dict)

    elif model_source == "sglang":
        print(f"ğŸš€ [Init] åˆå§‹åŒ– SGLang Client...")
        
        # 1. ä» config è¯»å–
        sglang_base_url = cfg.model.get("sglang_api_url", "http://127.0.0.1:30000/v1")
        # âš ï¸ ç¡®ä¿è¿™é‡Œè¯»åˆ°çš„æ˜¯ "Qwen/Qwen3-4B-Instruct-2507"
        sglang_model_name = cfg.model.get("sglang_model_name", "Qwen/Qwen3-4B-Instruct-2507")
        
        # 2. æ„é€  FlashRAG é…ç½®å­—å…¸
        sglang_config_dict = {
            "data_dir": root_dir,
            "save_dir": cfg.paths.rag_cache_dir,
            "corpus_path": corpus_file,
            "index_path": index_dir,
            "retriever_model_path": index_dir,
            "retrieval_method": cfg.experiment.retrieval_method,
            
            # --- å…³é”®ä¿®æ”¹ ---
            "device": "cpu",
            "gpu_num": 0,
            "generator_model": "openai",               
            "generator_model_path": sglang_model_name, 
            "generation_method": "openai", 
            "batch_size": cfg.model.batch_size,
            "max_input_len": cfg.model.max_input_len,
            "max_new_tokens": cfg.model.max_new_tokens,
        }
        
        config = Config(config_dict=sglang_config_dict)

        # 4. åˆå§‹åŒ– Generator
        generator = SGLangGenerator(
            base_url=sglang_base_url,
            model_name=sglang_model_name,
            max_new_tokens=cfg.model.max_new_tokens,
            batch_size=cfg.model.batch_size,
            temperature=0.7, # å¦‚æœéœ€è¦ï¼Œè¿™ä¸ªä¹Ÿå¯ä»¥æåˆ° yaml é‡Œ
        )
        print(f"âœ… SGLang Generator ({sglang_model_name}) å·²è¿æ¥è‡³ {sglang_base_url}")

    elif model_source == "huggingface":
        hf_name = cfg.model.hf_name
        print(f"ğŸ“¥ [Init] æ£€æŸ¥/ä¸‹è½½ HF æ¨¡å‹: {hf_name}...")
        try:
            model_path = snapshot_download(repo_id=hf_name)
        except:
            print("âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥")
            return

        hf_config_dict = {
            "data_dir": root_dir,
            "save_dir": cfg.paths.rag_cache_dir,
            "device": cfg.model.device,
            "gpu_num": torch.cuda.device_count(),
            "generator_model": "huggingface",
            "generator_model_path": model_path,
            "generation_method": "huggingface",
            "batch_size": cfg.model.batch_size,
            "max_input_len": cfg.model.max_input_len,
            "max_new_tokens": cfg.model.max_new_tokens,
        }
        print("ğŸš€ åŠ è½½ HF ç”Ÿæˆå™¨...")
        config = Config(config_dict=hf_config_dict)
        generator = get_generator(config)
        
        # ğŸ”¥ Tokenizer ä¿®æ­£ (ä¿æŒä½ åŸæœ‰çš„ padding ä¿®å¤)
        if hasattr(generator, 'tokenizer'):
            generator.tokenizer.padding_side = 'left' 
            if generator.tokenizer.pad_token is None:
                generator.tokenizer.pad_token = generator.tokenizer.eos_token
                generator.tokenizer.pad_token_id = generator.tokenizer.eos_token_id
            generator.tokenizer.model_max_length = cfg.model.max_input_len
        
        if hasattr(generator, 'model'):
            if hasattr(generator.model.config, 'pad_token_id') and generator.model.config.pad_token_id is None:
                generator.model.config.pad_token_id = generator.tokenizer.pad_token_id
        print(f"âœ… Tokenizer ä¿®æ­£å®Œæˆ")
    
    else:
        print(f"âŒ æœªæ”¯æŒçš„ MODEL_SOURCE: {model_source}")
        return

    # è¯»å–æµ‹è¯•æ•°æ®
    with open(test_file, "r") as f:
        test_dataset_raw = [json.loads(line) for line in f]

    acc_baseline = 0
    acc_rag = 0

    # æ ¼å¼åŒ– Prompt è¾…åŠ©å‡½æ•°
    def format_base_prompt(system_text, user_text):
        if model_source == "gemini":
            return f"{system_text}\n\n{user_text}" if system_text else user_text
        prompt = ""
        if system_text: prompt += f"{system_text}\n\n"
        prompt += f"### Question:\n{user_text}\n\n### Answer:\nLet's think step by step."
        return prompt

    # --- Task A: Baseline ---
    if cfg.experiment.mode in ['baseline', 'all']:
        print("\nâš”ï¸ [Task A] æ­£åœ¨è¿è¡Œ Baseline ...")
        
        baseline_inputs = []
        for item in test_dataset_raw:
            sys_msg = cfg.experiment.prompts.sys_msg
            formatted_prompt = format_base_prompt(sys_msg, item['question'])
            baseline_inputs.append(formatted_prompt)

        baseline_preds = generator.generate(baseline_inputs)
        
        baseline_results = []
        for item, pred in zip(test_dataset_raw, baseline_preds):
            baseline_results.append({
                "question": item['question'],
                "golden_answers": item['golden_answers'],
                "pred": pred
            })
        acc_baseline = evaluate_results(baseline_results, "Baseline (No RAG)", result_log_file)

    # --- Task B: FlashRAG ---
    if cfg.experiment.mode in ['rag', 'all']:
        print("\nâš”ï¸ [Task B] æ­£åœ¨è¿è¡Œ FlashRAG (Few-shot Retrieval)...")
        
        # å‡†å¤‡ RAG é…ç½®
        rag_config_dict = OmegaConf.to_container(cfg, resolve=True) # ä»…ä»…æ˜¯ä¸ºäº†è·å–ä¸€äº›åŸºç¡€ç±»å‹
        # å°† FlashRAG éœ€è¦çš„ç‰¹å®šå­—æ®µè¦†ç›–è¿›å»
        rag_update = {
            "data_dir": root_dir,
            "save_dir": cfg.paths.rag_cache_dir,
            "retrieval_method": cfg.experiment.retrieval_method,
            "corpus_path": corpus_file,
            "index_path": index_dir,
            "retriever_model_path": index_dir,
            "topk": cfg.experiment.retrieval_topk,
            # Generator é…ç½®ç»§æ‰¿ä¹‹å‰çš„
            "device": cfg.model.device,
            "generator_model_path": config['generator_model_path'] if 'generator_model_path' in config else "gpt2"
        }
        
        # é‡æ–°å®ä¾‹åŒ– Config ä»¥ç¡®ä¿ Retriever èƒ½è¯»åˆ°æ­£ç¡®å‚æ•°
        rag_config = Config(config_dict=rag_update)
        retriever = get_retriever(rag_config)
        
        rag_system_part = cfg.experiment.prompts.rag_system
        
        prompt_tpl = PromptTemplate(rag_config, system_prompt=rag_system_part, user_prompt="")
        pipeline = SequentialPipeline(rag_config, prompt_template=prompt_tpl, retriever=retriever, generator=generator)
        dataset_obj = Dataset(rag_config, test_file)
        
        rag_results = pipeline.run(dataset_obj)
        
        acc_rag = evaluate_results(rag_results, f"FlashRAG ({corpus_tag} Memory)", result_log_file)
        
        # ç»Ÿè®¡è®°å¿†çƒ­åº¦ (ä¼ å…¥ cfg)
        analyze_memory_usage(rag_results, cfg, corpus_file, vis_image_file)

    # --- Summary ---
    if cfg.experiment.mode == 'all':
        summary = (
            f"\n{'='*20} æœ€ç»ˆå¯¹æ¯”ç»“æœ {'='*20}\n"
            f"ğŸ“Š æ•°æ®é›†: {cfg.experiment.test_dataset_name}\n"
            f"ğŸ¤– æ¨¡å‹: {model_source}\n"
            f"ğŸ“‰ Baseline: {acc_baseline:.2f}%\n"
            f"ğŸ“ˆ FlashRAG: {acc_rag:.2f}%\n"
            f"ğŸš€ æå‡: {acc_rag - acc_baseline:+.2f}%\n"
            f"{'='*50}\n"
        )
        print(summary)
        with open(result_log_file, "a", encoding="utf-8") as f:
            f.write(summary)

if __name__ == "__main__":
    main()