import os
import json
import re
import time
import torch
import bm25s
import logging
import ast
import collections
import matplotlib.pyplot as plt
from tqdm import tqdm
from datasets import load_dataset
from huggingface_hub import snapshot_download

# Hydra & OmegaConf
import hydra
from omegaconf import DictConfig, OmegaConf

# FlashRAG
from flashrag.config import Config
from flashrag.pipeline import SequentialPipeline
from flashrag.utils import get_retriever, get_generator, Dataset
from flashrag.prompt import PromptTemplate

# å±è”½ transformers çš„å†—ä½™è­¦å‘Š
import transformers
transformers.logging.set_verbosity_error()

# ==========================================
# 1. å·¥å…·ç±»ä¸ç”Ÿæˆå™¨ (ä¿æŒåŸæ ·é€»è¾‘)
# ==========================================

class GeminiGenerator:
    def __init__(self, model_name, api_key):
        import google.generativeai as genai
        if not api_key:
            raise ValueError("âŒ æœªæ£€æµ‹åˆ° API Keyï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ GEMINI_API_KEY")
        
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model_name)
        print(f"ğŸ¤– Gemini Generator ({model_name}) å·²åŠ è½½")
        self.max_input_len = 30000 

    def generate(self, input_list, **kwargs):
        responses = []
        for prompt in input_list:
            try:
                if isinstance(prompt, list): prompt = " ".join(prompt)
                clean_prompt = str(prompt)
                result = self.model.generate_content(clean_prompt)
                if result.parts:
                    responses.append(result.text)
                else:
                    responses.append("Error: Empty Response (Safety Block)")
                time.sleep(2) 
            except Exception as e:
                print(f"âš ï¸ Gemini API Error: {e}")
                time.sleep(5)
                responses.append("Error")
        return responses


from typing import List
from openai import OpenAI

class SGLangGenerator:
    """ä¸€ä¸ªæœ€å°å®ç°çš„ç”Ÿæˆå™¨ï¼Œé€‚é… FlashRAG çš„ generator.generate(prompts) æ¥å£ã€‚"""
    def __init__(
        self,
        base_url: str,
        model_name: str,
        max_new_tokens: int = 1024,
        batch_size: int = 8,
        temperature: float = 0.0,
    ):
        self.client = OpenAI(
            api_key=os.getenv("SGLANG_API_KEY", "EMPTY"),
            base_url=base_url.rstrip("/"),
        )
        self.model = model_name
        self.max_new_tokens = max_new_tokens
        self.batch_size = batch_size
        self.temperature = temperature
        self.max_input_len = 4096

    def generate(self, prompts: List[str]) -> List[str]:
        outputs: List[str] = []
        for i in range(0, len(prompts), self.batch_size):
            batch = prompts[i : i + self.batch_size]
            for p in batch:
                resp = self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": p}],
                    temperature=self.temperature,
                    max_tokens=self.max_new_tokens,
                )
                outputs.append(resp.choices[0].message.content)
        return outputs

# ==========================================
# 2. è¯„ä¼°å·¥å…· (Math Logic)
# ==========================================

def extract_math_answer(text):
    """
    (å‡çº§ç‰ˆ) ä»æ¨¡å‹è¾“å‡ºä¸­æå–ç­”æ¡ˆ
    é€»è¾‘ä¸ _local_extract ä¿æŒä¸€è‡´ï¼š
    1. ä¼˜å…ˆæ‰¾ \boxed{}
    2. å…œåº•æ‰¾æœ€åä¸€è¡Œ
    3. æ¸…æ´— '=' å’Œ '\approx' ä»¥åŠ LaTeX æ‚è´¨
    """
    if not text: return None
    text = str(text).strip()
    
    # 1. ä¼˜å…ˆæå– \boxed{} å†…å®¹
    idx = text.rfind("\\boxed{")
    if idx != -1:
        content_start = idx + 7 
        balance = 0
        for i in range(content_start, len(text)):
            if text[i] == '{': balance += 1
            elif text[i] == '}':
                if balance == 0: return text[content_start:i] 
                balance -= 1
    
    # 2. å…œåº•ç­–ç•¥ï¼šå–æœ€åä¸€è¡Œå¹¶æ¸…æ´—
    lines = text.strip().split('\n')
    if lines:
        last_line = lines[-1].strip()
        if last_line.endswith('.'): last_line = last_line[:-1]
        
        # æ¸…æ´— LaTeX ç¬¦å·
        last_line = last_line.replace('$', '').replace('`', '')
        
        # å»æ‰ "The Answer is" å‰ç¼€
        last_line = re.sub(r'^(The )?Answer( is)?:?', '', last_line, flags=re.IGNORECASE).strip()
        
        # å¤„ç†ç­‰å¼ (å–ç­‰å·å³è¾¹)
        if '=' in last_line: last_line = last_line.split('=')[-1].strip()
        
        # å¤„ç†è¿‘ä¼¼ç¬¦å·
        if '\\approx' in last_line: last_line = last_line.split('\\approx')[-1].strip()
        
        # é•¿åº¦æ”¾å®½åˆ° 100 (åŸç‰ˆæ˜¯ 50)
        if len(last_line) < 100: return last_line
        
    return None

def normalize_latex(s):
    """
    (å‡çº§ç‰ˆ) æ ‡å‡†åŒ– LaTeX å­—ç¬¦ä¸²
    é€»è¾‘ä¸ _local_norm ä¿æŒä¸€è‡´ï¼š
    1. ç§»é™¤ left/right/mathrm ç­‰ä¿®é¥°ç¬¦
    2. ç»Ÿä¸€åˆ†å·ã€ç™¾åˆ†å·
    3. å†æ¬¡å¤„ç†å¯èƒ½æ®‹ç•™çš„ '=' æˆ– '\in'
    """
    if not s: return ""
    # åŸºç¡€æ¸…æ´—
    s = str(s).replace('$', '').replace('`', '').replace('\\%', '%')
    s = s.replace("\\dfrac", "\\frac").replace("\\text", "")
    
    # ç§»é™¤ä¿®é¥°ç¬¦ (è¿™æ˜¯å…³é”®å·®å¼‚ï¼Œé˜²æ­¢ \left( \right) å¯¼è‡´è¯¯åˆ¤)
    s = s.replace("\\left", "").replace("\\right", "").replace("\\mathrm", "")
    
    # å»é™¤ç©ºç™½
    s = "".join(s.split())
    
    # å†æ¬¡ç¡®ä¿å–ç­‰å·å³è¾¹ (åŒé‡ä¿é™©)
    if '=' in s: s = s.split('=')[-1]
    if '\\in' in s: s = s.split('\\in')[-1]
    
    return s.rstrip('.').strip()

def _get_field(item, attr: str, key: str = None, default=None):
    """å…¼å®¹ FlashRAG Dataset å¯¹è±¡ & dict"""
    if hasattr(item, attr):
        return getattr(item, attr)
    if isinstance(item, dict):
        return item.get(key or attr, default)
    return default

def judge_math_item(item):
    """
    è¿”å›: (is_right, gold_val, pred_val)
    gold_val/pred_val æ˜¯â€œæå–åçš„åŸå§‹ç­”æ¡ˆâ€ï¼ˆæœª normï¼‰ï¼Œæ–¹ä¾¿æ—¥å¿—/è°ƒè¯•ã€‚
    """
    pred = _get_field(item, "pred")
    golden_answers = _get_field(item, "golden_answers")
    gold_raw = golden_answers[0] if isinstance(golden_answers, (list, tuple)) and golden_answers else golden_answers

    gold_val = extract_math_answer(gold_raw)
    if gold_val is None:
        gold_val = str(gold_raw).strip() if gold_raw is not None else None

    pred_val = extract_math_answer(pred)

    if not gold_val or not pred_val:
        return False, gold_val, pred_val

    is_right = normalize_latex(gold_val) == normalize_latex(pred_val)
    return is_right, gold_val, pred_val

def evaluate_results(results, experiment_name, result_log_file):
    correct = 0
    total = len(results)
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(result_log_file), exist_ok=True)

    with open(result_log_file, "a", encoding="utf-8") as f:
        header = f"\n{'='*20} {experiment_name} {'='*20}\n"
        print(header.strip())
        f.write(header)
        
        for i, item in enumerate(results):
            # å…¼å®¹ FlashRAG Dataset å¯¹è±¡å’Œ dict
            pred = item.pred if hasattr(item, 'pred') else item['pred']
            gold_raw = item.golden_answers[0] if hasattr(item, 'golden_answers') else item['golden_answers'][0]
            question = item.question if hasattr(item, 'question') else item['question']

            # ä½¿ç”¨å‡çº§åçš„æå–é€»è¾‘
            is_right, gold_val, pred_val = judge_math_item(item)

            if is_right: correct += 1

            log_entry = (
                f"\n[ID]: {i}\n"
                f"[Question]: {str(question)[:100]}...\n"
                f"[Gold Raw]: ... => [Extracted]: {gold_val}\n"
                f"[Pred Raw]: ...{str(pred)[-50:].replace(chr(10), ' ')} => [Extracted]: {pred_val}\n"
                f"[Result]: {'âœ… Correct' if is_right else 'âŒ Wrong'}\n"
                f"{'-'*30}\n"
            )
            f.write(log_entry)
            if i < 5: print(log_entry.strip()) # å‡å°‘ä¸€ç‚¹æ§åˆ¶å°è¾“å‡º

        acc = correct / total * 100
        summary = (
            f"\nğŸ“Š ç»Ÿè®¡ ({experiment_name}):\n"
            f"Total: {total}, Correct: {correct}, Accuracy: {acc:.2f}%\n"
            f"{'='*50}\n"
        )
        print(summary)
        f.write(summary)
    return acc

# ==========================================
# 3. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° (Hydra é€‚é…)
# ==========================================

def prepare_data(cfg: DictConfig, corpus_file: str, test_file: str):
    """
    å‡†å¤‡æ•°æ®ï¼š
    1. æ£€æŸ¥ corpus_file (ä¼˜åŒ–åçš„è®°å¿†åº“) æ˜¯å¦å­˜åœ¨ (å¿…é¡»å­˜åœ¨ï¼)
    2. ç”Ÿæˆ test.jsonl (æµ‹è¯•é›†)
    """
    # ğŸ”¥ ä¿®æ”¹ 1: ä¸¥æ ¼æ£€æŸ¥è®°å¿†åº“æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™æŠ¥é”™ï¼Œç»ä¸è‡ªå·±ç”Ÿæˆ
    if not os.path.exists(corpus_file):
        print(f"âŒ ä¸¥é‡é”™è¯¯: æ‰¾ä¸åˆ°ä¼˜åŒ–åçš„è®°å¿†åº“æ–‡ä»¶: {corpus_file}")
        print("   è¯·å…ˆè¿è¡Œ optimizer.py ç”Ÿæˆè¯¥æ–‡ä»¶ï¼")
        return False
    else:
        print(f"âœ… [Memory] æ£€æµ‹åˆ°ä¼˜åŒ–åçš„è®°å¿†åº“: {corpus_file}")

    dataset_name = cfg.experiment.dataset_name
    dataset_config = cfg.experiment.dataset_config
    
    print(f"ğŸ“¥ [Test] æ­£åœ¨åŠ è½½æ•°æ®é›†ä»¥æ„å»ºæµ‹è¯•é›†: {dataset_name}...")
    try:
        if dataset_config:
            dataset = load_dataset(dataset_name, dataset_config)
        else:
            dataset = load_dataset(dataset_name)
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return False

    q_col = cfg.experiment.field_map.question
    a_col = cfg.experiment.field_map.answer
    split_test = "test"
    
    print(f"ğŸ”¨ [Test] æ­£åœ¨å¤„ç†æµ‹è¯•é›†...")
    with open(test_file, "w", encoding="utf-8") as f:
        # ... (è¿™é‡Œä¿ç•™åŸæœ‰çš„æµ‹è¯•é›†åˆ‡ç‰‡é€»è¾‘ï¼Œä¸ç”¨åŠ¨) ...
        # (å³åŸä»£ç ä¸­ "if split_test not in dataset:" å¼€å§‹åˆ° "f.write..." ç»“æŸçš„éƒ¨åˆ†)
        # ä¸ºäº†èŠ‚çœç¯‡å¹…ï¼Œè¿™é‡Œç•¥è¿‡ä¸­é—´æœªä¿®æ”¹çš„åˆ‡ç‰‡é€»è¾‘ï¼Œè¯·ä¿ç•™ä½ åŸä»£ç ä¸­ 202-208 è¡Œçš„é€»è¾‘
        if split_test not in dataset:
             print(f"âŒ é”™è¯¯: æ•°æ®é›†æ²¡æœ‰ {split_test} åˆ’åˆ†ï¼")
             return False
             
        test_data = dataset[split_test]
        start_idx = int(cfg.experiment.get("start_index", 0) or 0)
        debug_num = cfg.experiment.get("debug_num")
        total_len = len(test_data)
        print(f'\n debug_numæ˜¯{debug_num}\n')
        if debug_num:
            limit = int(debug_num)
            end_idx = min(start_idx + limit, total_len)
            print(f'\n debug_numæ˜¯{debug_num}\n')
            print(f'\n end_idxæ˜¯{end_idx}\n')
        else:
            end_idx = total_len

        if start_idx >= total_len:
            test_data = test_data.select([])
        else:
            indices = range(start_idx, end_idx)
            test_data = test_data.select(indices)

        for i, item in enumerate(test_data):
            q_text = item.get(q_col, "")
            raw_ans = item.get(a_col, "")
            real_id = start_idx + i
            f.write(json.dumps({
                "id": str(real_id),
                "question": q_text,
                "golden_answers": [str(raw_ans)]
            }) + "\n")
            
    return True

def build_index(corpus_file: str, index_dir: str):
    """æ„å»º BM25 ç´¢å¼•"""

    print(f"ğŸ”¨ [Index] æ­£åœ¨ä¸º {corpus_file} æ„å»º BM25 ç´¢å¼•...")
    corpus_texts = []
    # ä½¿ç”¨ bm25s åº“
    with open(corpus_file, "r", encoding="utf-8") as f:
        for line in f:
            corpus_texts.append(json.loads(line)['contents'])
    
    corpus_tokens = bm25s.tokenize(corpus_texts)
    retriever_builder = bm25s.BM25()
    retriever_builder.index(corpus_tokens)
    retriever_builder.save(index_dir)
    
    # FlashRAG è¦æ±‚çš„é¢å¤–æ–‡ä»¶
    with open(os.path.join(index_dir, "stopwords.tokenizer.json"), "w") as f:
        json.dump([], f)
    with open(os.path.join(index_dir, "vocab.tokenizer.json"), "w") as f:
        vocab = corpus_tokens.vocab
        # å…¼å®¹æ€§å¤„ç†
        json.dump({"word_to_id": vocab, "stem_to_sid": vocab, "word_to_stem": {k: k for k in vocab}}, f)
    print("âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼")

def analyze_memory_usage(rag_results, cfg: DictConfig, corpus_file: str, vis_image_file: str):
    """
    è®°å¿†çƒ­åº¦/æ•ˆç”¨ç»Ÿè®¡ä¸å¯¼å‡º (å¼ºåŒ–å­¦ä¹ ç‰ˆ)
    é€»è¾‘ï¼š
    - æ£€ç´¢å‘½ä¸­ & é¢˜ç›®åšå¯¹: freq += 2 (å¥–åŠ±)
    - æ£€ç´¢å‘½ä¸­ & é¢˜ç›®åšé”™: freq -= 2 (æƒ©ç½š)
    """
    # è¿™é‡Œçš„ freq_file ä» config ä¸­è¯»å–
    freq_file = cfg.paths.eval_freq_file
    
    print("\nğŸ” [Analysis] æ­£åœ¨è¿›è¡Œå…¨é‡è®°å¿†æ•ˆç”¨è¯„åˆ† (RL Scoring)...")
    
    all_memory_ids = set()
    id_to_content = {} 
    print(corpus_file)
    try:
        with open(corpus_file, "r", encoding="utf-8") as f:
            for line in f:
                item = json.loads(line)
                mid = str(item['id'])
                all_memory_ids.add(mid)
                id_to_content[mid] = item.get("contents", "")
    
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è¯»å–è®°å¿†åº“æ–‡ä»¶ {corpus_file}ï¼Œé”™è¯¯: {e}")
    # # ======== ã€jychenã€‘ æŸ¥çœ‹è®°å¿†åº“é‡Œçš„ç¬¬ä¸€æ¡ ========
    # if id_to_content:
    #     first_mid = next(iter(id_to_content))
    #     print(f"\nğŸ‘€ [DEBUG] è®°å¿†åº“é¦–æ¡å†…å®¹æ£€æŸ¥ (ID: {first_mid}):")
    #     print(f"{id_to_content[first_mid]}")
    #     print("-" * 50)
    # # ==================================================

    # åˆå§‹åŒ–åˆ†æ•°ï¼Œé»˜è®¤ 0
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ç”¨ score ä»£æ›¿åŸæ¥çš„å•çº¯è®¡æ•°ï¼Œä½†å˜é‡ååœ¨è¾“å‡ºæ—¶ä¾ç„¶å« freq ä»¥å…¼å®¹åç»­è„šæœ¬
    memory_scores = {mid: 0 for mid in all_memory_ids}
    
    # ç»Ÿè®¡å‘½ä¸­å¹¶æ‰“åˆ†
    total_questions = len(rag_results)
    correct_count = 0

    for item in tqdm(rag_results, desc="Scoring Memories"):
        is_correct, _, _ = judge_math_item(item)
        if is_correct:
            correct_count += 1

        scoreget = cfg.experiment.reward
        scoreloss = cfg.experiment.punishment
        reward = scoreget if is_correct else scoreloss

        retrieved_docs = getattr(item, 'retrieval_result', [])
        # # ======== ã€jychenã€‘ æŸ¥çœ‹å½“å‰é¢˜ç›®æ£€ç´¢åˆ°çš„ç¬¬ä¸€æ¡ ========
        # if retrieved_docs:
        #     first_doc = retrieved_docs[0]
        #     # å…¼å®¹å­—å…¸æˆ–å¯¹è±¡è¯»å– ID
        #     f_id = str(first_doc.get('id')) if isinstance(first_doc, dict) else str(getattr(first_doc, 'id', ''))
            
        #     print(f"\nğŸ‘€ [DEBUG] å½“å‰é¢˜ç›®æ£€ç´¢åˆ°çš„ Top1 è®°å¿† (ID: {f_id}):")
        #     # ä»ä¹‹å‰åŠ è½½çš„ id_to_content ä¸­æŸ¥å†…å®¹
        #     content = id_to_content.get(f_id, "âš ï¸ å†…å®¹æœªåœ¨ corpus æ–‡ä»¶ä¸­æ‰¾åˆ°")
        #     print(content)
        #     print("-" * 50)
        # # =======================================================
        for doc in retrieved_docs:
            doc_id = str(doc.get('id')) if isinstance(doc, dict) else str(getattr(doc, 'id', None))
            if doc_id and doc_id in memory_scores:
                memory_scores[doc_id] += reward

    # æ’åº (æŒ‰åˆ†æ•°ä»é«˜åˆ°ä½)
    sorted_memories = sorted(memory_scores.items(), key=lambda x: (-x[1], x[0]))
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_mem = len(sorted_memories)
    positive_mem = sum(1 for _, v in sorted_memories if v > 0)
    negative_mem = sum(1 for _, v in sorted_memories if v < 0)
    zero_mem = sum(1 for _, v in sorted_memories if v == 0)
    
    print(f"ğŸ“Š è®°å¿†åº“è¯„åˆ†ç»Ÿè®¡:")
    print(f"   - æ€»é‡: {total_mem}")
    print(f"   - æ­£åˆ†(è´¡çŒ®è€…): {positive_mem} ({(positive_mem/total_mem)*100:.1f}%)")
    print(f"   - è´Ÿåˆ†(å¹²æ‰°é¡¹): {negative_mem} ({(negative_mem/total_mem)*100:.1f}%)")
    print(f"   - é›¶åˆ†(å†·é—¨): {zero_mem}")
    print(f"   - å½“å‰é¢˜ç›®æ­£ç¡®ç‡: {correct_count/total_questions*100:.2f}%")

    # å¯¼å‡º jsonl (ä¿æŒ freq å­—æ®µåï¼Œä½†å­˜çš„æ˜¯åˆ†æ•°)
    try:
        print(f"ğŸ’¾ [Save] æ­£åœ¨å¯¼å‡ºè®°å¿†è¯„åˆ†ç»“æœåˆ°: {freq_file}")
        os.makedirs(os.path.dirname(freq_file), exist_ok=True)
        
        with open(freq_file, "w", encoding="utf-8") as f:
            for rank, (mid, score) in enumerate(sorted_memories, start=1):
                record = {
                    "rank": rank,
                    "memory_id": mid,
                    "freq": int(score), # ğŸ”¥ è¿™é‡Œå­˜çš„æ˜¯åˆ†æ•° (-2, 0, 2, 4...)
                    "contents": id_to_content.get(mid, "")
                }
                # print(record["contents"]) #jychen
                f.write(json.dumps(record, ensure_ascii=False) + "\n")
        print("âœ… è¯„åˆ†æ–‡ä»¶å¯¼å‡ºå®Œæˆï¼")
    except Exception as e:
        print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")

    # å¯è§†åŒ– (åˆ†æ•°åˆ†å¸ƒå›¾)
    if cfg.experiment.visualize_memory:
        print(f"ğŸ¨ [Visual] æ­£åœ¨ç”Ÿæˆåˆ†æ•°åˆ†å¸ƒå›¾: {vis_image_file}")
        try:
            ids = [m[0] for m in sorted_memories]
            scores = [m[1] for m in sorted_memories]
            
            display_limit = 30
            if len(ids) > display_limit * 2:
                plot_ids = ids[:display_limit] + ["..."] + ids[-display_limit:]
                plot_scores = scores[:display_limit] + [0] + scores[-display_limit:]
                # é¢œè‰²åŒºåˆ†ï¼šæ­£åˆ†è“ï¼Œè´Ÿåˆ†çº¢ï¼Œé›¶åˆ†ç™½
                colors = []
                for s in plot_scores:
                    if s > 0: colors.append('skyblue')
                    elif s < 0: colors.append('salmon')
                    else: colors.append('lightgrey')
            else:
                plot_ids = ids
                plot_scores = scores
                colors = ['skyblue' if s > 0 else 'salmon' if s < 0 else 'lightgrey' for s in plot_scores]

            plt.figure(figsize=(15, 6))
            # ç”»ä¸€æ¡ 0 åˆ†çº¿
            plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
            
            bars = plt.bar(plot_ids, plot_scores, color=colors, edgecolor='navy')
            plt.title(f'Memory Utility Score (Correct=+2, Wrong=-2)', fontsize=14)
            plt.ylabel('Score')
            plt.xticks(rotation=90, fontsize=8) 
            
            # æ˜¾ç¤ºæ•°å€¼
            for i, bar in enumerate(bars):
                height = bar.get_height()
                if plot_ids[i] != "...": 
                    # æ­£åˆ†æ˜¾ç¤ºåœ¨æ¡ä¸Šæ–¹ï¼Œè´Ÿåˆ†æ˜¾ç¤ºåœ¨æ¡ä¸‹æ–¹
                    y_pos = height if height >= 0 else height - (max(scores)*0.05)
                    va = 'bottom' if height >= 0 else 'top'
                    plt.text(bar.get_x() + bar.get_width()/2., y_pos, f'{int(height)}',
                             ha='center', va=va, fontsize=8)
            
            plt.tight_layout()
            plt.savefig(vis_image_file, dpi=300)
            print("âœ… å›¾ç‰‡ä¿å­˜æˆåŠŸï¼")
        except ImportError:
            print("âŒ ç¼ºå°‘ matplotlib")
    else:
        print("\nğŸ† [Top 10 High-Utility Memories]")
        for mid, score in sorted_memories[:10]:
            print(f"   ID: {mid:<5} | Score: {score}")


# ==========================================
# 4. ä¸»ç¨‹åº (Hydra Managed)
# ==========================================

@hydra.main(version_base=None, config_path="conf", config_name="config")
def main(cfg: DictConfig):
    
    # 0. åŸºç¡€è®¾ç½®ä¸è·¯å¾„æ„é€ 
    print("Visible GPU count:", torch.cuda.device_count())
    
    # æ„é€ æ–‡ä»¶è·¯å¾„ (å…¨éƒ¨åŸºäº cfg.paths.root)
    root_dir = cfg.paths.root
    dataset_tag = cfg.experiment.dataset_name.split('/')[-1]
    
    # å®šä¹‰ä¸­é—´æ–‡ä»¶è·¯å¾„
    # ğŸ”¥ ä¿®æ”¹è¿™é‡Œï¼šæ–‡ä»¶ååŠ ä¸Š _optimizedï¼Œä¸ eval.py é€»è¾‘ä¿æŒä¸€è‡´
    corpus_file = cfg.paths.optimized_memory
    test_file = os.path.join(root_dir, f"{dataset_tag}_test_data.jsonl")
    index_dir = os.path.join(root_dir, f"{dataset_tag}_optimized_bm25_index")
    
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    result_log_file = os.path.join(root_dir, f"eval_{dataset_tag}_{cfg.model.source}_{cfg.experiment.mode}_{timestamp}.txt")
    vis_image_file = os.path.join(root_dir, f"memory_distribution_{timestamp}.png")

    if os.path.exists(result_log_file): os.remove(result_log_file)
    print(f"ğŸ“ ç»“æœå°†ä¿å­˜è‡³: {result_log_file}")
    print(f"ğŸ› ï¸ æ¨¡å¼: {cfg.experiment.mode} | æº: {cfg.model.source} | æ•°æ®é›†: {cfg.experiment.dataset_name}")

    # 1. æ•°æ®å‡†å¤‡
    if not prepare_data(cfg, corpus_file, test_file): return
    
    # 2. ç´¢å¼•æ„å»º (å¦‚æœæ˜¯ rag æˆ– all æ¨¡å¼)
    if cfg.experiment.mode in ['rag', 'all']:
        build_index(corpus_file, index_dir)
    
    # 3. åˆå§‹åŒ– Generator
    generator = None
    config = None # FlashRAG config
    
    model_source = cfg.model.source
    
    if model_source == "gemini":
        print(f"ğŸ¤– [Init] åˆå§‹åŒ– Gemini: {cfg.model.gemini_name}...")
        api_key = os.environ.get("GEMINI_API_KEY") 
        generator = GeminiGenerator(cfg.model.gemini_name, api_key)
        
        # æ„é€  FlashRAG é…ç½®å­—å…¸
        gemini_config_dict = {
            "data_dir": root_dir,
            "save_dir": cfg.paths.rag_cache_dir,
            "device": "cpu",
            "retrieval_method": cfg.experiment.retrieval_method,
            "corpus_path": corpus_file,
            "index_path": index_dir,
            "retriever_model_path": index_dir,
            "generator_model": "huggingface", # å ä½
            "generator_model_path": "gpt2",   # å ä½
        }
        config = Config(config_dict=gemini_config_dict)

    elif model_source == "sglang":
        print(f"ğŸš€ [Init] åˆå§‹åŒ– SGLang Client...")
        
        # 1. ä» config è¯»å–
        sglang_base_url = cfg.model.get("sglang_api_url", "http://127.0.0.1:30000/v1")
        # âš ï¸ ç¡®ä¿è¿™é‡Œè¯»åˆ°çš„æ˜¯ "Qwen/Qwen3-4B-Instruct-2507"
        sglang_model_name = cfg.model.get("sglang_model_name", "Qwen/Qwen3-4B-Instruct-2507")
        
        # 2. æ„é€  FlashRAG é…ç½®å­—å…¸
        sglang_config_dict = {
            "data_dir": root_dir,
            "save_dir": cfg.paths.rag_cache_dir,
            "corpus_path": corpus_file,
            "index_path": index_dir,
            "retriever_model_path": index_dir,
            "retrieval_method": cfg.experiment.retrieval_method,
            
            # --- å…³é”®ä¿®æ”¹ ---
            "device": "cpu",
            "gpu_num": 0,
            
            # 1. å‘Šè¯‰ FlashRAG æˆ‘ä»¬åœ¨ç”¨ç±»ä¼¼ OpenAI çš„ç”Ÿæˆåè®® (è™½ç„¶æˆ‘ä»¬å®é™…ä¸Šæ˜¯ç”¨è‡ªå®šä¹‰ Generator è¦†ç›–äº†å®ƒ)
            "generator_model": "openai",   
            
            # 2. ğŸ”¥ğŸ”¥ğŸ”¥ æ ¸å¿ƒä¿®å¤åœ¨è¿™é‡Œ ğŸ”¥ğŸ”¥ğŸ”¥
            # ä¸è¦è®©å®ƒå»åŠ è½½ "openai" çš„ configï¼Œè€Œæ˜¯å»åŠ è½½ Qwen çš„ configï¼
            # PromptTemplate éœ€è¦è¿™ä¸ªè·¯å¾„æ¥ä¸‹è½½ tokenizer config
            "generator_model_path": sglang_model_name, 
            
            "generation_method": "openai", 
            "batch_size": cfg.model.batch_size,
            "max_input_len": cfg.model.max_input_len,
            "max_new_tokens": cfg.model.max_new_tokens,
        }
        
        config = Config(config_dict=sglang_config_dict)

        # 4. åˆå§‹åŒ– Generator
        generator = SGLangGenerator(
            base_url=sglang_base_url,
            model_name=sglang_model_name,
            max_new_tokens=cfg.model.max_new_tokens,
            batch_size=cfg.model.batch_size,
            temperature=0.7, # å¦‚æœéœ€è¦ï¼Œè¿™ä¸ªä¹Ÿå¯ä»¥æåˆ° yaml é‡Œ
        )
        print(f"âœ… SGLang Generator ({sglang_model_name}) å·²è¿æ¥è‡³ {sglang_base_url}")

    elif model_source == "huggingface":
        hf_name = cfg.model.hf_name
        print(f"ğŸ“¥ [Init] æ£€æŸ¥/ä¸‹è½½ HF æ¨¡å‹: {hf_name}...")
        try:
            model_path = snapshot_download(repo_id=hf_name)
        except:
            print("âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥")
            return

        hf_config_dict = {
            "data_dir": root_dir,
            "save_dir": cfg.paths.rag_cache_dir,
            "device": cfg.model.device,
            "gpu_num": torch.cuda.device_count(),
            "generator_model": "huggingface",
            "generator_model_path": model_path,
            "generation_method": "huggingface",
            "batch_size": cfg.model.batch_size,
            "max_input_len": cfg.model.max_input_len,
            "max_new_tokens": cfg.model.max_new_tokens,
        }
        print("ğŸš€ åŠ è½½ HF ç”Ÿæˆå™¨...")
        config = Config(config_dict=hf_config_dict)
        generator = get_generator(config)
        
        # ğŸ”¥ Tokenizer ä¿®æ­£ (ä¿æŒä½ åŸæœ‰çš„ padding ä¿®å¤)
        if hasattr(generator, 'tokenizer'):
            generator.tokenizer.padding_side = 'left' 
            if generator.tokenizer.pad_token is None:
                generator.tokenizer.pad_token = generator.tokenizer.eos_token
                generator.tokenizer.pad_token_id = generator.tokenizer.eos_token_id
            generator.tokenizer.model_max_length = cfg.model.max_input_len
        
        if hasattr(generator, 'model'):
            if hasattr(generator.model.config, 'pad_token_id') and generator.model.config.pad_token_id is None:
                generator.model.config.pad_token_id = generator.tokenizer.pad_token_id
        print(f"âœ… Tokenizer ä¿®æ­£å®Œæˆ")
    
    else:
        print(f"âŒ æœªæ”¯æŒçš„ MODEL_SOURCE: {model_source}")
        return

    # è¯»å–æµ‹è¯•æ•°æ®
    with open(test_file, "r") as f:
        test_dataset_raw = [json.loads(line) for line in f]

    acc_baseline = 0
    acc_rag = 0

    # æ ¼å¼åŒ– Prompt è¾…åŠ©å‡½æ•°
    def format_base_prompt(system_text, user_text):
        if model_source == "gemini":
            return f"{system_text}\n\n{user_text}" if system_text else user_text
        prompt = ""
        if system_text: prompt += f"{system_text}\n\n"
        prompt += f"### Question:\n{user_text}\n\n### Answer:\nLet's think step by step."
        return prompt

    # --- Task A: Baseline ---
    if cfg.experiment.mode in ['baseline']:
        print("\nâš”ï¸ [Task A] æ­£åœ¨è¿è¡Œ Baseline ...")
        
        baseline_inputs = []
        for item in test_dataset_raw:
            sys_msg = "You are a math expert. Solve the problem in a brief. Don't answer more than 50 words.End your answer with \\boxed{number}."
            formatted_prompt = format_base_prompt(sys_msg, item['question'])
            baseline_inputs.append(formatted_prompt)

        baseline_preds = generator.generate(baseline_inputs)
        
        baseline_results = []
        for item, pred in zip(test_dataset_raw, baseline_preds):
            baseline_results.append({
                "question": item['question'],
                "golden_answers": item['golden_answers'],
                "pred": pred
            })
        acc_baseline = evaluate_results(baseline_results, "Baseline (No RAG)", result_log_file)

    # --- Task B: FlashRAG ---
    if cfg.experiment.mode in ['rag', 'all']:
        print("\nâš”ï¸ [Task B] æ­£åœ¨è¿è¡Œ FlashRAG (Few-shot Retrieval)...")
        
        # å‡†å¤‡ RAG é…ç½®
        rag_config_dict = OmegaConf.to_container(cfg, resolve=True) # ä»…ä»…æ˜¯ä¸ºäº†è·å–ä¸€äº›åŸºç¡€ç±»å‹
        # å°† FlashRAG éœ€è¦çš„ç‰¹å®šå­—æ®µè¦†ç›–è¿›å»
        rag_update = {
            "data_dir": root_dir,
            "save_dir": cfg.paths.rag_cache_dir,
            "retrieval_method": cfg.experiment.retrieval_method,
            "corpus_path": corpus_file,
            "index_path": index_dir,
            "retriever_model_path": index_dir,
            "topk": cfg.experiment.retrieval_topk,
            # Generator é…ç½®ç»§æ‰¿ä¹‹å‰çš„
            "device": cfg.model.device,
            "generator_model_path": config['generator_model_path'] if 'generator_model_path' in config else "gpt2"
        }
        
        # é‡æ–°å®ä¾‹åŒ– Config ä»¥ç¡®ä¿ Retriever èƒ½è¯»åˆ°æ­£ç¡®å‚æ•°
        rag_config = Config(config_dict=rag_update)
        retriever = get_retriever(rag_config)
        
        rag_system_part = cfg.experiment.prompts.rag_system
        
        prompt_tpl = PromptTemplate(rag_config, system_prompt=rag_system_part, user_prompt="")
        pipeline = SequentialPipeline(rag_config, prompt_template=prompt_tpl, retriever=retriever, generator=generator)
        dataset_obj = Dataset(rag_config, test_file)
        
        rag_results = pipeline.run(dataset_obj)
        
        acc_rag = evaluate_results(rag_results, f"FlashRAG ({dataset_tag} Memory)", result_log_file)
        
        # ç»Ÿè®¡è®°å¿†çƒ­åº¦ (ä¼ å…¥ cfg)
        analyze_memory_usage(rag_results, cfg, corpus_file, vis_image_file)

    # --- Summary ---
    if cfg.experiment.mode == 'all':
        summary = (
            f"\n{'='*20} æœ€ç»ˆå¯¹æ¯”ç»“æœ {'='*20}\n"
            f"ğŸ“Š æ•°æ®é›†: {cfg.experiment.dataset_name}\n"
            f"ğŸ¤– æ¨¡å‹: {model_source}\n"
            f"ğŸ“‰ Baseline: {acc_baseline:.2f}%\n"
            f"ğŸ“ˆ FlashRAG: {acc_rag:.2f}%\n"
            f"ğŸš€ æå‡: {acc_rag - acc_baseline:+.2f}%\n"
            f"{'='*50}\n"
        )
        print(summary)
        with open(result_log_file, "a", encoding="utf-8") as f:
            f.write(summary)

if __name__ == "__main__":
    main()