import os
import re
import uuid
from typing import Set, List, Dict, Optional,Tuple
from omegaconf import DictConfig

# Tooling
from tools.optimize.callllm import call_llm_batch
from tools.optimize.callexpert import call_expert_batch
from utils.memorywrap import parse_memory

_MEMORY_START = r"\\memory{"

def _find_memory_spans(raw_output: str) -> List[Tuple[int, int, str]]:
    """Return list of (start_idx, end_idx_exclusive, inner_text) for each \\memory{...} block.
    Uses brace-depth counting so it won't break on nested braces (e.g., LaTeX \\frac{1}{2}).
    """
    if not raw_output:
        return []
    spans: List[Tuple[int, int, str]] = []
    i = 0
    n = len(raw_output)
    while i < n:
        j = raw_output.find(_MEMORY_START, i)
        if j < 0:
            break
        k = j + len(_MEMORY_START)
        depth = 1
        inner_chars: List[str] = []
        prev = ""
        while k < n and depth > 0:
            ch = raw_output[k]
            # Treat escaped braces (\\{, \\}) as literals w.r.t. depth
            if ch == "{" and prev != "\\":
                depth += 1
            elif ch == "}" and prev != "\\":
                depth -= 1
                if depth == 0:
                    k += 1  # include closing brace
                    break
            if depth > 0:
                inner_chars.append(ch)
            prev = ch
            k += 1
        inner = "".join(inner_chars).strip()
        spans.append((j, k, inner))
        i = max(k, j + 1)
    return spans

def _extract_memory_blocks(raw_output: str) -> List[str]:
    """Extract one or more memory blocks from raw model output.

    Returns a list of *contents* (without the wrapper). Falls back to parse_memory
    if no blocks are found.
    """
    spans = _find_memory_spans(raw_output or "")
    blocks = [s[2].strip() for s in spans if s[2] and s[2].strip()]
    if blocks:
        return blocks
    # Fallback: legacy parser (usually returns a single block)
    single = (parse_memory(raw_output) or "").strip()
    return [single] if single else []

def _basic_guard(text: str, *, min_len: int = 20, max_len: int = 2000) -> bool:
    if not text:
        return False
    t = text.strip()
    if len(t) < min_len or len(t) > max_len:
        return False
    banned = [
        "As an AI",
        "As a language model",
        "I can't",
        "I cannot",
        "I am unable",
        "æŠ±æ­‰",
        "æ— æ³•",
    ]
    if any(b in t for b in banned):
        return False
    return True

# ------------------------------------------------------------------------------
# Acceptance test (for high-score evolve candidates)
# ------------------------------------------------------------------------------

_EVOLVE_ACCEPT_PROMPT = r'''
You are a strict evaluator for adding NEW memories to a RAG memory store.

[Failed Queries] (optional; may be empty)
{failed_queries}

[Parent Memory]
{old_memory}

[Candidate New Memory]
{new_memory}

[Task]
Decide whether the Candidate New Memory is (1) relevant, (2) atomic, and (3) likely to improve answers for the Failed Queries without introducing hallucinations or redundant fluff.

[Output Format - STRICT]
Verdict: PASS|FAIL
Feedback: <If FAIL, 1-2 short sentences. If PASS, write "OK".>
'''

_EVOLVE_VERDICT_RE = re.compile(r"Verdict:\s*(PASS|FAIL)", re.IGNORECASE)
_EVOLVE_FEEDBACK_RE = re.compile(r"Feedback:\s*(.*)", re.IGNORECASE | re.DOTALL)

def _evolve_parse_acceptance(output: str):
    if not output:
        return {"verdict": "FAIL", "feedback": "No judge output."}
    m = _EVOLVE_VERDICT_RE.search(output)
    verdict = (m.group(1).upper() if m else "FAIL")
    m2 = _EVOLVE_FEEDBACK_RE.search(output)
    feedback = (m2.group(1).strip() if m2 else "").strip()
    if not feedback:
        feedback = "OK" if verdict == "PASS" else "Missing feedback."
    return {"verdict": verdict, "feedback": feedback}

def _evolve_acceptance_batch(cfg, items):
    prompts = []
    for it in items:
        prompts.append(_EVOLVE_ACCEPT_PROMPT.format(
            failed_queries=(it.get("failed_queries","") or "").strip(),
            old_memory=(it.get("old_memory","") or "").strip(),
            new_memory=(it.get("new_memory","") or "").strip(),
        ))
    if not prompts:
        return []
    outs = call_expert_batch(prompts, cfg)
    return [_evolve_parse_acceptance(o) for o in outs]

def _evolve_acceptance_enabled(cfg) -> bool:
    opt = getattr(cfg, "optimizer", None)
    if opt is None:
        return True
    acc = getattr(opt, "acceptance", None)
    if acc is None:
        return bool(getattr(opt, "acceptance_enabled", True))
    return bool(getattr(acc, "enabled", True))

# ------------------------------------------------------------------------------
# Rollback / retry logic (shared for SUPPLEMENT and SPLIT)
# ------------------------------------------------------------------------------

def _get_max_retries(cfg) -> int:
    try:
        return int(getattr(cfg.parameters, "max_retries", 2) or 2)
    except Exception:
        return int(getattr(cfg.parameters, "max_retries", 2) or 2)

def _judge_one_candidate(cfg, *, failed_queries: str, old_memory: str, new_memory: str) -> Optional[str]:
    """Return None if PASS; else return feedback string."""
    if not _evolve_acceptance_enabled(cfg):
        return None
    if not (failed_queries or "").strip():
        return None
    res = _evolve_acceptance_batch(cfg, [{
        "failed_queries": failed_queries,
        "old_memory": old_memory,
        "new_memory": new_memory,
    }])[0]
    if (res.get("verdict") or "").upper() == "PASS":
        return None
    return res.get("feedback", "Acceptance FAIL.")

def _pick_one_valid(cfg, *, blocks: List[str], failed_queries: str, old_memory: str) -> Tuple[Optional[str], str]:
    """Pick the first candidate that passes guard+acceptance.

    Returns (picked_or_none, failure_reason_for_retry).
    """
    if not blocks:
        return None, "No \\memory{...} block parsed."
    min_l = int(getattr(cfg.optimizer, "min_memory_len", 20) or 20)
    max_l = int(getattr(cfg.optimizer, "max_memory_len", 2000) or 2000)

    first_failure = "Unknown error"
    for cand in blocks[:1]:  # IMPORTANT: always ONE memory (SUPPLEMENT & SPLIT are 1-to-1)
        if not _basic_guard(cand, min_len=min_l, max_len=max_l):
            first_failure = "Rejected by basic guard (length or banned words)."
            continue
        fb = _judge_one_candidate(cfg, failed_queries=failed_queries, old_memory=old_memory, new_memory=cand)
        if fb is None:
            return cand, "OK"
        first_failure = fb
    return None, first_failure

def _rewrite_with_feedback(cfg, *, base_prompt: str, prev_output: str, feedback: str) -> str:
    retry_prompt = (
        base_prompt
        + "\n\n[Previous Attempt]\n"
        + (prev_output or "")[:800]
        + "\n\n[Judge Feedback]\n"
        + (feedback or "")
        + "\n\nRewrite following the feedback. Output ONLY ONE \\memory{...} block."
    )
    return call_llm_batch([retry_prompt], cfg)[0]

# ------------------------------------------------------------------------------
# Main high-score evolution
# ------------------------------------------------------------------------------

def evolve_high_score_opt(cfg: DictConfig, memories: Dict, memory_stats: Dict, high_ids: List[str]) -> Set[str]:
    """High-score memory evolution (SUPPLEMENT / SPLIT) with robust parsing & rollback.
    Semantics: both SUPPLEMENT and SPLIT add exactly ONE new memory; champion memory is never modified.
    """
    print("\n========== é«˜åˆ†è®°å¿†è¿›åŒ–é˜¶æ®µ (Ace Evolution) ==========")

    # --- 1) Log path ---
    log_file_path = cfg.paths.get("highfreq_textgrad_log", "textgrad_debug_log.txt")
    try:
        os.makedirs(os.path.dirname(log_file_path), exist_ok=True)
    except Exception:
        pass
    print(f"ğŸ“ è¿›åŒ–æ—¥å¿—å°†è¿½åŠ è‡³: {log_file_path}")

    def tee_print(msg):
        """åŒæ—¶æ‰“å°åˆ°ç»ˆç«¯å¹¶è¿½åŠ å†™å…¥æ—¥å¿—æ–‡ä»¶"""
        # print(msg) # æ‰“å°åˆ°å±å¹•
        try:
            with open(log_file_path, "a", encoding="utf-8") as f:
                f.write(str(msg) + "\n") # å†™å…¥æ–‡ä»¶
        except Exception:
            pass

    # --- 2) Target selection: only champions with failed queries ---
    target_ids: List[str] = []
    for mid in list(high_ids):
        if mid not in memories:
            continue
        stats = memory_stats.get(mid, {}) or {}
        neg_queries = stats.get("neg_queries", []) or []
        if len(neg_queries) > 0:
            target_ids.append(mid)

    # Optional debug limit
    debug_limit = int(getattr(cfg.optimizer, "debug_high_score_limit", 0) or 0)
    if debug_limit > 0:
        target_ids = target_ids[:debug_limit]

    print(f"ğŸ’ å¾…è¿›åŒ–çš„ç‹ç‰Œè®°å¿†æ•°é‡: {len(target_ids)}")
    if target_ids:
        print(f"   ğŸ†” ID åˆ—è¡¨: {target_ids}")
    if not target_ids:
        return set()

    batch_size = int(cfg.optimizer.llm_batch_size)
    new_created_ids_total: Set[str] = set()

    for i in range(0, len(target_ids), batch_size):
        chunk_ids = target_ids[i : i + batch_size]
        print(f" ğŸ§  [Expert-Batch] æ­£åœ¨å¤„ç†ç¬¬ {i} - {i+len(chunk_ids)} æ¡é«˜åˆ†è®°å¿†...")

        expert_prompts: List[str] = []
        chunk_metadata: List[dict] = []

        for mid in chunk_ids:
            rec = memories[mid]
            base_text = rec.get("contents", "")
            stats = memory_stats.get(mid, {}) or {}
            neg_queries = stats.get("neg_queries", []) or []

            top_k_neg = int(getattr(cfg.optimizer, "high_grad_topk_neg", 5) or 5)
            neg_text = "\n".join([f"- {q}" for q in neg_queries[:top_k_neg]])

            try:
                prompt = cfg.optimizer.prompts.high_grad_expert.format(content=base_text, neg_queries=neg_text)
            except Exception as e:
                print(f"âŒ Prompt æ ¼å¼åŒ–å¤±è´¥ (MID: {mid}): {e}")
                continue

            expert_prompts.append(prompt)
            chunk_metadata.append({
                "mid": mid,
                "base_text": base_text,
                "neg_text": neg_text,
                "expert_prompt_content": prompt,
            })

        if not expert_prompts:
            continue

        expert_outputs = call_expert_batch(expert_prompts, cfg)

        student_prompts: List[str] = []
        student_tasks: List[dict] = []

        for meta, expert_resp in zip(chunk_metadata, expert_outputs):
            mid = meta["mid"]

            log_info = {
                "mid": mid,
                "type": "evolve_high_score",
                "expert_prompt": meta.get("expert_prompt_content", ""),
                "expert_output": expert_resp,
                "action": "UNKNOWN",
                "gradient": "N/A",
                "split_num": 1,  # kept for backward compatibility in logs
                "student_prompt": "N/A",
            }

            if not expert_resp:
                tee_print(f"âŒ [Error] MID: {mid} - Expert output is empty.")
                continue

            action_match = re.search(r"\\box\{(IGNORE|SUPPLEMENT|SPLIT)\}", expert_resp)
            action = action_match.group(1).strip() if action_match else "IGNORE"

            gradient_match = re.search(r"\\gradient\{(.*?)\}", expert_resp, re.DOTALL)
            advice = gradient_match.group(1).strip() if gradient_match else "No specific advice provided."
            gradient = advice
            # ================= æ‰“å°ä»£ç  =================
            tee_print(f"\n[Expert Logic] MID: {mid}")
            tee_print(f"   >>> ğŸ› ï¸ åŸè¯­ (Action): {action}")
            tee_print(f"   >>> ğŸ§  æ¢¯åº¦ (Gradient): {gradient[:20]}...{gradient[-20:]}") # åªæ‰“å°40å­—
            # ====================================================

            # IMPORTANT: SPLIT is forced to ONE new memory (no 1-to-many)
            split_num = 1

            log_info["action"] = action
            log_info["gradient"] = advice
            log_info["split_num"] = split_num

            if action == "IGNORE":
                with open(log_file_path, "a", encoding="utf-8") as log_f:
                    mid_display = str(mid)[:8]
                    # æˆªæ–­ Advice/Gradient æ˜¾ç¤º
                    grad = str(advice)
                    grad_prev = f"{grad[:20]}...{grad[-20:]}" if len(grad) > 40 else grad
                    
                    log_lines = [
                        f"ğŸ†” [{mid_display}] | EVOLVE | ğŸš« IGNORED",
                        f"   Strategy: High-Score-Evolve",
                        f"   Action  : IGNORE",
                        f"   Reason  : {grad_prev}",  # è¿™é‡Œçš„ advice é€šå¸¸åŒ…å«ä¸ºä»€ä¹ˆå¿½ç•¥çš„åŸå› 
                        "-" * 60 + "\n"
                    ]
                    log_f.write("\n".join(log_lines))
                    log_f.flush()
                # ==============================================================
                continue

            if action == "SUPPLEMENT":
                tpl = cfg.optimizer.prompts.appgrad_high_supplement
                s_prompt = tpl.format(original_content=meta["base_text"], advice=advice)
                log_info["student_prompt"] = s_prompt
                student_prompts.append(s_prompt)
                student_tasks.append({
                    "parent_mid": mid,
                    "action": "SUPPLEMENT",
                    "log": log_info,
                    "old_content": meta.get("base_text",""),
                    "neg_text": meta.get("neg_text",""),
                })

            elif action == "SPLIT":
                tpl = cfg.optimizer.prompts.appgrad_high_split
                # allow templates with/without {num}
                try:
                    s_prompt = tpl.format(neg_text=meta["neg_text"], advice=advice, num=1)
                except Exception:
                    s_prompt = tpl.format(neg_text=meta["neg_text"], advice=advice)
                log_info["student_prompt"] = s_prompt
                student_prompts.append(s_prompt)
                student_tasks.append({
                    "parent_mid": mid,
                    "action": "SPLIT",
                    "log": log_info,
                    "old_content": meta.get("base_text",""),
                    "neg_text": meta.get("neg_text",""),
                })

        if not student_prompts:
            continue

        # ========== ğŸ”¥ã€æ–°å¢æ‰“å° 1ã€‘æŸ¥çœ‹å­¦ç”Ÿé¢†åˆ°çš„ä»»åŠ¡å• ==========
        # é€‚é…è¯´æ˜ï¼šåŸå˜é‡ student_metadata åœ¨è¿™é‡Œå¯¹åº” student_tasks
        tee_print(f"   ğŸ“‹ å­¦ç”Ÿä»»åŠ¡è¯¦æƒ…:")
        for task in student_tasks:
            # è·å–åŠ¨ä½œç±»å‹ (SUPPLEMENT/SPLIT)
            action_type = task.get("action", "UNKNOWN").upper()
            # è·å– ID
            mid_display = task['parent_mid'][:8] 
            
            tee_print(f"   -> ğŸ†” [{mid_display}] | åŠ¨ä½œ: {action_type} | ç­–ç•¥: High-Score-Evolve")
        tee_print("   ------------------------------------------------")
        # =======================================================

        student_outputs = call_llm_batch(student_prompts, cfg)

        # ========== ğŸ”¥ã€æ–°å¢æ‰“å° 2ã€‘æŸ¥çœ‹å­¦ç”Ÿåˆç¨¿ ==========
        # é€‚é…è¯´æ˜ï¼šoutputs å¯¹åº” student_outputs
        tee_print(f"\n   ğŸ“¨ [Student Output] æ”¶åˆ° {len(student_outputs)} æ¡åˆç¨¿:")
        for task, raw_txt in zip(student_tasks, student_outputs):
            clean_preview = raw_txt.strip().replace('\n', ' ')
            mid = task['parent_mid'][:8]
            # é˜²æ­¢å­—ç¬¦ä¸²åˆ‡ç‰‡æŠ¥é”™ï¼Œåšä¸ªç®€å•é•¿åº¦åˆ¤æ–­
            preview_str = clean_preview if len(clean_preview) < 40 else f"{clean_preview[:20]}...{clean_preview[-20:]}"
            tee_print(f"      -> ğŸ†” {mid} | é•¿åº¦: {len(raw_txt)} | é¢„è§ˆ: {preview_str}")
        # =================================================

        max_retries = _get_max_retries(cfg)

        # ==============================================================================
        # ğŸ”¥ æ ¸å¿ƒä¼˜åŒ–ï¼šå¹¶è¡ŒåŒ–éªŒè¯ä¸é‡å†™æµç¨‹ (Parallel Validation & Rewrite)
        # ==============================================================================
        
        # 1. åˆå§‹åŒ–å€™é€‰çŠ¶æ€åˆ—è¡¨
        candidates = []
        for task, raw_out in zip(student_tasks, student_outputs):
            candidates.append({
                "task": task,
                "history": [{"out": raw_out, "judge": None}],
                "status": "PENDING", # PENDING, PASS, FAIL
                "final_output": None,
                "fail_reason": ""
            })

        max_retries = _get_max_retries(cfg)
        min_len = int(getattr(cfg.optimizer, "min_memory_len", 20) or 20)
        max_len = int(getattr(cfg.optimizer, "max_memory_len", 2000) or 2000)

        # 2. æ‰¹å¤„ç†å¾ªç¯ (Batch Loop): éªŒè¯ -> ç­›é€‰å¤±è´¥è€… -> æ‰¹é‡é‡å†™ -> å†æ¬¡éªŒè¯
        # å¾ªç¯æ¬¡æ•° = åˆå§‹éªŒè¯(Round 0) + é‡è¯•æ¬¡æ•°(max_retries)
        
        for round_idx in range(max_retries + 1):
            
            # --- Step A: æ‰¹é‡éªŒè¯ (Batch Judge) ---
            to_judge_indices = []
            judge_payloads = []
            
            for i, cand in enumerate(candidates):
                # åªå¤„ç†çŠ¶æ€ä¸º PENDING çš„ä»»åŠ¡ (å³å°šæœªé€šè¿‡ä¸”æœ‰æ–°è¾“å‡ºçš„ä»»åŠ¡)
                if cand["status"] != "PENDING":
                    continue
                    
                last_attempt = cand["history"][-1]
                raw_txt = last_attempt["out"]
                
                # 1. æå– Memory Block
                blocks = _extract_memory_blocks(raw_txt)
                
                # 2. åŸºç¡€ Guard æ£€æŸ¥ (é•¿åº¦/è¿ç¦è¯)
                valid_block = None
                for b in blocks:
                    if _basic_guard(b, min_len=min_len, max_len=max_len):
                        valid_block = b
                        break
                
                if not valid_block:
                    cand["fail_reason"] = "Rejected by basic guard (length/format/banned)."
                    last_attempt["judge"] = {"verdict": "FAIL", "feedback": cand["fail_reason"]}
                    # çŠ¶æ€ä¿æŒ PENDINGï¼Œç•™ç»™é‡å†™é˜¶æ®µå¤„ç†
                else:
                    last_attempt["parsed_block"] = valid_block # æš‚å­˜åˆæ³•çš„å—
                    
                    # 3. å†³å®šæ˜¯å¦éœ€è¦è£åˆ¤ (æœ‰é”™é¢˜æ‰éœ€è¦ Expert Judge)
                    meta = cand["task"]
                    neg_text = meta.get("neg_text", "")
                    
                    if _evolve_acceptance_enabled(cfg) and neg_text:
                        to_judge_indices.append(i)
                        judge_payloads.append({
                            "failed_queries": neg_text,
                            "old_memory": meta.get("old_content", ""),
                            "new_memory": valid_block
                        })
                    else:
                        # æ²¡å¼€è£åˆ¤æˆ–æ²¡æœ‰é”™é¢˜ -> ç›´æ¥ PASS
                        cand["status"] = "PASS"
                        cand["final_output"] = valid_block
                        last_attempt["judge"] = {"verdict": "PASS", "feedback": "OK (Skipped)"}

            # --- å‘é€æ‰¹é‡è£åˆ¤è¯·æ±‚ (çœŸæ­£çš„å¹¶è¡ŒéªŒè¯) ---
            if judge_payloads:
                tee_print(f"   âš–ï¸ [Batch Judge] Round {round_idx}: æ­£åœ¨è¯„å®¡ {len(judge_payloads)} æ¡å€™é€‰é¡¹...")
                # è¿™é‡Œçš„ _evolve_acceptance_batch å†…éƒ¨ä¼šè°ƒç”¨ call_expert_batchï¼Œäº«å—ä½ åˆšæ‰æ”¹çš„ 16 å¹¶å‘
                judge_results = _evolve_acceptance_batch(cfg, judge_payloads)
                
                for idx, res in zip(to_judge_indices, judge_results):
                    cand = candidates[idx]
                    last_attempt = cand["history"][-1]
                    last_attempt["judge"] = res
                    
                    if res["verdict"] == "PASS":
                        cand["status"] = "PASS"
                        cand["final_output"] = last_attempt["parsed_block"]
                    else:
                        cand["fail_reason"] = res["feedback"]
                        # çŠ¶æ€ä¿æŒ PENDING

            # --- Step B: å‡†å¤‡æ‰¹é‡é‡å†™ (Batch Rewrite) ---
            # å¦‚æœæ˜¯æœ€åä¸€æ¬¡å¾ªç¯ï¼Œå°±ä¸é‡å†™äº†ï¼Œç›´æ¥ç»“æŸ
            if round_idx == max_retries:
                break

            to_rewrite_indices = []
            retry_prompts = []

            for i, cand in enumerate(candidates):
                if cand["status"] == "PENDING":
                    to_rewrite_indices.append(i)
                    
                    # æ„é€ é‡å†™ Prompt
                    last_attempt = cand["history"][-1]
                    prev_out = last_attempt.get("out", "")
                    fb = cand.get("fail_reason", "Improve logic.")
                    base_prompt = cand["task"]["log"].get("student_prompt", "")
                    
                    # æ„é€ é‡å†™æç¤ºè¯
                    retry_prompt = (
                        base_prompt
                        + "\n\n[Previous Attempt]\n"
                        + (prev_out or "")[:800]
                        + "\n\n[Judge Feedback]\n"
                        + (fb or "")
                        + "\n\nRewrite following the feedback. Output ONLY ONE \\memory{...} block."
                    )
                    retry_prompts.append(retry_prompt)

            if not retry_prompts:
                # æ‰€æœ‰ä»»åŠ¡éƒ½å¤„ç†å®Œäº† (å…¨PASSæˆ–æ²¡å¾—æ•‘äº†)
                break
            
            # --- å‘é€æ‰¹é‡é‡å†™è¯·æ±‚ (çœŸæ­£çš„å¹¶è¡Œé‡å†™) ---
            tee_print(f"\n   ğŸ”„ [Batch Retry] Round {round_idx+1}: {len(retry_prompts)} æ¡ä»»åŠ¡è¢«æ‰“å›ï¼Œæ­£åœ¨æ‰¹é‡é‡å†™...")
            
            # ç®€å•æ‰“å°å‰å‡ ä¸ªå¤±è´¥åŸå› ä¾›è°ƒè¯•
            for idx in to_rewrite_indices[:2]:
                    mid = candidates[idx]["task"]["parent_mid"][:8]
                    fb = candidates[idx].get("fail_reason", "")
                    tee_print(f"      -> âŒ ID: {mid} | åŸå› : {fb[:50]}...")

            # è¿™é‡Œçš„ call_llm_batch å†…éƒ¨æ˜¯ä½ åˆšæ‰æ”¹çš„ SGLang å¹¶å‘ç‰ˆï¼Œäº«å— 32 å¹¶å‘
            retry_outputs = call_llm_batch(retry_prompts, cfg)
            
            # å¡«å…¥æ–°ç»“æœï¼Œç­‰å¾…ä¸‹ä¸€è½®éªŒè¯
            for idx, new_out in zip(to_rewrite_indices, retry_outputs):
                # æ‰“å°ä¸€ä¸‹é‡å†™ç»“æœé¢„è§ˆ
                mid = candidates[idx]["task"]["parent_mid"][:8]
                clean = new_out.strip().replace('\n', ' ')
                prev = clean if len(clean) < 40 else f"{clean[:20]}...{clean[-20:]}"
                tee_print(f"      -> ğŸ“¨ [Retry] ID: {mid} | é¢„è§ˆ: {prev}")
                
                candidates[idx]["history"].append({"out": new_out, "judge": None})

        # ==============================================================================
        # 3. æœ€ç»ˆç»“ç®—ä¸æ—¥å¿—ä¿å­˜ (Finalize)
        # ==============================================================================
        for cand in candidates:
            task = cand["task"]
            log_info = task["log"]
            parent_mid = task["parent_mid"]
            action_type = task["action"]
            
            # åˆ¤å®šæœ€ç»ˆçŠ¶æ€
            accepted = (cand["status"] == "PASS" and cand["final_output"] is not None)
            
            # å‡†å¤‡æ—¥å¿—å†…å®¹
            final_txt = cand["final_output"] if accepted else cand["history"][-1]["out"]
            final_txt = (final_txt or "").strip().replace('\n', ' ')
            content_prev = f"{final_txt[:30]}...{final_txt[-30:]}" if len(final_txt) > 60 else final_txt
            
            last_judge = cand["history"][-1].get("judge") or {}
            judge_verdict = last_judge.get("verdict", "FAIL")
            judge_fb = last_judge.get("feedback", cand.get("fail_reason", "Unknown"))
            judge_prev = f"{judge_fb[:50]}..." if len(judge_fb) > 50 else judge_fb
            
            # å†™å…¥ç®€æ´æ—¥å¿—
            with open(log_file_path, "a", encoding="utf-8") as log_f:
                status_str = "âœ… ACCEPTED" if accepted else "âŒ REJECTED"
                grad = str(log_info.get("gradient", ""))
                grad_prev = f"{grad[:20]}...{grad[-20:]}" if len(grad) > 40 else grad
                
                log_lines = [
                    f"ğŸ†” [{parent_mid[:8]}] | {action_type} | {status_str}",
                    f"   Strategy: High-Score-Evolve (Batched)",
                    f"   Action  : {action_type}",
                    f"   Gradient: {grad_prev}",
                    f"   Result  : {content_prev}",
                    f"   Judge   : {judge_verdict} ({judge_prev})",
                    "-" * 60 + "\n"
                ]
                log_f.write("\n".join(log_lines))
                log_f.flush()

            # å¦‚æœæˆåŠŸï¼Œä¿å­˜åˆ°å†…å­˜åº“
            if accepted:
                new_id = str(uuid.uuid4())
                suffix = "supplement" if action_type == "SUPPLEMENT" else "split"
                _save_new_memory(memories, memory_stats, new_id, cand["final_output"], parent_mid, f"high_score_{suffix}")
                new_created_ids_total.add(new_id)
                tee_print(f"  âœ¨ [NEW] {parent_mid[:8]} -> {new_id[:8]} ({action_type})")

    print(f"âœ… [Evolve] è¿›åŒ–å®Œæˆï¼Œå…±æ–°å¢ {len(new_created_ids_total)} æ¡é«˜é˜¶è®°å¿†")
    return new_created_ids_total

# ------------------------------------------------------------------------------
# Helpers
# ------------------------------------------------------------------------------

def _save_new_memory(memories, memory_stats, new_id, content, parent_id, opt_type):
    memories[new_id] = {
        "id": new_id,
        "contents": content,
        "cluster_id": -1,
        "opt_type": opt_type,
        "parent_id": parent_id,
    }
    memory_stats[new_id] = {
        "alpha": 1.0,
        "beta": 1.0,
        "neg_queries": [],
        "pos_queries": [],
    }

def _write_log(log_file_path: str, info: dict, result_content: str):
    try:
        with open(log_file_path, "a", encoding="utf-8") as f:
            log_entry = (
                f"\n{'='*60}\n"
                f"ğŸ†” Parent Memory ID: {info.get('mid','')}\n"
                f"--- ğŸ§  Expert Prompt (Input) ---\n{info.get('expert_prompt','')}\n\n"
                f"--- ğŸ—£ï¸ Expert Output (Raw) ---\n{info.get('expert_output','')}\n\n"
                f"--- ğŸ“¦ Parsed Decision ---\n"
                f"   Action   : {info.get('action','')}\n"
                f"   Advice   : {info.get('gradient','')}\n"
                f"   Split Num: {info.get('split_num',1)}\n\n"
                f"--- ğŸ“ Student Prompt ---\n{info.get('student_prompt','')}\n\n"
                f"--- âœ¨ Final Result (New Memories) ---\n{result_content}\n"
                f"{'='*60}\n"
            )
            f.write(log_entry)
            f.flush()
    except Exception as e:
        print(f"âš ï¸ æ—¥å¿—å†™å…¥å¼‚å¸¸: {e}")