import os
import re
import uuid
from omegaconf import DictConfig
# å‡è®¾è¿™æ˜¯ä½ çš„å·¥å…·åº“è·¯å¾„
from tools.optimize.callllm import init_llm, call_llm_batch
from tools.optimize.callexpert import init_expert_llm, call_expert, call_expert_batch
from tools.optimize.memoryload import load_clustered_memories, load_cluster_summary
# ä¿ç•™åŸæœ‰å¯¼å…¥
from optimize.prompt_generate import summarize_experience_prompt, expand_low_freq_memory_prompt
from utils.memorywrap import parse_memory

_MEMORY_BLOCK_RE = re.compile(r"\\memory\{(.*)\}", re.DOTALL)

def _extract_single_memory(raw_output: str) -> str:
    if not raw_output:
        return ""
    # Primary: project parser
    txt = (parse_memory(raw_output) or "").strip()
    if txt:
        return txt
    # Fallback: regex
    m = _MEMORY_BLOCK_RE.search(raw_output)
    return (m.group(1).strip() if m else "")

def _basic_guard(text: str, *, min_len: int = 20, max_len: int = 2000) -> bool:
    if not text:
        return False
    t = text.strip()
    if len(t) < min_len or len(t) > max_len:
        return False
    banned = [
        "As an AI",
        "As a language model",
        "I can't",
        "I cannot",
        "I am unable",
        "æŠ±æ­‰",
        "æ— æ³•",
    ]
    if any(b in t for b in banned):
        return False
    return True


# ------------------------------------------------------------------------------
# Acceptance test + rollback (retry) helpers
# ------------------------------------------------------------------------------
# Motivation:
# - Prevent "writeback pollution": do not overwrite the memory store unless the edit
#   is judged to improve the previously failed queries.
# - If a candidate fails the acceptance test, retry generation up to a small budget.
#
# Default behavior (if cfg is missing): enabled=True, max_retries=2.

_ACCEPTANCE_PROMPT = r'''
You are a Cognitive Logic Auditor for a RAG memory store.
[Failed Queries]
{failed_queries}

[Old Memory]
{old_memory}

[New Memory]
{new_memory}

[Audit Criteria]
1. **Methodology Check**: Does the New Memory explain the *reasoning logic*, *step-by-step derivation*, or *general principle*? (Reject if it just gives the factual answer).
2. **Generalization**: Is the logic abstract enough to apply to similar problems, not just the specific failed queries?
3. **Accuracy**: No hallucinations or uncertain facts.
4. **Atomicity**: Focuses on one core concept/framework.

[Output Format â€” STRICT]
Verdict: PASS|FAIL
Feedback: <If FAIL, explain specifically which logic is missing. If PASS, write "OK".>
'''

_VERDICT_RE = re.compile(r"Verdict:\s*(PASS|FAIL)", re.IGNORECASE)
_FEEDBACK_RE = re.compile(r"Feedback:\s*(.*)", re.IGNORECASE | re.DOTALL)

def _get_acceptance_params(cfg):
    """
    ä¿®æ”¹åï¼šç›´æ¥è¯»å– cfg.parameters.acceptance ä¸‹çš„é…ç½®
    """
    max_retries = cfg.parameters.max_retries
    print(f'é™åˆ¶è½®æ¬¡æ˜¯ï¼š{max_retries}')

    return True, max_retries

def _parse_acceptance(output: str):
    if not output:
        return {"verdict": "FAIL", "feedback": "No judge output."}
    m = _VERDICT_RE.search(output)
    verdict = (m.group(1).upper() if m else "FAIL")
    m2 = _FEEDBACK_RE.search(output)
    feedback = (m2.group(1).strip() if m2 else "").strip()
    if not feedback:
        feedback = "OK" if verdict == "PASS" else "Missing feedback."
    return {"verdict": verdict, "feedback": feedback}

def _acceptance_test_batch(cfg, items):
    prompts = []
    for it in items:
        prompts.append(_ACCEPTANCE_PROMPT.format(
            failed_queries=(it.get("failed_queries","") or "").strip(),
            old_memory=(it.get("old_memory","") or "").strip(),
            new_memory=(it.get("new_memory","") or "").strip(),
        ))
    if not prompts:
        return []
    judge_outs = call_expert_batch(prompts, cfg)
    return [_parse_acceptance(o) for o in judge_outs]

def _build_retry_prompt(original_student_prompt: str, prev_memory: str, judge_feedback: str) -> str:
    return (
        original_student_prompt
        + "\n\n[Previous Attempt]\n"
        + f"\\memory{{{(prev_memory or '').strip()}}}\n\n"
        + "[Judge Feedback]\n"
        + (judge_feedback or "").strip()
        + "\n\nRewrite again. Output ONLY the memory wrapper."
    )

def textgrad_opt(cfg, memories, memory_stats, cluster_to_ids, bad_ids, to_delete_ids):
    """
    ğŸ”¥ [Step 5] æ‰§è¡Œ TextGrad æ‰¹é‡ä¼˜åŒ–é€»è¾‘ (Expert Decision Guided)
    åŒ…å«ï¼šæ¢¯åº¦å†³ç­– (Expert Agent) -> åŸè¯­åˆ†å‘ (Refine/Expand/Replace) -> æ‰§è¡Œä¼˜åŒ– (Student)
    
    Args:
        cfg: Hydra é…ç½®å¯¹è±¡
        memories: è®°å¿†åº“å­—å…¸ (ä¼šè¢«åŸåœ°ä¿®æ”¹)
        memory_stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸ (éœ€è¦æ”¯æŒåŠ¨æ€æ–°å¢)
        cluster_to_ids: èšç±»åå‘ç´¢å¼•
        bad_ids: å¾…ä¼˜åŒ–çš„ä½åˆ† ID åˆ—è¡¨
        to_delete_ids: å·²ç»è¢«æ ‡è®°åˆ é™¤çš„ ID é›†åˆ (ç”¨äºè¿‡æ»¤)
        
    Returns:
        set: æœ¬è½®è¢«æˆåŠŸä¼˜åŒ–çš„ ID é›†åˆ
    """
    print("\n========== TextGrad è®°å¿†ä¿®æ­£é˜¶æ®µ (Expert Batch Guided with Primitives) ==========")
    log_file_path = cfg.paths.get("lowfreq_textgrad_log", "textgrad_debug_log.txt")
    print(f"ğŸ“ è°ƒè¯•æ—¥å¿—å°†å†™å…¥: {log_file_path}")
    
    def tee_print(msg):
        """åŒæ—¶æ‰“å°åˆ°ç»ˆç«¯å¹¶è¿½åŠ å†™å…¥æ—¥å¿—æ–‡ä»¶"""
        # print(msg) # æ‰“å°åˆ°å±å¹•
        try:
            with open(log_file_path, "a", encoding="utf-8") as f:
                f.write(str(msg) + "\n") # å†™å…¥æ–‡ä»¶
        except Exception:
            pass
    
    # 1. è¿‡æ»¤å‡ºçœŸæ­£éœ€è¦å¤„ç†çš„ ID
    bad_ids_list = list(bad_ids)
    if not isinstance(bad_ids, list):
        bad_ids_list.sort()
    low_expand_ids = [mid for mid in bad_ids_list if mid in memories and mid not in to_delete_ids]
    print(f"ğŸ¯ å¾…ä¼˜åŒ–ç›®æ ‡æ•°é‡: {len(low_expand_ids)}")
    if low_expand_ids:
        print(f"   ğŸ†” ID åˆ—è¡¨: {low_expand_ids}")
    batch_size = cfg.optimizer.llm_batch_size
    optimized_ids = set()

    # 2. æŒ‰ Chunk éå†å¤„ç†
    for i in range(0, len(low_expand_ids), batch_size):
        chunk_ids = low_expand_ids[i : i + batch_size]
        
        # --- Step 5.1: å‡†å¤‡ä¸“å®¶ Prompts (Diagnosis Phase) ---
        grad_prompts = []
        grad_metadata = [] 
        
        for mid in chunk_ids:
            rec = memories[mid]
            base_text = rec.get("contents", "")
            cluster_id = rec.get("cluster_id")
            stats = memory_stats.get(mid, {})
            neg_queries = stats.get('neg_queries', [])
            
            # --- å¯»æ‰¾ä¼˜ç­‰ç”Ÿ (Momentum / Good Neighbors) ---
            good_neighbors_text = []
            if cluster_id is not None:
                cluster_id = int(cluster_id)
                members = cluster_to_ids.get(cluster_id, [])
                for m_id in members:
                    if str(m_id) == mid: continue
                    s = memory_stats.get(str(m_id), {})
                    s_total = s.get('alpha', 0) + s.get('beta', 0)
                    # åªæœ‰èƒœç‡ > 0.8 çš„æ‰ç®—å¥½æ¦œæ ·
                    if s_total > 0 and (s.get('alpha', 0)/s_total) > 0.8:
                        
                        m_key = str(m_id)
                        if m_key not in memories:
                            continue
                        good_neighbors_text.append(memories[m_key].get("contents", ""))
                good_neighbors_text = good_neighbors_text[:3]
            good_examples_str = "\n".join([f"- {t}" for t in good_neighbors_text])
            
            # --- æ„å»ºä¸“å®¶è¾“å…¥ ---
            # å¦‚æœæœ‰è´Ÿåé¦ˆï¼ˆé”™é¢˜ï¼‰ï¼Œåˆ™è¿›å…¥â€œä¸“å®¶å†³ç­–æ¨¡å¼â€
            if len(neg_queries) > 0:
                # è·å– Top-K é”™é¢˜
                top_k = cfg.optimizer.get("top_k_neg_queries", 3)
                selected_negs = neg_queries[:top_k]
                neg_text = "\n".join([f"- {q}" for q in selected_negs])
                
                # ğŸ”¥ ç›´æ¥ä½¿ç”¨ Config ä¸­çš„å†³ç­– Promptï¼Œè€Œä¸æ˜¯è°ƒç”¨å›ºå®šå‡½æ•°
                decision_prompt = cfg.optimizer.prompts.low_grad_expert.format(
                    content=base_text,
                    neg_queries=neg_text
                )
                
                grad_prompts.append(decision_prompt)
                grad_metadata.append({
                    "mid": mid, 
                    "need_expert": True,
                    "expert_prompt_content": decision_prompt,  # <--- ğŸ”¥ æ–°å¢ï¼šå­˜ä¸‹ä¸“å®¶Prompt
                    "base_text": base_text,
                    "neg_text": neg_text, # å­˜ä¸‹æ¥ï¼ŒREPLACE/EXPAND è¦ç”¨
                    "good_examples_str": good_examples_str,
                    "good_neighbors_text": good_neighbors_text, 
                    "err_count": len(neg_queries)
                })
            else:
                # æ²¡æœ‰é”™é¢˜ï¼Œåªæœ‰ä½åˆ†/ä½é¢‘ -> è¿›å…¥é™çº§æ¨¡å¼ (Imitation/Reflection)
                grad_metadata.append({
                    "mid": mid, 
                    "need_expert": False,
                    "base_text": base_text,
                    "good_neighbors_text": good_neighbors_text,
                    "good_examples_str": good_examples_str,
                })

        # --- Step 5.2: æ‰¹é‡è°ƒç”¨ä¸“å®¶ (Expert Execution) ---
        expert_outputs = []
        if grad_prompts:
            print(f" ğŸ§  [Expert-Batch] æ­£åœ¨åˆ†æ {len(grad_prompts)} æ¡æ¢¯åº¦å¹¶ç”Ÿæˆå†³ç­–...")
            expert_outputs = call_expert_batch(grad_prompts, cfg)
        
    # --- Step 5.3: è§£æå†³ç­–å¹¶åˆ†å‘å­¦ç”Ÿä»»åŠ¡ (Dispatch Phase) ---
        student_prompts = []
        student_metadata = [] # è®°å½• task ç±»å‹å’Œ ID
        expert_idx = 0 
        
        for meta in grad_metadata:
            mid = meta['mid']
            # åˆå§‹åŒ–æ—¥å¿—å¯¹è±¡
            log_info = {
                "mid": mid,
                "expert_prompt": meta.get("expert_prompt_content", "N/A"),
                "expert_output": "N/A",
                "action": "N/A",
                "gradient": "N/A",
                "student_prompt": ""
            }

            if meta['need_expert']:
                expert_resp = expert_outputs[expert_idx] if expert_idx < len(expert_outputs) else ""
                expert_idx += 1
                log_info["expert_output"] = expert_resp
                
                if expert_resp:
                    # æ­£åˆ™è§£æ
                    action_match = re.search(r'\\box\{(REFINE|EXPAND|REPLACE)\}', expert_resp)
                    advice_match = re.search(r'\\advice\{(.*?)\}', expert_resp, re.DOTALL)
                    
                    action = action_match.group(1) if action_match else "REFINE" 
                    gradient = advice_match.group(1).strip() if advice_match else expert_resp
                    
                    # ================= æ‰“å°ä»£ç  =================
                    tee_print(f"\n[Expert Logic] MID: {mid}")
                    tee_print(f"   >>> ğŸ› ï¸ åŸè¯­ (Action): {action}")
                    tee_print(f"   >>> ğŸ§  æ¢¯åº¦ (Gradient): {gradient[:20]}...{gradient[-20:]}") # åªæ‰“å°40å­—
                    # ====================================================

                    log_info["action"] = action
                    log_info["gradient"] = gradient
                    
                    # === åŸè¯­åˆ†å‘é€»è¾‘ ===
                    
                    # 1. REFINE (ä¼˜åŒ–)
                    if action == "REFINE":
                        reconstruct_tpl = cfg.optimizer.prompts.appgrad_low_refine
                        prompt = reconstruct_tpl.format(content=meta['base_text'], gradient=gradient)                       
                        log_info["student_prompt"] = prompt
                        student_prompts.append(prompt)
                        # ğŸ”¥ [ä¿®å¤ç‚¹ 1] åŠ ä¸Š "log": log_info
                        student_metadata.append({
                            "mid": mid, 
                            "type": "refine", 
                            "opt_type": "expert_refine", 
                            "log": log_info,
                            "old_content": meta.get("base_text",""),
                            "neg_text": meta.get("neg_text",""),
                            "student_prompt": prompt,
                        })
                        
                    # 2. REPLACE (åˆ å¢/æ›¿æ¢)
                    elif action == "REPLACE":
                        reconstruct_tpl = cfg.optimizer.prompts.appgrad_low_replace
                        prompt = reconstruct_tpl.format(neg_queries=meta['neg_text'], gradient=gradient)
                        log_info["student_prompt"] = prompt
                        student_prompts.append(prompt)
                        # ğŸ”¥ [ä¿®å¤ç‚¹ 2] åŠ ä¸Š "log": log_info
                        student_metadata.append({
                            "mid": mid, 
                            "type": "replace", 
                            "opt_type": "expert_replace", 
                            "log": log_info,
                            "old_content": meta.get("base_text",""),
                            "neg_text": meta.get("neg_text",""),
                            "student_prompt": prompt,
                        })
                        
                    # 3. EXPAND (å¢åŠ )
                    elif action == "EXPAND":
                        # Task A
                        reconstruct_tpl = cfg.optimizer.prompts.appgrad_low_refine
                        prompt_a = reconstruct_tpl.format(content=meta['base_text'], gradient=gradient)    
                        
                        log_info_a = log_info.copy()
                        log_info_a["student_prompt"] = prompt_a
                        log_info_a["action"] = "EXPAND (Part A: Refine Old)"
                        
                        student_prompts.append(prompt_a)
                        # ğŸ”¥ [ä¿®å¤ç‚¹ 3] åŠ ä¸Š "log": log_info_a
                        student_metadata.append({
                            "mid": mid, 
                            "type": "refine", 
                            "opt_type": "expert_expand_old", 
                            "log": log_info_a,
                            "old_content": meta.get("base_text",""),
                            "neg_text": meta.get("neg_text",""),
                            "student_prompt": prompt_a,
                        })
                        
                        # Task B
                        new_id = str(uuid.uuid4())
                        reconstruct_tpl = cfg.optimizer.prompts.appgrad_low_replace
                        prompt_b = reconstruct_tpl.format(neg_queries=meta['neg_text'], gradient=gradient)
                        
                        log_info_b = log_info.copy()
                        log_info_b["student_prompt"] = prompt_b
                        log_info_b["mid"] = new_id
                        log_info_b["action"] = "EXPAND (Part B: Create New)"

                        student_prompts.append(prompt_b)
                        # ğŸ”¥ [ä¿®å¤ç‚¹ 4] åŠ ä¸Š "log": log_info_b
                        student_metadata.append({
                            "mid": new_id, 
                            "type": "create", 
                            "opt_type": "expert_expand_new", 
                            "log": log_info_b,
                            "old_content": "",
                            "neg_text": meta.get("neg_text",""),
                            "student_prompt": prompt_b,
                            "parent_mid": mid,
                        })
                        
                else:
                    # ä¸“å®¶å¤±è´¥å›é€€
                    if meta['good_neighbors_text']:
                        prompt = summarize_experience_prompt(meta['base_text'], meta['good_neighbors_text'], cfg)
                        log_info["action"] = "FALLBACK (Imitation)"
                        log_info["student_prompt"] = prompt
                        student_prompts.append(prompt)
                        # ğŸ”¥ [ä¿®å¤ç‚¹ 5] åŠ ä¸Š "log": log_info
                        student_metadata.append({
                            "mid": mid, 
                            "type": "refine", 
                            "opt_type": "neighbor_imitation", 
                            "log": log_info 
                        })
            
            else:
                # éä¸“å®¶æ¨¡å¼
                if meta['good_neighbors_text']:
                    prompt = summarize_experience_prompt(meta['base_text'], meta['good_neighbors_text'], cfg)
                    opt_type = "neighbor_imitation"
                    log_info["action"] = "IMITATION (No Expert)"
                else:
                    prompt = expand_low_freq_memory_prompt(meta['base_text'], "", cfg)
                    opt_type = "self_reflection"
                    log_info["action"] = "REFLECTION (No Expert)"
                
                log_info["student_prompt"] = prompt
                student_prompts.append(prompt)
                # ğŸ”¥ [ä¿®å¤ç‚¹ 6] åŠ ä¸Š "log": log_info
                student_metadata.append({
                    "mid": mid, 
                    "type": "refine", 
                    "opt_type": opt_type, 
                    "log": log_info 
                })

        # --- Step 5.4: æ‰¹é‡è°ƒç”¨å­¦ç”Ÿ (Student Execution) ---
        if student_prompts:
            print(f" ğŸš€ [Student-Batch] æ­£åœ¨æ‰§è¡Œ {len(student_prompts)} é¡¹ä¼˜åŒ–ä»»åŠ¡ (Refine/Replace/Create)...")
            
            # ========== ğŸ”¥ã€æ–°å¢æ‰“å° 1ã€‘æŸ¥çœ‹å­¦ç”Ÿé¢†åˆ°çš„ä»»åŠ¡å• ==========
            tee_print(f"   ğŸ“‹ å­¦ç”Ÿä»»åŠ¡è¯¦æƒ…:")
            for meta in student_metadata:
                # è·å–åŸè¯­ç±»å‹ (REFINE/REPLACE/EXPAND)
                action_type = meta.get("type", "UNKNOWN").upper()
                # è·å–æ›´å…·ä½“çš„ç­–ç•¥ (å¦‚ expert_refine, neighbor_imitation)
                strategy = meta.get("opt_type", "N/A")
                mid_display = meta['mid'][:8] # åªæ˜¾ç¤ºIDå‰8ä½æ–¹ä¾¿é˜…è¯»
                
                tee_print(f"   -> ğŸ†” [{mid_display}] | åŠ¨ä½œ: {action_type} | ç­–ç•¥: {strategy}")
                # å¦‚æœä½ æƒ³çœ‹å…·ä½“çš„ prompt å¼€å¤´ï¼Œå¯ä»¥æŠŠä¸‹é¢è¿™è¡Œæ³¨é‡Šæ‰“å¼€
                tee_print(f"      Prompté¢„è§ˆ: {meta.get('student_prompt', '')[:60]}...")
            tee_print("   ------------------------------------------------")
            # =======================================================

            # è¿™é‡Œå»ºè®®ç”¨ call_llm_batch (å­¦ç”Ÿæ¨¡å‹)ï¼Œå¦‚æœä½ æƒ³ç”¨ä¸“å®¶æ¨¡å‹å†™ä¹Ÿå¯ä»¥ç»´æŒ call_expert_batch
            outputs = call_llm_batch(student_prompts, cfg)

            # ========== ğŸ”¥ã€æ–°å¢æ‰“å° 2ã€‘æŸ¥çœ‹å­¦ç”Ÿåˆç¨¿ ==========
            tee_print(f"\n   ğŸ“¨ [Student Output] æ”¶åˆ° {len(outputs)} æ¡åˆç¨¿:")
            for meta, raw_txt in zip(student_metadata, outputs):
                # ç¨å¾®æ¸…æ´—ä¸€ä¸‹æ¢è¡Œç¬¦ä»¥ä¾¿åœ¨ä¸€è¡Œæ˜¾ç¤º
                clean_preview = raw_txt.strip().replace('\n', ' ')
                mid = meta['mid'][:8]
                tee_print(f"      -> ğŸ†” {mid} | é•¿åº¦: {len(raw_txt)} | é¢„è§ˆ: {clean_preview[0:20]}...{clean_preview[-20:]}")
            # =================================================

            min_len = int(getattr(cfg.optimizer, "min_memory_len", 20) or 20)
            max_len = int(getattr(cfg.optimizer, "max_memory_len", 2000) or 2000)

            candidates = []
            for meta, raw_output in zip(student_metadata, outputs):
                out_text = _extract_single_memory(raw_output)
                # out_text = raw_output
                candidates.append({
                    "meta": meta,
                    "attempts": [{"out_text": out_text, "judge": {}}],
                })

            acc_enabled, max_retries = _get_acceptance_params(cfg)

            def _need_accept(meta: dict) -> bool:
                return bool(acc_enabled and meta.get("neg_text"))

            # Judge attempt-0 (batched)
            judge_items, judge_map = [], []
            for i, cand in enumerate(candidates):
                meta = cand["meta"]
                out_text = cand["attempts"][-1]["out_text"]
                if not out_text or not _basic_guard(out_text, min_len=min_len, max_len=max_len):
                    continue
                if _need_accept(meta):
                    judge_items.append({
                        "failed_queries": meta.get("neg_text",""),
                        "old_memory": meta.get("old_content",""),
                        "new_memory": out_text,
                    })
                    judge_map.append(i)

            judge_results = _acceptance_test_batch(cfg, judge_items)
            for idx, res in zip(judge_map, judge_results):
                candidates[idx]["attempts"][-1]["judge"] = res

            # Retry loop (rollback)
            for _retry_idx in range(max_retries):
                retry_prompts, retry_indices = [], []
                for i, cand in enumerate(candidates):
                    meta = cand["meta"]
                    if not _need_accept(meta):
                        continue

                    last = cand["attempts"][-1]
                    out_text = (last.get("out_text") or "").strip()
                    judge = last.get("judge") or {}

                    ok_guard = bool(out_text and _basic_guard(out_text, min_len=min_len, max_len=max_len))
                    ok_accept = (judge.get("verdict") == "PASS")

                    if ok_guard and ok_accept:
                        continue

                    if not ok_guard:
                        feedback = (
                            "Rejected by basic guard. Make it concise, atomic, and factual. "
                            "Add a 'Keywords:' line with retrieval terms."
                        )
                    else:
                        feedback = judge.get("feedback") or "Still missing key info; be more specific and include retrieval keywords."

                    orig_prompt = meta.get("student_prompt") or (meta.get("log", {}) or {}).get("student_prompt", "")
                    if not orig_prompt:
                        continue

                    retry_prompts.append(_build_retry_prompt(orig_prompt, out_text, feedback))
                    retry_indices.append(i)

                if not retry_prompts:
                    break

                # ========== ğŸ”¥ã€æ–°å¢æ‰“å° 3ã€‘æŸ¥çœ‹å›é€€é‡å†™çš„æƒ…å†µ ==========
                tee_print(f"\n   ğŸ”„ [Retry Round {_retry_idx + 1}] è£åˆ¤æ‰“å›äº† {len(retry_prompts)} æ¡ï¼Œæ­£åœ¨é‡å†™...")
                for idx_in_cand, prompt in zip(retry_indices, retry_prompts):
                    # è·å–ä¹‹å‰çš„è£åˆ¤æ„è§
                    last_attempt = candidates[idx_in_cand]["attempts"][-1]
                    
                    # ğŸ”¥ ä¿®å¤ç‚¹ï¼šä½¿ç”¨ 'or {}' å¤„ç† NoneType
                    judge_res = last_attempt.get("judge") or {}
                    
                    # ä¼˜åŒ–æ˜¾ç¤ºï¼šå¦‚æœæ²¡æœ‰è£åˆ¤è®°å½•ï¼Œè¯´æ˜æ˜¯ Guard æ‹¦æˆª
                    if not judge_res:
                        feedback = "Rejected by Basic Guard (Length/Format)"
                    else:
                        feedback = judge_res.get("feedback", "No feedback")
                        
                    target_mid = candidates[idx_in_cand]["meta"]["mid"][:8]
                    
                    # é˜²æ­¢ feedback å¤ªçŸ­å¯¼è‡´åˆ‡ç‰‡é‡å¤æ˜¾ç¤º
                    fb_display = feedback if len(feedback) < 100 else f"{feedback[:50]}...{feedback[-50:]}"
                    tee_print(f"      -> âŒ ID: {target_mid} | è£åˆ¤æ„è§: {fb_display}")
                # ======================================================
                
                retry_outs = call_llm_batch(retry_prompts, cfg)
                # ========== ğŸ”¥ã€æ–°å¢æ‰“å° 4ã€‘æŸ¥çœ‹å­¦ç”Ÿé‡å†™ç»“æœ ==========
                tee_print(f"   ğŸ“¨ [Retry Output] æ”¶åˆ° {len(retry_outs)} æ¡é‡å†™ç¨¿:")
                for i, raw_out in zip(retry_indices, retry_outs):
                    # è¿™é‡Œè¦é€šè¿‡ i åæŸ¥ ID
                    mid = candidates[i]["meta"]["mid"][:8]
                    clean_preview = raw_out.strip().replace('\n', ' ')
                    tee_print(f"      -> ğŸ†” {mid} | é•¿åº¦: {len(raw_out)} | é¢„è§ˆ: {clean_preview[:20]}...{clean_preview[-20:]}")
                # ====================================================
                for i, raw_out in zip(retry_indices, retry_outs):
                    out_text = _extract_single_memory(raw_out)
                    candidates[i]["attempts"].append({"out_text": out_text, "judge": None})

                judge_items, judge_map = [], []
                for i in retry_indices:
                    meta = candidates[i]["meta"]
                    out_text = candidates[i]["attempts"][-1]["out_text"]
                    if out_text and _basic_guard(out_text, min_len=min_len, max_len=max_len):
                        judge_items.append({
                            "failed_queries": meta.get("neg_text",""),
                            "old_memory": meta.get("old_content",""),
                            "new_memory": out_text,
                        })
                        judge_map.append(i)

                judge_results = _acceptance_test_batch(cfg, judge_items)
                
                # ========== ğŸ”¥ã€æ–°å¢æ‰“å° 5ã€‘æŸ¥çœ‹é‡å†™åçš„è£åˆ¤ç»“æœ ==========
                if judge_results:
                    tee_print(f"   âš–ï¸ [Retry Judgment] æ”¶åˆ° {len(judge_results)} æ¡é‡å®¡ç»“æœ:")

                for idx, res in zip(judge_map, judge_results):
                    # 1. ä¿å­˜ç»“æœï¼ˆåŸé€»è¾‘ï¼‰
                    candidates[idx]["attempts"][-1]["judge"] = res
                    
                    # 2. æ‰“å°æ—¥å¿—ï¼ˆæ–°å¢é€»è¾‘ï¼‰
                    mid = candidates[idx]["meta"]["mid"][:8]
                    verdict = res.get("verdict", "UNKNOWN")
                    feedback = res.get("feedback", "No feedback")
                    
                    # å›¾æ ‡å’Œæˆªæ–­
                    icon = "âœ…" if verdict == "PASS" else "âŒ"
                    fb_prev = feedback if len(feedback) < 50 else f"{feedback[:50]}...{feedback[-50:]}"
                    
                    tee_print(f"      -> {icon} ID: {mid} | ç»“æœ: {verdict} | æ„è§: {fb_prev}")
                # ========================================================

            # Commit accepted changes + log
            with open(log_file_path, "a", encoding="utf-8") as log_f:
                for cand in candidates:
                    meta = cand["meta"]
                    info = meta.get("log", {}) or {}
                    target_mid = meta.get("mid")
                    task_type = meta.get("type")

                    # Choose last PASS attempt if needed
                    chosen = None
                    if _need_accept(meta):
                        for att in reversed(cand["attempts"]):
                            txt = (att.get("out_text") or "").strip()
                            j = att.get("judge") or {}
                            if txt and _basic_guard(txt, min_len=min_len, max_len=max_len) and j.get("verdict") == "PASS":
                                chosen = att
                                break
                    if chosen is None:
                        chosen = cand["attempts"][-1]

                    chosen_text = (chosen.get("out_text") or "").strip()
                    chosen_judge = chosen.get("judge") or {}

                    accepted = False
                    if chosen_text and _basic_guard(chosen_text, min_len=min_len, max_len=max_len):
                        if _need_accept(meta):
                            accepted = (chosen_judge.get("verdict") == "PASS")
                        else:
                            accepted = True

                    # # Logging
                    # log_f.write("\n" + "=" * 40 + "\n")
                    # log_f.write(f"ğŸ†” Memory ID: {info.get('mid', target_mid)} | Type: {task_type}\n")
                    # if info.get("expert_prompt"):
                    #     log_f.write(f"--- ğŸ§  Expert Prompt ---\n{info.get('expert_prompt','')}\n\n")
                    # if info.get("expert_output"):
                    #     log_f.write(f"--- ğŸ—£ï¸ Expert Output ---\n{info.get('expert_output','')}\n\n")
                    # if info.get("action") or info.get("gradient"):
                    #     log_f.write(f"--- ğŸ“¦ Parsed Action ---\nPrimitive: {info.get('action','')}\nGradient: {info.get('gradient','')}\n\n")
                    # if info.get("student_prompt"):
                    #     log_f.write(f"--- ğŸ“ Student Prompt ---\n{info.get('student_prompt','')}\n\n")
                    # for k, att in enumerate(cand["attempts"]):
                    #     log_f.write(f"--- âœ¨ Attempt {k} Output ---\n{(att.get('out_text') or '')}\n\n")
                    #     j = att.get("judge")
                    #     if j:
                    #         log_f.write(f"--- âœ… Acceptance (Attempt {k}) ---\nVerdict: {j.get('verdict')}\nFeedback: {j.get('feedback')}\n\n")
                    # log_f.write(f"--- ğŸ§¾ Final Decision ---\nAccepted: {accepted}\n")
                    # log_f.write("=" * 40 + "\n")
                    # log_f.flush()

                    if not accepted:
                        continue

                    if task_type in ["refine", "replace"]:
                        if target_mid in memories:
                            memories[target_mid]["contents"] = chosen_text
                            memories[target_mid]["cluster_id"] = -1
                            memories[target_mid]["opt_type"] = meta.get("opt_type", "textgrad")
                            optimized_ids.add(target_mid)

                    elif task_type == "create":
                        print(f"  âœ¨ [EXPAND] æ­£åœ¨åˆ†è£‚äº§ç”Ÿæ–°è®°å¿† ID: {target_mid[:8]}...")
                        memories[target_mid] = {
                            "id": target_mid,
                            "contents": chosen_text,
                            "cluster_id": -1,
                            "opt_type": meta.get("opt_type", "textgrad_expand"),
                            "parent_id": meta.get("parent_mid"),
                        }
                        memory_stats[target_mid] = {
                            "alpha": 1.0,
                            "beta": 1.0,
                            "neg_queries": [],
                            "pos_queries": [],
                        }
                        optimized_ids.add(target_mid)
    return optimized_ids