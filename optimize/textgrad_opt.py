# Hydra
import hydra
from omegaconf import DictConfig
from tools.optimize.callllm import init_llm, call_llm_batch
from tools.optimize.callexpert import init_expert_llm, call_expert,call_expert_batch
from tools.optimize.memoryload import load_clustered_memories, load_cluster_summary
from optimize.prompt_generate import generate_gradient_prompt,apply_gradient_prompt,summarize_experience_prompt,expand_low_freq_memory_prompt

def textgrad_opt(cfg, memories, memory_stats, cluster_to_ids, bad_ids, to_delete_ids):
    """
    ğŸ”¥ [Step 5] æ‰§è¡Œ TextGrad æ‰¹é‡ä¼˜åŒ–é€»è¾‘
    åŒ…å«ï¼šæ¢¯åº¦è®¡ç®— (Expert) -> æ¢¯åº¦åº”ç”¨/æ¨¡ä»¿/åæ€ (Student)
    
    Args:
        cfg: Hydra é…ç½®å¯¹è±¡
        memories: è®°å¿†åº“å­—å…¸ (ä¼šè¢«åŸåœ°ä¿®æ”¹)
        memory_stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        cluster_to_ids: èšç±»åå‘ç´¢å¼•
        bad_ids: å¾…ä¼˜åŒ–çš„ä½åˆ† ID åˆ—è¡¨
        to_delete_ids: å·²ç»è¢«æ ‡è®°åˆ é™¤çš„ ID é›†åˆ (ç”¨äºè¿‡æ»¤)
        
    Returns:
        set: æœ¬è½®è¢«æˆåŠŸä¼˜åŒ–çš„ ID é›†åˆ
    """
    print("\n========== TextGrad è®°å¿†ä¿®æ­£é˜¶æ®µ (Expert Batch Guided) ==========")
    
    # 1. è¿‡æ»¤å‡ºçœŸæ­£éœ€è¦å¤„ç†çš„ ID
    low_expand_ids = [mid for mid in bad_ids if mid in memories and mid not in to_delete_ids]
    print(f"ğŸ¯ å¾…ä¼˜åŒ–ç›®æ ‡æ•°é‡: {len(low_expand_ids)}")

    batch_size = cfg.optimizer.llm_batch_size
    optimized_ids = set()

    # 2. æŒ‰ Chunk éå†å¤„ç†
    for i in range(0, len(low_expand_ids), batch_size):
        chunk_ids = low_expand_ids[i : i + batch_size]
        
        # --- Step 5.1: å‡†å¤‡ä¸“å®¶ Prompts ---
        grad_prompts = []
        grad_metadata = [] 
        
        for mid in chunk_ids:
            rec = memories[mid]
            base_text = rec.get("contents", "")
            cluster_id = rec.get("cluster_id")
            stats = memory_stats.get(mid, {})
            neg_queries = stats.get('neg_queries', [])
            
            # æ‰¾ä¼˜ç­‰ç”Ÿ (Momentum)
            good_neighbors_text = []
            if cluster_id is not None:
                cluster_id = int(cluster_id)
                members = cluster_to_ids.get(cluster_id, [])
                for m_id in members:
                    if str(m_id) == mid: continue
                    s = memory_stats.get(str(m_id), {})
                    s_total = s.get('alpha', 0) + s.get('beta', 0)
                    if s_total > 0 and (s.get('alpha', 0)/s_total) > 0.8:
                        good_neighbors_text.append(memories[str(m_id)].get("contents", ""))
                good_neighbors_text = good_neighbors_text[:3]
            good_examples_str = "\n".join([f"- {t}" for t in good_neighbors_text])
            
            # åˆ¤æ–­æ˜¯å¦éœ€è¦ä¸“å®¶ä»‹å…¥
            if len(neg_queries) > 0:
                prompt = generate_gradient_prompt(base_text, neg_queries, cfg) # æ³¨æ„è¿™é‡Œæˆ‘åŠ äº† cfgï¼ŒåŸå‡½æ•°å¦‚æœéœ€è¦çš„è¯
                grad_prompts.append(prompt)
                grad_metadata.append({
                    "mid": mid, 
                    "need_expert": True,
                    "base_text": base_text,
                    "good_examples_str": good_examples_str,
                    "good_neighbors_text": good_neighbors_text, 
                    "err_count": len(neg_queries)
                })
            else:
                grad_metadata.append({
                    "mid": mid, 
                    "need_expert": False,
                    "base_text": base_text,
                    "good_neighbors_text": good_neighbors_text,
                    "good_examples_str": good_examples_str,
                })

        # --- Step 5.2: æ‰¹é‡è°ƒç”¨ä¸“å®¶ ---
        gradients = []
        if grad_prompts:
            print(f" ğŸ§  [Expert-Batch] æ­£åœ¨åˆ†æ {len(grad_prompts)} æ¡æ¢¯åº¦...")
            # æ³¨æ„ï¼šç¡®ä¿ call_expert_batch åœ¨è¿™ä¸ªä½œç”¨åŸŸå¯ç”¨
            gradients = call_expert_batch(grad_prompts, cfg)
        
        # --- Step 5.3: å‡†å¤‡å­¦ç”Ÿ Prompts ---
        student_prompts = []
        student_metadata = []
        grad_idx = 0 
        
        for meta in grad_metadata:
            mid = meta['mid']
            opt_type = "unknown"
            prompt = ""
            
            if meta['need_expert']:
                gradient_text = gradients[grad_idx] if grad_idx < len(gradients) else None
                grad_idx += 1
                
                if gradient_text:
                    prompt = apply_gradient_prompt(meta['base_text'], gradient_text, meta['good_examples_str'], cfg)
                    opt_type = f"textgrad_{meta['err_count']}_errors"
                else:
                    # é™çº§ç­–ç•¥
                    if meta['good_neighbors_text']:
                        prompt = summarize_experience_prompt(meta['base_text'], meta['good_neighbors_text'], cfg)
                        opt_type = "neighbor_imitation"
                    else:
                        prompt = expand_low_freq_memory_prompt(meta['base_text'], "", cfg)
                        opt_type = "self_reflection"
            else:
                if meta['good_neighbors_text']:
                    prompt = summarize_experience_prompt(meta['base_text'], meta['good_neighbors_text'], cfg)
                    opt_type = "neighbor_imitation"
                else:
                    prompt = expand_low_freq_memory_prompt(meta['base_text'], "", cfg)
                    opt_type = "self_reflection"
            
            student_prompts.append(prompt)
            student_metadata.append({"mid": mid, "opt_type": opt_type})

        # --- Step 5.4: æ‰¹é‡è°ƒç”¨å­¦ç”Ÿ ---
        if student_prompts:
            print(f" ğŸš€ [Student-Batch] æ­£åœ¨ä¼˜åŒ– {len(student_prompts)} æ¡è®°å¿†...")
            # å‡è®¾ä½ ç”¨åŒä¸€ä¸ªå‡½æ•°è°ƒç”¨ LLMï¼Œæˆ–è€…è¿™é‡Œåº”è¯¥æ˜¯ call_student_batch
            outputs = call_expert_batch(student_prompts, cfg) 
            
            for meta, output_text in zip(student_metadata, outputs):
                if output_text and len(output_text) > 10:
                    mid = meta['mid']
                    # åŸåœ°ä¿®æ”¹ memories
                    memories[mid]["contents"] = output_text
                    memories[mid]["opt_type"] = meta['opt_type']
                    optimized_ids.add(mid)
                    
    return optimized_ids