import os
import re
import uuid
import logging
from typing import Set, List, Dict
from omegaconf import DictConfig

# å·¥å…·åº“å¯¼å…¥
from tools.optimize.callllm import call_llm_batch
from tools.optimize.callllm import init_llm  # å¦‚æœéœ€è¦é‡æ–°åˆå§‹åŒ–
from tools.optimize.callexpert import call_expert_batch
from utils.memorywrap import parse_memory


def evolve_high_score_opt(cfg: DictConfig, memories: Dict, memory_stats: Dict, high_ids: List[str]) -> Set[str]:
    """
    ğŸ† [Step 4.5] é«˜åˆ†è®°å¿†è¿›åŒ–é˜¶æ®µ (Ace Evolution)
    
    ç­–ç•¥ï¼š
        1. ç­›é€‰æœ‰é”™é¢˜çš„é«˜åˆ†è®°å¿†ã€‚
        2. ä¸“å®¶è¯Šæ–­ï¼šIGNORE (å¿½ç•¥), SUPPLEMENT (è¡¥å……), SPLIT (åˆ†è£‚)ã€‚
        3. å­¦ç”Ÿæ‰§è¡Œï¼šç”Ÿæˆæ–°çš„è®°å¿†å†…å®¹ã€‚
        4. å†™å…¥æ—¥å¿—ä¸è®°å¿†åº“ã€‚
    
    Args:
        cfg: Hydraé…ç½®
        memories: è®°å¿†åº“ (In-place modification)
        memory_stats: ç»Ÿè®¡ä¿¡æ¯ (In-place modification)
        high_ids: é«˜åˆ†è®°å¿† ID åˆ—è¡¨
        
    Returns:
        Set[str]: æ–°ç”Ÿæˆçš„è®°å¿† ID é›†åˆ
    """
    print("\n========== é«˜åˆ†è®°å¿†è¿›åŒ–é˜¶æ®µ (Ace Evolution) ==========")
    
    # --- 1. ç¯å¢ƒä¸æ—¥å¿—å‡†å¤‡ ---
    # å¼ºåˆ¶æŒ‡å®šæ—¥å¿—è·¯å¾„
    log_file_path = cfg.paths.get("highfreq_textgrad_log", "textgrad_debug_log.txt")
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    try:
        os.makedirs(os.path.dirname(log_file_path), exist_ok=True)
    except Exception as e:
        print(f"âš ï¸ æ— æ³•åˆ›å»ºæ—¥å¿—ç›®å½•: {e}")

    print(f"ğŸ“ è¿›åŒ–æ—¥å¿—å°†è¿½åŠ è‡³: {log_file_path}")

    # --- 2. ç›®æ ‡ç­›é€‰ (Target Selection) ---
    target_ids = []
    for mid in high_ids:
        # é˜²å¾¡æ€§æ£€æŸ¥ï¼šç¡®ä¿IDå­˜åœ¨ä¸”æœ‰ç»Ÿè®¡æ•°æ®
        if mid not in memories: continue
        stats = memory_stats.get(mid, {})
        neg_queries = stats.get('neg_queries', [])
        
        # åªæœ‰å½“å­˜åœ¨é”™é¢˜æ—¶ï¼Œæ‰éœ€è¦è¿›åŒ–
        if len(neg_queries) > 0:
            target_ids.append(mid)

    # è°ƒè¯•æˆªæ–­é€»è¾‘ (Debug Limit)
    max_count = cfg.optimizer.get("max_high_opt_count", 5)
    if len(target_ids) > max_count:
        print(f"âœ‚ï¸ [Evolve] å‘½ä¸­é«˜åˆ†ä¼˜åŒ–ä¸Šé™: {len(target_ids)} -> {max_count}")
        target_ids = target_ids[:max_count]
    
    print(f"ğŸ’ å¾…è¿›åŒ–çš„ç‹ç‰Œè®°å¿†æ•°é‡: {len(target_ids)}")
    if target_ids:
        print(f"   ğŸ†” ID åˆ—è¡¨: {target_ids}")
    if not target_ids:
        return set()

    # --- 3. å‡†å¤‡ä¸“å®¶ Prompt (Expert Phase) ---
    expert_prompts = []
    evolve_metadata = []
    
    # æ‰¹å¤„ç†å¤§å°æ§åˆ¶
    batch_size = cfg.optimizer.llm_batch_size
    new_created_ids_total = set()

    for i in range(0, len(target_ids), batch_size):
        chunk_ids = target_ids[i : i + batch_size]
        expert_prompts = []
        chunk_metadata = []

        print(f" ğŸ§  [Expert-Batch] æ­£åœ¨å¤„ç†ç¬¬ {i} - {i+len(chunk_ids)} æ¡é«˜åˆ†è®°å¿†...")

        for mid in chunk_ids:
            rec = memories[mid]
            base_text = rec.get("contents", "")
            stats = memory_stats.get(mid, {})
            neg_queries = stats.get('neg_queries', [])
            
            # å–å‰ K ä¸ªé”™é¢˜ï¼Œé¿å… Context Window çˆ†ç‚¸
            top_k_neg = 5
            neg_text = "\n".join([f"- {q}" for q in neg_queries[:top_k_neg]])
            
            # æ„é€  Prompt
            try:
                prompt = cfg.optimizer.prompts.high_grad_expert.format(
                    content=base_text,
                    neg_queries=neg_text
                )
            except Exception as e:
                print(f"âŒ Prompt æ ¼å¼åŒ–å¤±è´¥ (MID: {mid}): {e}")
                continue

            expert_prompts.append(prompt)
            chunk_metadata.append({
                "mid": mid,
                "base_text": base_text,
                "neg_text": neg_text,
                "expert_prompt_content": prompt
            })

        if not expert_prompts:
            continue

        # è°ƒç”¨ä¸“å®¶æ¨¡å‹
        expert_outputs = call_expert_batch(expert_prompts, cfg)

        # --- 4. è§£æå†³ç­–å¹¶åˆ†å‘ (Dispatch Phase) ---
        student_prompts = []
        student_tasks = [] # å­˜å‚¨å¾…æ‰§è¡Œçš„ä»»åŠ¡ä¿¡æ¯

        for meta, expert_resp in zip(chunk_metadata, expert_outputs):
            mid = meta['mid']
            
            # åˆå§‹åŒ–æ—¥å¿—å¯¹è±¡
            log_info = {
                "mid": mid,
                "type": "evolve_high_score",
                "expert_prompt": meta["expert_prompt_content"],
                "expert_output": expert_resp,
                "action": "UNKNOWN",
                "gradient": "N/A",
                "split_num": 0,
                "student_prompt": "N/A"
            }

            if not expert_resp:
                print(f"âš ï¸ ä¸“å®¶æ¨¡å‹è¿”å›ä¸ºç©º (MID: {mid})")
                continue

            # === æ­£åˆ™è§£æ ===
            # 1. æå– Action: \box{IGNORE} / \box{SUPPLEMENT} / \box{SPLIT}
            action_match = re.search(r'\\box\{(IGNORE|SUPPLEMENT|SPLIT)\}', expert_resp)
            action = action_match.group(1) if action_match else "IGNORE" # é»˜è®¤ä¿å®ˆç­–ç•¥ï¼šå¿½ç•¥
            
            # 2. æå– Gradient (å»ºè®®): \gradient{...}
            # ä½¿ç”¨ DOTALL åŒ¹é…è·¨è¡Œæ–‡æœ¬
            gradient_match = re.search(r'\\gradient\{(.*?)\}', expert_resp, re.DOTALL)
            gradient = gradient_match.group(1).strip() if gradient_match else "No specific advice provided."
            
            # 3. æå– Num (ä»… SPLIT): \num{...}
            num_match = re.search(r'\\num\{(\d+)\}', expert_resp)
            split_num = int(num_match.group(1)) if num_match else 1

            # æ›´æ–°æ—¥å¿—ä¿¡æ¯
            log_info["action"] = action
            log_info["gradient"] = gradient
            if action == "SPLIT":
                log_info["split_num"] = split_num

            # === ä»»åŠ¡åˆ†å‘ ===
            if action == "IGNORE":
                # ç›´æ¥è®°å½•æ—¥å¿—ï¼Œä¸è°ƒç”¨å­¦ç”Ÿ
                _write_log(log_file_path, log_info, "Skipped (IGNORE Action)")
                continue

            elif action == "SUPPLEMENT":
                # ç”Ÿæˆå•æ¡è¡¥å……è®°å¿†
                reconstruct_tpl = cfg.optimizer.prompts.appgrad_high_supplement
                s_prompt = reconstruct_tpl.format(content=meta['base_text'], gradient=gradient)
                log_info["student_prompt"] = s_prompt
                
                student_prompts.append(s_prompt)
                student_tasks.append({
                    "parent_mid": mid,
                    "action": "SUPPLEMENT",
                    "log": log_info
                })

            elif action == "SPLIT":
                reconstruct_tpl = cfg.optimizer.prompts.appgrad_high_split
                s_prompt = reconstruct_tpl.format(neg_text=meta['neg_text'], gradient=gradient)
                log_info["student_prompt"] = s_prompt
                student_prompts.append(s_prompt)
                student_tasks.append({
                    "parent_mid": mid,
                    "action": "SPLIT",
                    "log": log_info
                })

        # --- 5. å­¦ç”Ÿæ‰§è¡Œä¸ç»“æœä¿å­˜ (Student Phase) ---
        if student_prompts:
            print(f" ğŸš€ [Student-Batch] æ­£åœ¨æ‰§è¡Œ {len(student_prompts)} é¡¹è¿›åŒ–ä»»åŠ¡...")
            student_outputs = call_llm_batch(student_prompts, cfg)

            for task, raw_output in zip(student_tasks, student_outputs):
                output_text = parse_memory(raw_output)
                log_info = task["log"]
                parent_mid = task["parent_mid"]
                action_type = task["action"]
                
                # ç»“æœå¤„ç†å®¹å™¨
                final_results_for_log = [] 

                if action_type == "SUPPLEMENT":
                    if output_text and len(output_text) > 10:
                        new_id = str(uuid.uuid4())
                        _save_new_memory(memories, memory_stats, new_id, output_text, parent_mid, "high_score_supplement")
                        new_created_ids_total.add(new_id)
                        final_results_for_log.append(f"[ID: {new_id}] {output_text[:100]}...")
                        print(f"  âœ¨ [SUPPLEMENT] ä¸º {parent_mid[:8]} å¢åŠ å‰¯å®˜: {new_id[:8]}")
                    else:
                        final_results_for_log.append("FAILED: Output too short.")

                elif action_type == "SPLIT":
                    # æŒ‰ç…§åˆ†éš”ç¬¦åˆ‡åˆ†
                    delimiter = "==========SPLIT=========="
                    raw_splits = output_text.split(delimiter)
                    # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
                    valid_splits = [s.strip() for s in raw_splits if len(s.strip()) > 10]
                    
                    if valid_splits:
                        print(f"  âœ¨ [SPLIT] è®°å¿† {parent_mid[:8]} åˆ†è£‚å‡º {len(valid_splits)} æ¡æ–°çŸ¥è¯†")
                        for idx, content in enumerate(valid_splits):
                            new_id = str(uuid.uuid4())
                            _save_new_memory(memories, memory_stats, new_id, content, parent_mid, f"high_score_split_{idx+1}")
                            new_created_ids_total.add(new_id)
                            final_results_for_log.append(f"[ID: {new_id}] {content[:100]}...")
                    else:
                        final_results_for_log.append("FAILED: No valid splits found.")

                # --- 6. å†™å…¥æ—¥å¿— (Write Log) ---
                # å°†æœ€ç»ˆç”Ÿæˆçš„è®°å¿†å†…å®¹æ‘˜è¦åˆå¹¶å†™å…¥æ—¥å¿—
                result_summary = "\n".join(final_results_for_log)
                _write_log(log_file_path, log_info, result_summary)

    print(f"âœ… [Evolve] è¿›åŒ–å®Œæˆï¼Œå…±æ–°å¢ {len(new_created_ids_total)} æ¡é«˜é˜¶è®°å¿†")
    return new_created_ids_total


# ------------------------------------------------------------------------------
# å†…éƒ¨ç§æœ‰è¾…åŠ©å‡½æ•° (ä¿æŒä¸»é€»è¾‘ç®€æ´)
# ------------------------------------------------------------------------------

def _save_new_memory(memories, memory_stats, new_id, content, parent_id, opt_type):
    """
    å°†æ–°ç”Ÿæˆçš„è®°å¿†å®‰å…¨åœ°ä¿å­˜åˆ°å­—å…¸ä¸­ï¼Œå¹¶åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯ã€‚
    """
    # 1. ä¿å­˜åˆ°è®°å¿†åº“
    memories[new_id] = {
        "id": new_id,  # ğŸ”¥ æ ¸å¿ƒï¼šå¿…é¡»åŒ…å« ID å­—æ®µ
        "contents": content,
        "cluster_id": -1, # æ ‡è®°ä¸ºæœªèšç±»ï¼Œç­‰å¾…ä¸‹ä¸€è½®å¤„ç†
        "opt_type": opt_type,
        "parent_id": parent_id # è®°å½•è¡€ç¼˜å…³ç³»ï¼ˆå¯ç”¨äºåç»­è¿½æº¯ï¼‰
    }
    
    # 2. åˆå§‹åŒ–ç»Ÿè®¡ (ç»™ä¸€ä¸ªå…¬å¹³çš„åˆå§‹åˆ†ï¼Œæ¯”å¦‚ alpha=1, beta=1)
    memory_stats[new_id] = {
        "alpha": 1.0,
        "beta": 1.0,
        "neg_queries": [],
        "pos_queries": []
    }

def _write_log(file_path, info, result_content):
    """
    å°†å•æ¡å¤„ç†è®°å½•è¿½åŠ å†™å…¥åˆ° TXT æ–‡ä»¶ã€‚
    """
    try:
        with open(file_path, "a", encoding="utf-8") as f:
            log_entry = (
                f"\n{'='*60}\n"
                f"ğŸ•’ Processing Log: Ace Evolution\n"
                f"ğŸ†” Parent Memory ID: {info['mid']}\n"
                f"--- ğŸ§  Expert Prompt (Input) ---\n{info['expert_prompt']}\n\n"
                f"--- ğŸ—£ï¸ Expert Output (Raw) ---\n{info['expert_output']}\n\n"
                f"--- ğŸ“¦ Parsed Decision ---\n"
                f"   Action   : {info['action']}\n"
                f"   Gradient : {info['gradient']}\n"
                f"   Split Num: {info.get('split_num', 0)}\n\n"
                f"--- ğŸ“ Student Prompt ---\n{info['student_prompt']}\n\n"
                f"--- âœ¨ Final Result (New Memories) ---\n{result_content}\n"
                f"{'='*60}\n"
            )
            f.write(log_entry)
            f.flush()
    except Exception as e:
        print(f"âš ï¸ æ—¥å¿—å†™å…¥å¼‚å¸¸: {e}")