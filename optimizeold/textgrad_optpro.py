import os
import re
import uuid
from omegaconf import DictConfig
# å‡è®¾è¿™æ˜¯ä½ çš„å·¥å…·åº“è·¯å¾„
from tools.optimize.callllm import init_llm, call_llm_batch
from tools.optimize.callexpert import init_expert_llm, call_expert, call_expert_batch
from tools.optimize.memoryload import load_clustered_memories, load_cluster_summary
# ä¿ç•™åŸæœ‰å¯¼å…¥
from optimize.prompt_generate import summarize_experience_prompt, expand_low_freq_memory_prompt
from utils.memorywrap import parse_memory

def textgrad_opt(cfg, memories, memory_stats, cluster_to_ids, bad_ids, to_delete_ids):
    """
    ğŸ”¥ [Step 5] æ‰§è¡Œ TextGrad æ‰¹é‡ä¼˜åŒ–é€»è¾‘ (Expert Decision Guided)
    åŒ…å«ï¼šæ¢¯åº¦å†³ç­– (Expert Agent) -> åŸè¯­åˆ†å‘ (Refine/Expand/Replace) -> æ‰§è¡Œä¼˜åŒ– (Student)
    
    Args:
        cfg: Hydra é…ç½®å¯¹è±¡
        memories: è®°å¿†åº“å­—å…¸ (ä¼šè¢«åŸåœ°ä¿®æ”¹)
        memory_stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸ (éœ€è¦æ”¯æŒåŠ¨æ€æ–°å¢)
        cluster_to_ids: èšç±»åå‘ç´¢å¼•
        bad_ids: å¾…ä¼˜åŒ–çš„ä½åˆ† ID åˆ—è¡¨
        to_delete_ids: å·²ç»è¢«æ ‡è®°åˆ é™¤çš„ ID é›†åˆ (ç”¨äºè¿‡æ»¤)
        
    Returns:
        set: æœ¬è½®è¢«æˆåŠŸä¼˜åŒ–çš„ ID é›†åˆ
    """
    print("\n========== TextGrad è®°å¿†ä¿®æ­£é˜¶æ®µ (Expert Batch Guided with Primitives) ==========")
    log_file_path = cfg.paths.get("lowfreq_textgrad_log", "textgrad_debug_log.txt")
    print(f"ğŸ“ è°ƒè¯•æ—¥å¿—å°†å†™å…¥: {log_file_path}")
    # 1. è¿‡æ»¤å‡ºçœŸæ­£éœ€è¦å¤„ç†çš„ ID
    low_expand_ids = [mid for mid in bad_ids if mid in memories and mid not in to_delete_ids]
    print(f"ğŸ¯ å¾…ä¼˜åŒ–ç›®æ ‡æ•°é‡: {len(low_expand_ids)}")
    if low_expand_ids:
        print(f"   ğŸ†” ID åˆ—è¡¨: {low_expand_ids}")
    batch_size = cfg.optimizer.llm_batch_size
    optimized_ids = set()

    # 2. æŒ‰ Chunk éå†å¤„ç†
    for i in range(0, len(low_expand_ids), batch_size):
        chunk_ids = low_expand_ids[i : i + batch_size]
        
        # --- Step 5.1: å‡†å¤‡ä¸“å®¶ Prompts (Diagnosis Phase) ---
        grad_prompts = []
        grad_metadata = [] 
        
        for mid in chunk_ids:
            rec = memories[mid]
            base_text = rec.get("contents", "")
            cluster_id = rec.get("cluster_id")
            stats = memory_stats.get(mid, {})
            neg_queries = stats.get('neg_queries', [])
            
            # --- å¯»æ‰¾ä¼˜ç­‰ç”Ÿ (Momentum / Good Neighbors) ---
            good_neighbors_text = []
            if cluster_id is not None:
                cluster_id = int(cluster_id)
                members = cluster_to_ids.get(cluster_id, [])
                for m_id in members:
                    if str(m_id) == mid: continue
                    s = memory_stats.get(str(m_id), {})
                    s_total = s.get('alpha', 0) + s.get('beta', 0)
                    # åªæœ‰èƒœç‡ > 0.8 çš„æ‰ç®—å¥½æ¦œæ ·
                    if s_total > 0 and (s.get('alpha', 0)/s_total) > 0.8:
                        good_neighbors_text.append(memories[str(m_id)].get("contents", ""))
                good_neighbors_text = good_neighbors_text[:3]
            good_examples_str = "\n".join([f"- {t}" for t in good_neighbors_text])
            
            # --- æ„å»ºä¸“å®¶è¾“å…¥ ---
            # å¦‚æœæœ‰è´Ÿåé¦ˆï¼ˆé”™é¢˜ï¼‰ï¼Œåˆ™è¿›å…¥â€œä¸“å®¶å†³ç­–æ¨¡å¼â€
            if len(neg_queries) > 0:
                # è·å– Top-K é”™é¢˜
                top_k = cfg.optimizer.get("top_k_neg_queries", 3)
                selected_negs = neg_queries[:top_k]
                neg_text = "\n".join([f"- {q}" for q in selected_negs])
                
                # ğŸ”¥ ç›´æ¥ä½¿ç”¨ Config ä¸­çš„å†³ç­– Promptï¼Œè€Œä¸æ˜¯è°ƒç”¨å›ºå®šå‡½æ•°
                decision_prompt = cfg.optimizer.prompts.low_grad_expert.format(
                    content=base_text,
                    neg_queries=neg_text
                )
                
                grad_prompts.append(decision_prompt)
                grad_metadata.append({
                    "mid": mid, 
                    "need_expert": True,
                    "expert_prompt_content": decision_prompt,  # <--- ğŸ”¥ æ–°å¢ï¼šå­˜ä¸‹ä¸“å®¶Prompt
                    "base_text": base_text,
                    "neg_text": neg_text, # å­˜ä¸‹æ¥ï¼ŒREPLACE/EXPAND è¦ç”¨
                    "good_examples_str": good_examples_str,
                    "good_neighbors_text": good_neighbors_text, 
                    "err_count": len(neg_queries)
                })
            else:
                # æ²¡æœ‰é”™é¢˜ï¼Œåªæœ‰ä½åˆ†/ä½é¢‘ -> è¿›å…¥é™çº§æ¨¡å¼ (Imitation/Reflection)
                grad_metadata.append({
                    "mid": mid, 
                    "need_expert": False,
                    "base_text": base_text,
                    "good_neighbors_text": good_neighbors_text,
                    "good_examples_str": good_examples_str,
                })

        # --- Step 5.2: æ‰¹é‡è°ƒç”¨ä¸“å®¶ (Expert Execution) ---
        expert_outputs = []
        if grad_prompts:
            print(f" ğŸ§  [Expert-Batch] æ­£åœ¨åˆ†æ {len(grad_prompts)} æ¡æ¢¯åº¦å¹¶ç”Ÿæˆå†³ç­–...")
            expert_outputs = call_expert_batch(grad_prompts, cfg)
        
    # --- Step 5.3: è§£æå†³ç­–å¹¶åˆ†å‘å­¦ç”Ÿä»»åŠ¡ (Dispatch Phase) ---
        student_prompts = []
        student_metadata = [] # è®°å½• task ç±»å‹å’Œ ID
        expert_idx = 0 
        
        for meta in grad_metadata:
            mid = meta['mid']
            # åˆå§‹åŒ–æ—¥å¿—å¯¹è±¡
            log_info = {
                "mid": mid,
                "expert_prompt": meta.get("expert_prompt_content", "N/A"),
                "expert_output": "N/A",
                "action": "N/A",
                "gradient": "N/A",
                "student_prompt": ""
            }

            if meta['need_expert']:
                expert_resp = expert_outputs[expert_idx] if expert_idx < len(expert_outputs) else ""
                expert_idx += 1
                log_info["expert_output"] = expert_resp
                
                if expert_resp:
                    # æ­£åˆ™è§£æ
                    action_match = re.search(r'\\box\{(REFINE|EXPAND|REPLACE)\}', expert_resp)
                    advice_match = re.search(r'\\advice\{(.*?)\}', expert_resp, re.DOTALL)
                    
                    action = action_match.group(1) if action_match else "REFINE" 
                    gradient = advice_match.group(1).strip() if advice_match else expert_resp
                    
                    log_info["action"] = action
                    log_info["gradient"] = gradient
                    
                    # === åŸè¯­åˆ†å‘é€»è¾‘ ===
                    
                    # 1. REFINE (ä¼˜åŒ–)
                    if action == "REFINE":
                        reconstruct_tpl = cfg.optimizer.prompts.appgrad_low_refine
                        prompt = reconstruct_tpl.format(content=meta['base_text'], gradient=gradient)                       
                        log_info["student_prompt"] = prompt
                        student_prompts.append(prompt)
                        # ğŸ”¥ [ä¿®å¤ç‚¹ 1] åŠ ä¸Š "log": log_info
                        student_metadata.append({
                            "mid": mid, 
                            "type": "refine", 
                            "opt_type": "expert_refine", 
                            "log": log_info  # <--- å¿…é¡»åŠ è¿™ä¸ªï¼
                        })
                        
                    # 2. REPLACE (åˆ å¢/æ›¿æ¢)
                    elif action == "REPLACE":
                        reconstruct_tpl = cfg.optimizer.prompts.appgrad_low_replace
                        prompt = reconstruct_tpl.format(neg_queries=meta['neg_text'], gradient=gradient)
                        log_info["student_prompt"] = prompt
                        student_prompts.append(prompt)
                        # ğŸ”¥ [ä¿®å¤ç‚¹ 2] åŠ ä¸Š "log": log_info
                        student_metadata.append({
                            "mid": mid, 
                            "type": "replace", 
                            "opt_type": "expert_replace", 
                            "log": log_info 
                        })
                        
                    # 3. EXPAND (å¢åŠ )
                    elif action == "EXPAND":
                        # Task A
                        reconstruct_tpl = cfg.optimizer.prompts.appgrad_low_refine
                        prompt_a = reconstruct_tpl.format(content=meta['base_text'], gradient=gradient)    
                        
                        log_info_a = log_info.copy()
                        log_info_a["student_prompt"] = prompt_a
                        log_info_a["action"] = "EXPAND (Part A: Refine Old)"
                        
                        student_prompts.append(prompt_a)
                        # ğŸ”¥ [ä¿®å¤ç‚¹ 3] åŠ ä¸Š "log": log_info_a
                        student_metadata.append({
                            "mid": mid, 
                            "type": "refine", 
                            "opt_type": "expert_expand_old", 
                            "log": log_info_a 
                        })
                        
                        # Task B
                        new_id = str(uuid.uuid4())
                        reconstruct_tpl = cfg.optimizer.prompts.appgrad_low_replace
                        prompt_b = reconstruct_tpl.format(neg_queries=meta['neg_text'], gradient=gradient)
                        
                        log_info_b = log_info.copy()
                        log_info_b["student_prompt"] = prompt_b
                        log_info_b["mid"] = new_id
                        log_info_b["action"] = "EXPAND (Part B: Create New)"

                        student_prompts.append(prompt_b)
                        # ğŸ”¥ [ä¿®å¤ç‚¹ 4] åŠ ä¸Š "log": log_info_b
                        student_metadata.append({
                            "mid": new_id, 
                            "type": "create", 
                            "opt_type": "expert_expand_new", 
                            "log": log_info_b 
                        })
                        
                else:
                    # ä¸“å®¶å¤±è´¥å›é€€
                    if meta['good_neighbors_text']:
                        prompt = summarize_experience_prompt(meta['base_text'], meta['good_neighbors_text'], cfg)
                        log_info["action"] = "FALLBACK (Imitation)"
                        log_info["student_prompt"] = prompt
                        student_prompts.append(prompt)
                        # ğŸ”¥ [ä¿®å¤ç‚¹ 5] åŠ ä¸Š "log": log_info
                        student_metadata.append({
                            "mid": mid, 
                            "type": "refine", 
                            "opt_type": "neighbor_imitation", 
                            "log": log_info 
                        })
            
            else:
                # éä¸“å®¶æ¨¡å¼
                if meta['good_neighbors_text']:
                    prompt = summarize_experience_prompt(meta['base_text'], meta['good_neighbors_text'], cfg)
                    opt_type = "neighbor_imitation"
                    log_info["action"] = "IMITATION (No Expert)"
                else:
                    prompt = expand_low_freq_memory_prompt(meta['base_text'], "", cfg)
                    opt_type = "self_reflection"
                    log_info["action"] = "REFLECTION (No Expert)"
                
                log_info["student_prompt"] = prompt
                student_prompts.append(prompt)
                # ğŸ”¥ [ä¿®å¤ç‚¹ 6] åŠ ä¸Š "log": log_info
                student_metadata.append({
                    "mid": mid, 
                    "type": "refine", 
                    "opt_type": opt_type, 
                    "log": log_info 
                })

        # --- Step 5.4: æ‰¹é‡è°ƒç”¨å­¦ç”Ÿ (Student Execution) ---
        if student_prompts:
            print(f" ğŸš€ [Student-Batch] æ­£åœ¨æ‰§è¡Œ {len(student_prompts)} é¡¹ä¼˜åŒ–ä»»åŠ¡ (Refine/Replace/Create)...")
            # è¿™é‡Œå»ºè®®ç”¨ call_llm_batch (å­¦ç”Ÿæ¨¡å‹)ï¼Œå¦‚æœä½ æƒ³ç”¨ä¸“å®¶æ¨¡å‹å†™ä¹Ÿå¯ä»¥ç»´æŒ call_expert_batch
            outputs = call_llm_batch(student_prompts, cfg) 

            # [ä¿®æ”¹ç‚¹ 4] æ‰“å¼€æ–‡ä»¶å‡†å¤‡è¿½åŠ å†™å…¥ (Append Mode)
            with open(log_file_path, "a", encoding="utf-8") as log_f:
                for meta, raw_output in zip(student_metadata, outputs):
                    output_text = parse_memory(raw_output)
                    # --- ğŸ”¥ å†™å…¥æ—¥å¿—çš„æ ¸å¿ƒé€»è¾‘ ---
                    if "log" in meta:
                        info = meta["log"]
                        log_entry = (
                            f"\n{'='*40}\n"
                            f"ğŸ†” Memory ID: {info['mid']} | Type: {meta['type']}\n"
                            f"--- ğŸ§  Expert Prompt ---\n{info['expert_prompt']}\n\n"
                            f"--- ğŸ—£ï¸ Expert Output ---\n{info['expert_output']}\n\n"
                            f"--- ğŸ“¦ Parsed Action ---\nPrimitive: {info['action']}\nGradient: {info['gradient']}\n\n"
                            f"--- ğŸ“ Student Prompt ---\n{info['student_prompt']}\n\n"
                            f"--- âœ¨ New Memory Content ---\n{output_text}\n"
                            f"{'='*40}\n"
                        )
                        log_f.write(log_entry)
                        log_f.flush() # å¼ºåˆ¶åˆ·æ–°ç¼“å†²åŒºï¼Œé˜²æ­¢ç¨‹åºå´©æºƒä¸¢å¤±æ—¥å¿—
            
                    if output_text and len(output_text) > 10:
                        target_mid = meta['mid']
                        task_type = meta['type']
                        
                        if task_type in ["refine", "replace"]:
                            # åŸåœ°æ›´æ–° (Update)
                            if target_mid in memories:
                                memories[target_mid]["contents"] = output_text
                                memories[target_mid]["opt_type"] = meta['opt_type']
                                optimized_ids.add(target_mid)
                                
                        elif task_type == "create":
                            # æ–°å¢æ’å…¥ (Insert)
                            print(f"  âœ¨ [EXPAND] æ­£åœ¨åˆ†è£‚äº§ç”Ÿæ–°è®°å¿† ID: {target_mid[:8]}...")
                            memories[target_mid] = {
                                "id": target_mid,
                                "contents": output_text,
                                "cluster_id": -1, # æ–°è®°å¿†æš‚æ—¶æ¸¸ç¦»ï¼Œç­‰å¾…ä¸‹ä¸€è½®èšç±»åˆ†é…
                                "opt_type": meta['opt_type']
                            }
                            # é‡è¦ï¼šåˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯ï¼Œé¿å…ä¸‹ä¸€è½®æŠ¥é”™
                            memory_stats[target_mid] = {
                                "alpha": 1.0, 
                                "beta": 1.0, 
                                "neg_queries": []
                            }
                            optimized_ids.add(target_mid)

    return optimized_ids