import os
import json
import time
from typing import Dict, List, Tuple, Set
import numpy as np
import torch
from sentence_transformers import SentenceTransformer
# Hydra
import hydra
from omegaconf import DictConfig
from utils.toolfunction import clean_special_chars,has_cuda
from tools.optimize.callllm import init_llm,call_llm,call_llm_batch
from tools.optimize.memoryload import load_clustered_memories,load_cluster_summary,load_memory_freq
# ================= å…¨å±€å˜é‡ (ä¿æŒåŸé€»è¾‘) =================
GLOBAL_MODEL = None
GLOBAL_TOKENIZER = None
GLOBAL_SGLANG_CLIENT = None

# ===== é«˜é¢‘ & ä½é¢‘è®°å¿†çš„ LLM æ“ä½œ =====
def summarize_high_freq_prompt(group_texts: List[str], cfg: DictConfig) -> str:
    items_formatted = "\n".join(
        f"[{i+1}] {t}" for i, t in enumerate(group_texts)
    )
    template = cfg.optimizer.prompts.summarize_high_freq
    prompt = template.format(items_formatted=items_formatted)
    return prompt

def expand_low_freq_memory_prompt(text: str, good_examples: str, cfg: DictConfig) -> str:
    """æ„é€ ä½é¢‘è®°å¿†æ‰©å†™çš„ prompt"""
    template = cfg.optimizer.prompts.expand_low_freq
    prompt = template.format(text=text,good_examples = good_examples)
    
    return prompt


# =============== Embedding & ç›¸ä¼¼åº¦ ===============

def build_embeddings_for_memories(memories: Dict[str, dict], model_name: str) -> Dict[str, np.ndarray]:
    device = "cuda" if has_cuda() else "cpu"
    print(f"ğŸš€ æ­£åœ¨è®¡ç®—è®°å¿†å‘é‡ ({model_name}) on {device}...")
    model = SentenceTransformer(model_name, device=device)

    ids = list(memories.keys())
    texts = []
    for mid in ids:
        rec = memories[mid]
        text = rec.get("contents", "")
        texts.append(text)

    embeddings = model.encode(
        texts,
        batch_size=32,
        show_progress_bar=True,
        normalize_embeddings=True
    )
    id_to_emb = {mid: embeddings[i] for i, mid in enumerate(ids)}
    print(f"âœ… å‘é‡æ„å»ºå®Œæˆï¼Œå…± {len(id_to_emb)} æ¡")
    return id_to_emb


def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
    return float(np.dot(a, b))


# =============== é«˜é¢‘/ä½é¢‘é›†åˆé€‰æ‹© ===============

def select_high_low_ids(
    freq_map: Dict[str, int],
    top_k_high: int,
    bottom_k_low: int,
    low_freq_for_low_only: int = 1
):
    items = list(freq_map.items())
    # é«˜é¢‘ï¼šæŒ‰ freq é™åº
    sorted_desc = sorted(items, key=lambda x: -x[1])
    high_ids = []
    for mid, f in sorted_desc:
        if f < 2: 
            # ä¸€æ—¦åˆ†æ•°æ‰åˆ° 2 ä»¥ä¸‹ï¼Œåé¢çš„éƒ½ä¸çœ‹äº†ï¼Œç›´æ¥æˆªæ–­
            break
        high_ids.append(mid)
        if len(high_ids) >= top_k_high:
            break

    # ä½é¢‘ï¼šæŒ‰ freq å‡åº
    sorted_asc = sorted(items, key=lambda x: x[1])
    bad_ids = []
    for mid, f in sorted_asc:
        if f <= -1:
            bad_ids.append(mid)

    print(f"ğŸ”¥ é«˜é¢‘ anchor æ•°é‡: {len(high_ids)}")
    print(f"ğŸ§Š åˆ†æ•°å°äº-1çš„è®°å¿†æ•°é‡: {len(bad_ids)}ï¼ˆä¹‹åä¼šä¿®æ­£ï¼‰")
    return set(high_ids), set(bad_ids)

def summarize_experience_prompt(target_text: str, good_neighbors: List[str], cfg: DictConfig) -> str:
    """æ„é€ åˆ©ç”¨é«˜åˆ†é‚»å±…ä¿®æ­£ä½åˆ†è®°å¿†çš„ Prompt"""
    good_examples_text = "\n".join(
        f"[{i+1}] {t}" for i, t in enumerate(good_neighbors)
    )
    template = cfg.optimizer.prompts.expand_low_freq
    prompt = template.format(text=target_text, good_examples=good_examples_text)
    return prompt

# =============== ä¸»ä¼˜åŒ–é€»è¾‘ (Hydra Managed) ===============

@hydra.main(version_base=None, config_path="conf", config_name="config")
def optimize_memory(cfg: DictConfig):
    # 0. åˆå§‹åŒ– LLM
    init_llm(cfg)

    # 1. è¯»å…¥åŸºç¡€æ•°æ® (ä½¿ç”¨ config ä¸­çš„è·¯å¾„)
    cluster_file = cfg.paths.cluster_output
    summary_file = cfg.paths.cluster_summary
    freq_file = cfg.paths.freq_file
    output_file = cfg.paths.optimized_memory

    memories, id_order = load_clustered_memories(cluster_file)
    cluster_to_ids = load_cluster_summary(summary_file)
    freq_map = load_memory_freq(freq_file)

    if not memories:
        print("âŒ æ— æ³•åŠ è½½è®°å¿†æ•°æ®ï¼Œç¨‹åºé€€å‡ºã€‚")
        return

    # ä¸ºæ‰€æœ‰è®°å¿†è¡¥é½é¢‘æ¬¡
    for mid in memories.keys():
        freq_map.setdefault(mid, 0)

    # 2. é€‰å‡ºé«˜é¢‘ã€ä½é¢‘ã€0 é¢‘é›†åˆ (ä½¿ç”¨ config ä¸­çš„å‚æ•°)
    high_ids, bad_ids = select_high_low_ids(
        freq_map,
        top_k_high=cfg.optimizer.top_k_high,
        bottom_k_low=cfg.optimizer.bottom_k_low,
        low_freq_for_low_only=cfg.optimizer.low_freq_threshold
    )

    # 3. å‡†å¤‡å‘é‡
    id_to_emb = build_embeddings_for_memories(memories, cfg.model.embedding_name)

    # 4. é«˜é¢‘ï¼šç±»å†…æ¸…ç†ï¼ˆPruningï¼‰â€”â€” ä»…åˆ é™¤ä½åˆ†é‚»å±…ï¼Œä¸è°ƒç”¨ LLM
    print("\n========== é«˜é¢‘è®°å¿†ä¼˜åŒ–é˜¶æ®µ (Pruning Only: Delete Low Score Neighbors) ==========")
    to_delete_ids = set()

    # æŒ‰ç…§é¢‘æ¬¡ä»é«˜åˆ°ä½æ’åºï¼Œä¼˜å…ˆå¤„ç†é«˜åˆ† Anchor
    high_ids_sorted = sorted(list(high_ids), key=lambda x: -freq_map.get(x, 0))
    
    count_pruned = 0

    for anchor_id in high_ids_sorted:
        if anchor_id not in memories: continue
        # å¦‚æœ Anchor è‡ªå·±æœ¬èº«å°±åœ¨åˆ é™¤åˆ—è¡¨é‡Œï¼ˆè™½ç„¶é€»è¾‘ä¸Šé«˜åˆ†ä¸åº”è¯¥åœ¨ï¼‰ï¼Œè·³è¿‡
        if anchor_id in to_delete_ids: continue

        rec_anchor = memories[anchor_id]
        cluster_id = rec_anchor.get("cluster_id")
        if cluster_id is None: continue
        cluster_id = int(cluster_id)
        
        # è·å–åŒ Cluster çš„æ‰€æœ‰æˆå‘˜
        cluster_member_ids = [str(x) for x in cluster_to_ids.get(cluster_id, [])]
        if not cluster_member_ids: continue

        # ç­›é€‰å‡ºéœ€è¦â€œæ¸…ç†â€çš„é‚»å±…
        # æ¡ä»¶ï¼š
        # 1. ä¸æ˜¯ Anchor è‡ªå·±
        # 2. è¿˜æ²¡è¢«æ ‡è®°åˆ é™¤
        # 3. åˆ†æ•° < 1 (æ ¹æ®ä½ çš„è¦æ±‚ï¼šåˆ†æ•°å°äº1çš„å…¨éƒ¨åˆ æ‰)
        victims = []
        for mid in cluster_member_ids:
            if mid == anchor_id: continue
            if mid in to_delete_ids: continue
            
            # è·å–è¯¥é‚»å±…çš„åˆ†æ•°ï¼Œé»˜è®¤ä¸º 0
            score = freq_map.get(mid, 0)
            
            if score < 1:
                victims.append(mid)
        
        if not victims: continue

        print(f"ğŸ”¥ [Pruning] Anchor {anchor_id} (Score={freq_map[anchor_id]}) æ‰€åœ¨ Cluster {cluster_id} æ¸…ç†:")
        print(f"   >>> åˆ é™¤ {len(victims)} ä¸ªä½åˆ†é‚»å±… (Score < 1)")
        
        # æ‰§è¡Œåˆ é™¤æ ‡è®°
        for mid in victims:
            to_delete_ids.add(mid)
            count_pruned += 1
            # åªæœ‰å°‘é‡åˆ é™¤æ—¶å¯ä»¥æ‰“å°å‡ºæ¥çœ‹çœ‹ï¼Œå¤ªå¤šå°±ä¸æ‰“å°äº†
            if len(victims) <= 50:
                print(f"       - ğŸ—‘ï¸ Delete ID: {mid:<6} (Score: {freq_map.get(mid, 0)})")

    print(f"\nâœ¨ é«˜é¢‘ä¼˜åŒ–é˜¶æ®µç»“æŸï¼Œå…±æ¸…ç†äº† {count_pruned} æ¡ä½åˆ†å†—ä½™è®°å¿†ã€‚")
    # æ³¨æ„ï¼šè¿™é‡Œä¸å†æœ‰ batch_prompts æˆ– call_llm_batch çš„é€»è¾‘äº†

# 5. ä½é¢‘/è´Ÿåˆ†ï¼šåˆ©ç”¨ç±»å†…é«˜åˆ†â€œä¼˜ç­‰ç”Ÿâ€è¿›è¡Œä¿®æ­£
    print("\n========== ä½é¢‘/è´Ÿåˆ†è®°å¿†ä¿®æ­£é˜¶æ®µ (Correct with Top-5 Neighbors) ==========")

    # ç­›é€‰éœ€è¦å¤„ç†çš„ä½åˆ†è®°å¿† (åœ¨ memories ä¸­ä¸”æœªè¢«åˆ é™¤)
    low_expand_ids = [
        mid for mid in bad_ids
        if mid in memories and mid not in to_delete_ids
    ]
    print(f"ğŸ¥¶ éœ€è¦ä¿®æ­£çš„ä½é¢‘/è´Ÿåˆ†è®°å¿†æ¡ç›®æ•°: {len(low_expand_ids)}")

    batch_size = cfg.optimizer.llm_batch_size
    batch_prompts = []
    batch_metadata = [] # å­˜å…ƒæ•°æ®: (mid, åŸæ–‡, æ˜¯å¦ä½¿ç”¨äº†é‚»å±…ä¿®æ­£)

    for mid in low_expand_ids:
        rec = memories[mid]
        base_text = rec.get("contents", "")
        cluster_id = rec.get("cluster_id")
        
        # 1. å°è¯•å¯»æ‰¾ç±»å†…çš„é«˜åˆ†â€œä¼˜ç­‰ç”Ÿâ€
        good_neighbors_text = []
        if cluster_id is not None:
            cluster_id = int(cluster_id)
            members = cluster_to_ids.get(cluster_id, [])
            
            # ç­›é€‰æ¡ä»¶ï¼šScore >= 2 ä¸”ä¸æ˜¯è‡ªå·±
            candidates = []
            for m_id in members:
                m_id = str(m_id)
                if m_id == mid: continue
                if freq_map.get(m_id, 0) >= 2: # ğŸ”¥ æ ¸å¿ƒæ¡ä»¶ï¼šåªå­¦å¥½çš„
                    candidates.append(m_id)
            
            # å– Top-5 (æŒ‰åˆ†æ•°é™åº)
            candidates_sorted = sorted(candidates, key=lambda x: -freq_map.get(x, 0))
            top_k_candidates = candidates_sorted[:5]
            
            # è·å–æ–‡æœ¬
            for m_id in top_k_candidates:
                if m_id in memories:
                    good_neighbors_text.append(memories[m_id].get("contents", ""))

        # 2. æ ¹æ®æ˜¯å¦æ‰¾åˆ°â€œä¼˜ç­‰ç”Ÿâ€æ„å»º Prompt
        if good_neighbors_text:
            # Plan A: æœ‰ä¼˜ç­‰ç”Ÿå¸¦é£ -> ç»“åˆ Top-5 ä¿®æ­£
            prompt = summarize_experience_prompt(base_text, good_neighbors_text, cfg)
            use_neighbors = True
        else:
            # Plan B: æ•´ä¸ªèšç±»éƒ½åªæœ‰å®ƒè‡ªå·±æˆ–éƒ½å¾ˆçƒ‚ -> åªèƒ½è‡ªå·±è‡ªæˆ‘åæ€/æ‰©å†™ (å…œåº•)
            prompt = expand_low_freq_memory_prompt(base_text, good_examples = '' , cfg = cfg)
            use_neighbors = False
            
        batch_prompts.append(prompt)
        batch_metadata.append({
            "mid": mid,
            "use_neighbors": use_neighbors,
            "neighbor_count": len(good_neighbors_text)
        })

        # 3. å‡‘å¤Ÿ Batch æ‰§è¡Œ
        if len(batch_prompts) >= batch_size:
            print(f"ğŸš€ [Batch Execution] å¤„ç† {len(batch_prompts)} æ¡ä½åˆ†è®°å¿†...")
            outputs = call_llm_batch(batch_prompts, cfg)
            
            for meta, output_text in zip(batch_metadata, outputs):
                mid = meta['mid']
                if not output_text:
                    print(f"   âš ï¸ LLM è¿”å›ä¸ºç©ºï¼ŒID={mid} ä¿æŒä¸å˜")
                    continue
                
                rec = memories[mid]
                rec["contents"] = output_text
                
                if meta['use_neighbors']:
                    rec["opt_type"] = f"corrected_by_{meta['neighbor_count']}_neighbors"
                    # å¯ä»¥åœ¨æ—¥å¿—é‡Œæ ‡è®°ä¸€ä¸‹
                    # print(f"   âœ… ID {mid} å·²åˆ©ç”¨ {meta['neighbor_count']} ä¸ªé«˜åˆ†é‚»å±…ä¿®æ­£")
                else:
                    rec["opt_type"] = "self_expanded_fallback"

            batch_prompts = []
            batch_metadata = []

    # å¤„ç†å‰©ä½™çš„
    if batch_prompts:
        print(f"ğŸš€ [Batch Execution] å¤„ç†å‰©ä½™ {len(batch_prompts)} æ¡ä½åˆ†è®°å¿†...")
        outputs = call_llm_batch(batch_prompts, cfg)
        for meta, output_text in zip(batch_metadata, outputs):
            if not output_text: continue
            rec = memories[meta['mid']]
            rec["contents"] = output_text
            rec["opt_type"] = f"corrected_by_{meta['neighbor_count']}_neighbors" if meta['use_neighbors'] else "self_expanded_fallback"
    # 6. å†™å‡ºæ–°çš„è®°å¿†åº“
    print("\n========== å†™å‡ºä¼˜åŒ–åçš„è®°å¿†åº“ ==========")
    kept_count = 0
    with open(output_file, "w", encoding="utf-8") as f:
        for mid in id_order:
            if mid not in memories: continue
            if mid in to_delete_ids: continue
            f.write(json.dumps(memories[mid], ensure_ascii=False) + "\n")
            kept_count += 1

    print(f"âœ… æ–°è®°å¿†åº“å†™å…¥å®Œæˆ: {output_file}")
    print(f"   ä¿ç•™è®°å¿†æ¡ç›®: {kept_count}")
    print(f"   åˆ é™¤è®°å¿†æ¡ç›®: {len(to_delete_ids)}")

if __name__ == "__main__":
    optimize_memory()