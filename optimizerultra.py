import os
import json
from typing import Dict, List
# Hydra
import hydra
from omegaconf import DictConfig
from tools.optimize.callllm import init_llm, call_llm_batch
from tools.optimize.callexpert import init_expert_llm, call_expert,call_expert_batch
from tools.optimize.memoryload import load_clustered_memories, load_cluster_summary

# ==========================================
# 1. Prompt æ„é€ å‡½æ•° (TextGrad åŒé˜¶æ®µ)
# ==========================================

def generate_gradient_prompt(content: str, neg_queries: List[str]) -> str:
    """
    ğŸ”¥ [Step 1: Backward Pass - Expert]
    è¯·æ±‚ä¸“å®¶æ¨¡å‹è¿›è¡Œâ€œå½’å› åˆ†æâ€ï¼Œè®¡ç®—æ–‡æœ¬æ¢¯åº¦ã€‚
    """
    neg_text = "\n".join([f"- {q}" for q in neg_queries[:5]])
    prompt = f"""
You are a Senior Knowledge Engineer diagnosing a RAG system memory.

[Target Memory]
{content}

[Failure Cases]
The system used this memory to answer the following queries but failed:
{neg_text}

[Task: Calculate Gradient]
Analyze WHY this memory failed.
- Is it missing specific formulas or conditions?
- Is it ambiguous?
- Is it a "Hubness" problem (irrelevant but high similarity)?

Provide a concise **Improvement Instruction** (The Gradient).
Start with "To fix this, you should..."
"""
    return prompt

def apply_gradient_prompt(content: str, gradient: str, good_examples: str, cfg: DictConfig) -> str:
    """
    ğŸ”¥ [Step 2: Update Step - Student]
    è¯·æ±‚ Qwen æ ¹æ®ä¸“å®¶çš„æ¢¯åº¦é‡å†™è®°å¿†ã€‚
    """
    momentum_part = ""
    if good_examples:
        momentum_part = f"\n[Reference (Momentum)]\nHigh-quality neighbors:\n{good_examples}\n"

    # å°è¯•è¯»å– config é‡Œçš„æ¨¡æ¿ï¼Œå¦åˆ™ç”¨é»˜è®¤
    template = cfg.optimizer.prompts.apply_gradient
    return template.format(content=content, gradient=gradient, momentum_part=momentum_part)

def summarize_experience_prompt(target_text: str, good_neighbors: List[str], cfg: DictConfig) -> str:
    """æ—§é€»è¾‘ï¼šæ¨¡ä»¿"""
    good_examples_text = "\n".join(f"[{i+1}] {t}" for i, t in enumerate(good_neighbors))
    template = cfg.optimizer.prompts.expand_low_freq
    prompt = template.format(text=target_text, good_examples=good_examples_text)
    return prompt

def expand_low_freq_memory_prompt(text: str, good_examples: str, cfg: DictConfig) -> str:
    """æ—§é€»è¾‘ï¼šè‡ªçœ"""
    template = cfg.optimizer.prompts.expand_low_freq
    prompt = template.format(text=text, good_examples=good_examples)
    return prompt

# ==========================================
# 2. ç­›é€‰é€»è¾‘ (ä¿æŒä¸å˜)
# ==========================================
def select_ids_from_stats(memory_stats: Dict[str, dict], cfg: DictConfig):
    scores = []
    for mid, stats in memory_stats.items():
        alpha = stats.get('alpha', 1.0)
        beta = stats.get('beta', 1.0)
        total = alpha + beta
        win_rate = alpha / total if total > 0 else 0.5
        scores.append((mid, win_rate, total))
    
    scores.sort(key=lambda x: (-x[1], -x[2]))
    
    top_k = cfg.optimizer.top_k_high
    bottom_k = cfg.optimizer.bottom_k_low
    
    high_ids = [x[0] for x in scores[:top_k]]
    # ç­›é€‰æœ‰é”™è¯¯è®°å½•çš„ ID
    bad_ids = [x[0] for x in scores if x[1] < 0.4 and x[2] > 2]
    
    if len(bad_ids) < 10:
         bad_ids = [x[0] for x in scores[-bottom_k:]]

    print(f"ğŸ”¥ é«˜åˆ† Anchor: {len(high_ids)}")
    print(f"ğŸ¥¶ ä½åˆ† Candidates: {len(bad_ids)}")
    return set(high_ids), set(bad_ids)

# ==========================================
# 3. ä¸»ç¨‹åº
# ==========================================
@hydra.main(version_base=None, config_path="conf", config_name="config")
def optimize_memory(cfg: DictConfig):
    # 0. åˆå§‹åŒ–åŒæ¨¡å‹
    init_llm(cfg)          # å­¦ç”Ÿ (Qwen)
    init_expert_llm(cfg)   # ä¸“å®¶ (Gemini/GPT)

    # 1. è·¯å¾„
    cluster_file = cfg.paths.cluster_output
    summary_file = cfg.paths.cluster_summary
    stats_file = cfg.paths.stats_file
    output_file = cfg.paths.optimized_memory
    
    root_dir = cfg.paths.root
    corpus_name = cfg.experiment.get("corpus_dataset_name") or cfg.experiment.dataset_name
    corpus_tag = corpus_name.split('/')[-1]
    # ä¼˜åŒ–åçš„ Stats ä¿å­˜è·¯å¾„
    stats_optimized_file = cfg.paths.stats_optimized_file

    # 2. åŠ è½½æ•°æ®
    if not os.path.exists(stats_file):
        print(f"âŒ æ‰¾ä¸åˆ°çŠ¶æ€æ–‡ä»¶: {stats_file}")
        return
    with open(stats_file, 'r', encoding='utf-8') as f:
        memory_stats = json.load(f)

    memories, id_order = load_clustered_memories(cluster_file)
    cluster_to_ids = load_cluster_summary(summary_file)
    if not memories: return

    # 3. ç­›é€‰
    high_ids, bad_ids = select_ids_from_stats(memory_stats, cfg)

    # =========================================================
    # 4. Pruning (é«˜åˆ†å»å™ª)
    # =========================================================
    print("\n========== é«˜åˆ†è®°å¿†æ¸…ç†é˜¶æ®µ (Pruning) ==========")
    to_delete_ids = set() 
    
    cluster_groups = {}
    for mid, rec in memories.items():
        cid = rec.get("cluster_id")
        if cid is not None:
            cid = int(cid)
            if cid not in cluster_groups: cluster_groups[cid] = []
            cluster_groups[cid].append(mid)
    
    pruned_count = 0
    for cid, members in cluster_groups.items():
        if len(members) < 2: continue
        
        member_stats_list = []
        for mid in members:
            stats = memory_stats.get(mid, {'alpha': 1.0, 'beta': 1.0})
            total = stats['alpha'] + stats['beta']
            win_rate = stats['alpha'] / total if total > 0 else 0.5
            member_stats_list.append({'id': mid, 'win_rate': win_rate, 'total': total})
            
        member_stats_list.sort(key=lambda x: (-x['win_rate'], -x['total']))
        best_mem = member_stats_list[0]
        
        if best_mem['win_rate'] > 0.7 and best_mem['total'] > 2:
            for mem in member_stats_list[1:]:
                is_trash = False
                if mem['win_rate'] < 0.4 and mem['total'] > 2: is_trash = True
                if best_mem['win_rate'] > 0.9 and mem['win_rate'] < 0.5: is_trash = True
                if is_trash:
                    to_delete_ids.add(mem['id'])
                    pruned_count += 1
    print(to_delete_ids)
    print(f"âœ¨ Pruning å®Œæˆï¼Œåˆ é™¤: {pruned_count}")

# =========================================================
    # 5. TextGrad (ä¸“å®¶å½’å›  -> å­¦ç”Ÿä¿®æ­£) - Batch ä¼˜åŒ–ç‰ˆ
    # =========================================================
    print("\n========== TextGrad è®°å¿†ä¿®æ­£é˜¶æ®µ (Expert Batch Guided) ==========")
    low_expand_ids = [mid for mid in bad_ids if mid in memories and mid not in to_delete_ids]
    print(f"ğŸ¯ å¾…ä¼˜åŒ–ç›®æ ‡æ•°é‡: {len(low_expand_ids)}")

    # ä½¿ç”¨ config ä¸­çš„ batch size
    batch_size = cfg.optimizer.llm_batch_size
    
    # è®°å½•ä¼˜åŒ–çŠ¶æ€
    optimized_ids = set()

    # ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šæŒ‰ Chunk å¤„ç†ï¼Œå®ç°å…¨é“¾è·¯ Batch
    for i in range(0, len(low_expand_ids), batch_size):
        chunk_ids = low_expand_ids[i : i + batch_size]
        
        # --- Step 5.1: å‡†å¤‡ä¸“å®¶ Prompts (Gradient Calculation) ---
        grad_prompts = []
        grad_metadata = [] # å­˜ (mid, base_text, good_examples_str, neg_queries_len)
        
        # è¿™ä¸€æ­¥ä¸éœ€è¦è°ƒ LLMï¼Œåªæ˜¯æŸ¥è¡¨æ„å»º Prompt
        for mid in chunk_ids:
            rec = memories[mid]
            base_text = rec.get("contents", "")
            cluster_id = rec.get("cluster_id")
            stats = memory_stats.get(mid, {})
            neg_queries = stats.get('neg_queries', [])
            
            # æ‰¾ä¼˜ç­‰ç”Ÿ (Momentum)
            good_neighbors_text = []
            if cluster_id is not None:
                cluster_id = int(cluster_id)
                members = cluster_to_ids.get(cluster_id, [])
                for m_id in members:
                    if str(m_id) == mid: continue
                    s = memory_stats.get(str(m_id), {})
                    s_total = s.get('alpha', 0) + s.get('beta', 0)
                    if s_total > 0 and (s.get('alpha', 0)/s_total) > 0.8:
                        good_neighbors_text.append(memories[str(m_id)].get("contents", ""))
                good_neighbors_text = good_neighbors_text[:3]
            good_examples_str = "\n".join([f"- {t}" for t in good_neighbors_text])
            
            # åªæœ‰æœ‰é”™è¯¯è®°å½•çš„æ‰éœ€è¦ä¸“å®¶ä»‹å…¥
            if len(neg_queries) > 0:
                prompt = generate_gradient_prompt(base_text, neg_queries)
                grad_prompts.append(prompt)
                grad_metadata.append({
                    "mid": mid, 
                    "need_expert": True,
                    "base_text": base_text,
                    "good_examples_str": good_examples_str,
                    "good_neighbors_text": good_neighbors_text, # å¤‡ç”¨
                    "err_count": len(neg_queries)
                })
            else:
                # ä¸éœ€è¦ä¸“å®¶çš„ï¼Œæ ‡è®°ä¸€ä¸‹ï¼Œåé¢ç›´æ¥è¿›å­¦ç”Ÿ Prompt æ„é€ 
                grad_metadata.append({
                    "mid": mid, 
                    "need_expert": False,
                    "base_text": base_text,
                    "good_neighbors_text": good_neighbors_text,
                    "good_examples_str": good_examples_str, # ä¿æŒå¯¹é½
                })

        # --- Step 5.2: æ‰¹é‡è°ƒç”¨ä¸“å®¶ (Expert Batch) ---
        gradients = []
        if grad_prompts:
            print(f" ğŸ§  [Expert-Batch] æ­£åœ¨åˆ†æ {len(grad_prompts)} æ¡æ¢¯åº¦...")
            gradients = call_expert_batch(grad_prompts, cfg)
        
        # --- Step 5.3: å‡†å¤‡å­¦ç”Ÿ Prompts (Update Step) ---
        student_prompts = []
        student_metadata = []
        
        grad_idx = 0 # æ¸¸æ ‡ï¼Œç”¨äºä» gradients åˆ—è¡¨é‡Œå–ç»“æœ
        
        for meta in grad_metadata:
            mid = meta['mid']
            opt_type = "unknown"
            prompt = ""
            
            if meta['need_expert']:
                # è·å–åˆšæ‰ä¸“å®¶çš„è¾“å‡º
                gradient_text = gradients[grad_idx] if grad_idx < len(gradients) else None
                grad_idx += 1
                
                if gradient_text:
                    # æˆåŠŸæ‹¿åˆ°æ¢¯åº¦ -> TextGrad Update
                    prompt = apply_gradient_prompt(meta['base_text'], gradient_text, meta['good_examples_str'], cfg)
                    opt_type = f"textgrad_{meta['err_count']}_errors"
                else:
                    # ä¸“å®¶è°ƒç”¨å¤±è´¥ -> é™çº§ä¸º Imitation
                    if meta['good_neighbors_text']:
                        prompt = summarize_experience_prompt(meta['base_text'], meta['good_neighbors_text'], cfg)
                        opt_type = "neighbor_imitation"
                    else:
                        prompt = expand_low_freq_memory_prompt(meta['base_text'], "", cfg)
                        opt_type = "self_reflection"
            else:
                # ä¸éœ€è¦ä¸“å®¶ -> Imitation or Reflection
                if meta['good_neighbors_text']:
                    prompt = summarize_experience_prompt(meta['base_text'], meta['good_neighbors_text'], cfg)
                    opt_type = "neighbor_imitation"
                else:
                    prompt = expand_low_freq_memory_prompt(meta['base_text'], "", cfg)
                    opt_type = "self_reflection"
            
            student_prompts.append(prompt)
            student_metadata.append({"mid": mid, "opt_type": opt_type})

        # --- Step 5.4: æ‰¹é‡è°ƒç”¨å­¦ç”Ÿ (Student Batch) ---
        if student_prompts:
            print(f" ğŸš€ [Student-Batch] æ­£åœ¨ä¼˜åŒ– {len(student_prompts)} æ¡è®°å¿†...")
            outputs = call_expert_batch(student_prompts, cfg)
            
            # å›å¡«ç»“æœ
            for meta, output_text in zip(student_metadata, outputs):
                if output_text and len(output_text) > 10:
                    mid = meta['mid']
                    memories[mid]["contents"] = output_text
                    memories[mid]["opt_type"] = meta['opt_type']
                    optimized_ids.add(mid)

    # =========================================================
    # 6. å†™å‡ºæ–°è®°å¿†åº“
    # =========================================================
    print("\n========== å†™å‡ºä¼˜åŒ–åçš„è®°å¿†åº“ ==========")
    kept_count = 0
    with open(output_file, "w", encoding="utf-8") as f:
        for mid in id_order:
            if mid not in memories: continue
            if mid in to_delete_ids: continue
            f.write(json.dumps(memories[mid], ensure_ascii=False) + "\n")
            kept_count += 1
    print(f"âœ… è®°å¿†åº“å·²ä¿å­˜: {output_file}")

    # =========================================================
    # 7. çŠ¶æ€åŒæ­¥ (Clean & Reset)
    # =========================================================
    print("\n========== åŒæ­¥ BEMR çŠ¶æ€ (Stats Sync) ==========")
    
    # 1. ç§»é™¤å·²åˆ é™¤çš„ ID
    for del_id in to_delete_ids:
        if del_id in memory_stats:
            del memory_stats[del_id]
            
    # 2. é‡ç½®å·²ä¼˜åŒ–çš„ ID (Reset to Prior)
    for opt_id in optimized_ids:
        if opt_id in memory_stats:
            memory_stats[opt_id] = {
                'alpha': 1.0, 
                'beta': 1.0, 
                'pos_queries': [], 
                'neg_queries': [] # æ¸…ç©ºæ¢¯åº¦ï¼Œå› ä¸ºå·²ç»ä¿®å¥½äº†
            }
            
    print(f" Â  ğŸ—‘ï¸ å·²ä» Stats ä¸­ç§»é™¤ {len(to_delete_ids)} æ¡")
    print(f" Â  ğŸ”„ å·²é‡ç½® {len(optimized_ids)} æ¡ TextGrad ä¼˜åŒ–é¡¹")
    
    try:
        with open(stats_optimized_file, 'w', encoding='utf-8') as f:
            json.dump(memory_stats, f, ensure_ascii=False, indent=2)
        print(f"âœ… [BEMR] çŠ¶æ€å·²åŒæ­¥: {stats_optimized_file}")
    except Exception as e:
        print(f"âŒ ä¿å­˜ Stats å¤±è´¥: {e}")

if __name__ == "__main__":
    optimize_memory()