import os
import json
import re
import time
import torch
import bm25s
import logging
import ast
import collections
from datasets import load_dataset
from tqdm import tqdm
from huggingface_hub import snapshot_download
from flashrag.config import Config
from flashrag.pipeline import SequentialPipeline
from flashrag.utils import get_retriever, get_generator, Dataset
from flashrag.prompt import PromptTemplate
import matplotlib.pyplot as plt
import transformers

# å±è”½ transformers çš„å†—ä½™è­¦å‘Š
transformers.logging.set_verbosity_error()

# ==========================================
# ğŸ› ï¸ æ ¸å¿ƒé…ç½®åŒºåŸŸ
# ==========================================

# 1. å®éªŒæ§åˆ¶å¼€å…³
# è¿™é‡Œçš„ "å•çº¯æµ‹è¯•ä»£ç " é»˜è®¤åªè¿è¡Œ RAG æ¨¡å¼
EXPERIMENT_MODE = "rag" 

# 2. è®°å¿†åº“æ–‡ä»¶é…ç½® (æ ¸å¿ƒä¿®æ”¹)
# æŒ‡å®šå¤–éƒ¨ä¼˜åŒ–è¿‡çš„è®°å¿†åº“æ–‡ä»¶
MEMORY_SOURCE_FILE = "AMATH-lighteval_optimized_memory_k50.jsonl"

# 3. ç»“æœå¯è§†åŒ–å¼€å…³
VISUALIZE_MEMORY_DISTRIBUTION = True

# 4. è°ƒè¯•æ ·æœ¬æ•°
# None è¡¨ç¤ºè·‘å…¨é‡æµ‹è¯•é›†ï¼Œè®¾ç½®æ•°å­—(å¦‚ 10)å¯å¿«é€Ÿè°ƒè¯•
DEBUG_NUM = None

# 5. æ¨¡å‹è®¾ç½®
MODEL_SOURCE = "huggingface" 
HF_MODEL_NAME = "Qwen/Qwen3-4B-Instruct-2507" 
# HF_MODEL_NAME = "Qwen/Qwen2.5-7B-Instruct" # å¤‡é€‰æ¨è

# [Gemini é…ç½®]
GEMINI_MODEL_NAME = "gemini-2.5-flash"
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY") 

# ==========================================
# 6. æ•°æ®é›†é…ç½® (æµ‹è¯•é›†æ¥æº)
# ==========================================

DATASET_NAME = "DigitalLearningGmbH/MATH-lighteval" 
# MATH æ•°æ®é›†éœ€è¦æŒ‡å®šå­é›†ï¼Œ"all" è¡¨ç¤ºåŠ è½½ä»£æ•°ã€å‡ ä½•ç­‰æ‰€æœ‰ç±»åˆ«
DATASET_CONFIG = "algebra" 
SPLIT_TEST = "test"  
FIELD_MAP = {
    'question': 'problem',        
    'answer': 'solution' 
}
# ==========================================
# âš™ï¸ è‡ªåŠ¨è·¯å¾„ç”Ÿæˆ
# ==========================================
dataset_tag = DATASET_NAME.split('/')[-1]
# ä¸ºäº†é€‚é… FlashRAGï¼Œæˆ‘ä»¬å°†å¤–éƒ¨è®°å¿†åº“è½¬æ¢ä¸ºæ ‡å‡†çš„ corpus.jsonl æ ¼å¼
corpus_file = f"{dataset_tag}_custom_memory_corpus.jsonl"
test_file = f"{dataset_tag}_test_data.jsonl"
index_dir = f"{dataset_tag}_custom_memory_bm25_index"

timestamp = time.strftime("%Y%m%d_%H%M%S")
RESULT_LOG_FILE = f"{dataset_tag}_{MODEL_SOURCE}_{EXPERIMENT_MODE}_{timestamp}.txt"
VIS_IMAGE_FILE = f"memory_distribution_{timestamp}.png"
MEM_FREQ_JSONL_FILE = f"{dataset_tag}_memory_freq_{timestamp}.jsonl"

# ==========================================
# 1. æ•°æ®å‡†å¤‡æ¨¡å— (å·²ä¿®æ”¹ï¼šè¯»å–æŒ‡å®šè®°å¿†æ–‡ä»¶)
# ==========================================
def prepare_data():
    print(f"ğŸ“¥ [Step 1] æ­£åœ¨åŠ è½½æµ‹è¯•æ•°æ®é›†: {DATASET_NAME} (Config: {DATASET_CONFIG})...")
    try:
        if DATASET_CONFIG:
            dataset = load_dataset(DATASET_NAME, DATASET_CONFIG)
        else:
            dataset = load_dataset(DATASET_NAME)
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return False

    q_col = FIELD_MAP['question']
    a_col = FIELD_MAP['answer']

    # --- A. é€‚é…è®°å¿†åº“ (è¯»å– MATH_optimized_memory_k30.jsonl) ---
    # å¦‚æœä¸å­˜åœ¨è½¬æ¢åçš„ corpus æ–‡ä»¶ï¼Œåˆ™è¿›è¡Œè½¬æ¢
    with open(MEMORY_SOURCE_FILE, "r", encoding="utf-8") as fin, open(corpus_file, "w", encoding="utf-8") as fout:       
        count = 0
        for line in tqdm(fin, desc="Converting Memory"):
            try:
                item = json.loads(line)
                # è¿™é‡Œçš„ item ç»“æ„: {"id": "2", "question": "...\nAnswer:...", "cluster_id": ...}
                # FlashRAG éœ€è¦ "contents" å­—æ®µç”¨äºæ£€ç´¢
                # ç›´æ¥ä½¿ç”¨ question å­—æ®µï¼ˆå› ä¸ºå®ƒåŒ…å«äº†é—®é¢˜å’Œç­”æ¡ˆï¼‰
                new_item = {
                    "id": str(item.get("id")),
                    "contents": item.get("question", ""),
                    # ä¿ç•™å…¶ä»–å…ƒæ•°æ®ä»¥å¤‡ä¸æ—¶ä¹‹éœ€ï¼ˆå¯é€‰ï¼‰
                    "cluster_id": item.get("cluster_id"),
                    "cluster_label": item.get("cluster_label") 
                }
                fout.write(json.dumps(new_item) + "\n")
                count += 1
            except json.JSONDecodeError:
                continue
        print(f"âœ… è®°å¿†åº“è½¬æ¢å®Œæˆï¼Œå…±å¤„ç† {count} æ¡è®°å¿†ã€‚")

    # --- B. å‡†å¤‡æµ‹è¯•é›† (ä¿æŒä¸å˜) ---
    print(f"ğŸ”¨ [Test] æ­£åœ¨æå–æµ‹è¯•é›† (æ ·æœ¬æ•°: {DEBUG_NUM if DEBUG_NUM else 'ALL'})...")
    with open(test_file, "w", encoding="utf-8") as f:
        if SPLIT_TEST not in dataset:
             print(f"âŒ é”™è¯¯: æ•°æ®é›†æ²¡æœ‰ {SPLIT_TEST} åˆ’åˆ†ï¼")
             return False
             
        test_data = dataset[SPLIT_TEST]
        if DEBUG_NUM:
            limit = min(DEBUG_NUM, len(test_data))
            test_data = test_data.select(range(limit))
            
        for i, item in enumerate(test_data):
            q_text = item.get(q_col, "")
            raw_ans = item.get(a_col, "") 
            
            f.write(json.dumps({
                "id": str(i),
                "question": q_text,
                "golden_answers": [str(raw_ans)] 
            }) + "\n")
    return True

# ==========================================
# 2. ç´¢å¼•æ„å»ºæ¨¡å— (BM25)
# ==========================================
def build_index():
    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å·²ç»å­˜åœ¨ä¸”åŒ¹é…
    if os.path.exists(index_dir) and os.path.exists(os.path.join(index_dir, "vocab.tokenizer.json")):
        print(f"âœ… [Index] ç´¢å¼•ç›®å½•å·²å­˜åœ¨: {index_dir}ï¼Œè·³è¿‡æ„å»ºã€‚")
        # æ³¨æ„ï¼šå¦‚æœæ›´æ¢äº†è®°å¿†æ–‡ä»¶ï¼Œå»ºè®®æ‰‹åŠ¨åˆ é™¤ index æ–‡ä»¶å¤¹ä»¥å¼ºåˆ¶é‡å»º
        return

    print(f"ğŸ”¨ [Index] æ­£åœ¨ä¸º {corpus_file} æ„å»º BM25 ç´¢å¼•...")
    corpus_texts = []
    
    # è¯»å–è½¬æ¢åçš„æ ‡å‡† corpus æ–‡ä»¶
    with open(corpus_file, "r", encoding="utf-8") as f:
        for line in f:
            corpus_texts.append(json.loads(line)['contents'])
    
    corpus_tokens = bm25s.tokenize(corpus_texts)
    retriever_builder = bm25s.BM25()
    retriever_builder.index(corpus_tokens)
    retriever_builder.save(index_dir)
    
    with open(os.path.join(index_dir, "stopwords.tokenizer.json"), "w") as f:
        json.dump([], f)
    with open(os.path.join(index_dir, "vocab.tokenizer.json"), "w") as f:
        vocab = corpus_tokens.vocab
        json.dump({"word_to_id": vocab, "stem_to_sid": vocab, "word_to_stem": {k: k for k in vocab}}, f)
    print("âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼")

# ==========================================
# 3. Gemini ç”Ÿæˆå™¨ç±»
# ==========================================
class GeminiGenerator:
    def __init__(self, model_name, api_key):
        import google.generativeai as genai
        if not api_key:
            raise ValueError("âŒ æœªæ£€æµ‹åˆ° API Keyï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ GEMINI_API_KEY")
        
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model_name)
        print(f"ğŸ¤– Gemini Generator ({model_name}) å·²åŠ è½½")
        
        self.max_input_len = 30000 

    def generate(self, input_list, **kwargs):
        responses = []
        for prompt in input_list:
            try:
                if isinstance(prompt, list): prompt = " ".join(prompt)
                clean_prompt = str(prompt)
                result = self.model.generate_content(clean_prompt)
                if result.parts:
                    responses.append(result.text)
                else:
                    responses.append("Error: Empty Response (Safety Block)")
                time.sleep(1) 
            except Exception as e:
                print(f"âš ï¸ Gemini API Error: {e}")
                time.sleep(2)
                responses.append("Error")
        return responses

# ==========================================
# 4. è¯„ä¼°å·¥å…·
# ==========================================
def extract_math_answer(text):
    if not text:
        return None
    text = str(text)

    # ç­–ç•¥ 1: æ ‡å‡† \boxed{...} æå–
    idx = text.rfind("\\boxed{")
    if idx != -1:
        content_start = idx + 7 
        balance = 0
        for i in range(content_start, len(text)):
            char = text[i]
            if char == '{':
                balance += 1
            elif char == '}':
                if balance == 0:
                    return text[content_start:i] 
                balance -= 1
    
    # ç­–ç•¥ 2: æå–æœ€åä¸€è¡Œ
    lines = text.strip().split('\n')
    if lines:
        last_line = lines[-1].strip()
        last_line = re.sub(r'^(The )?Answer( is)?:?', '', last_line, flags=re.IGNORECASE).strip()
        if len(last_line) < 50: 
            return last_line

    return None

def normalize_latex(s):
    if not s: return ""
    s = str(s)
    s = "".join(s.split())
    s = s.replace("\\dfrac", "\\frac")
    s = s.replace("\\text", "")
    return s.strip()

def evaluate_results(results, experiment_name):
    correct = 0
    total = len(results)
    
    with open(RESULT_LOG_FILE, "a", encoding="utf-8") as f:
        header = f"\n{'='*20} {experiment_name} {'='*20}\n"
        print(header.strip())
        f.write(header)
        
        for i, item in enumerate(results):
            pred = item.pred if hasattr(item, 'pred') else item['pred']
            gold_raw = item.golden_answers[0] if hasattr(item, 'golden_answers') else item['golden_answers'][0]
            question = item.question if hasattr(item, 'question') else item['question']

            gold_val = extract_math_answer(gold_raw) or str(gold_raw).strip()
            pred_val = extract_math_answer(pred)
            
            is_right = False
            if gold_val and pred_val:
                norm_gold = normalize_latex(gold_val)
                norm_pred = normalize_latex(pred_val)
                if norm_gold == norm_pred:
                    is_right = True

            if is_right:
                correct += 1

            log_entry = (
                f"\n[ID]: {i}\n"
                f"[Question]: {str(question)[:100]}...\n"
                f"[Gold]: {gold_val}\n"
                f"[Pred]: {pred_val}\n"
                f"[Result]: {'âœ… Correct' if is_right else 'âŒ Wrong'}\n"
                f"{'-'*30}\n"
            )
            f.write(log_entry)
            if i < 5: print(log_entry.strip())

        acc = correct / total * 100 if total > 0 else 0
        summary = (
            f"\nğŸ“Š ç»Ÿè®¡ ({experiment_name}):\n"
            f"Total: {total}, Correct: {correct}, Accuracy: {acc:.2f}%\n"
            f"{'='*50}\n"
        )
        print(summary)
        f.write(summary)
    return acc


# def extract_last_number(text):
#     """
#     ä¸“é—¨ç”¨äº GSM8K çš„ç­”æ¡ˆæå–é€»è¾‘ã€‚
#     1. ä¼˜å…ˆå¯»æ‰¾ '####' æ ‡è®°ï¼Œå–å…¶åå†…å®¹ã€‚
#     2. å¦‚æœæ²¡æœ‰æ ‡è®°ï¼Œä½¿ç”¨æ­£åˆ™æå–æ–‡æœ¬ä¸­çš„æœ€åä¸€ä¸ªæ•°å­—ã€‚
#     """
#     text = str(text)
    
#     # ç­–ç•¥ 1: æ ‡å‡† GSM8K æ ¼å¼åˆ†å‰² (####)
#     if "####" in text:
#         text = text.split("####")[-1]
    
#     # ç­–ç•¥ 2: æ­£åˆ™æå–æ•°å­— (æ”¯æŒæ•´æ•°ã€æµ®ç‚¹æ•°ã€ç§»é™¤é€—å·)
#     # åŒ¹é…æ¨¡å¼: è´Ÿå·å¯é€‰ï¼Œæ•°å­—ï¼Œå¯èƒ½æœ‰é€—å·ï¼Œå¯èƒ½æœ‰å°æ•°ç‚¹
#     # ä¾‹å¦‚: -1,234.56
#     text = text.replace(',', '') # å»æ‰åƒåˆ†ä½é€—å·
#     matches = re.findall(r'-?\d+(?:\.\d+)?', text)
    
#     if matches:
#         return float(matches[-1]) # è¿”å›æœ€åä¸€ä¸ªæ•°å­—
#     return None

# def evaluate_results(results, experiment_name):
#     correct = 0
#     total = len(results)
    
#     with open(RESULT_LOG_FILE, "a", encoding="utf-8") as f:
#         header = f"\n{'='*20} {experiment_name} {'='*20}\n"
#         print(header.strip())
#         f.write(header)
        
#         for i, item in enumerate(results):
#             pred = item.pred if hasattr(item, 'pred') else item['pred']
#             gold_raw = item.golden_answers[0] if hasattr(item, 'golden_answers') else item['golden_answers'][0]
#             question = item.question if hasattr(item, 'question') else item['question']

#             # --- 1. æå– Gold Answer (æ ‡å‡†æ•°å€¼) ---
#             # GSM8K æ•°æ®é›†ä¸­çš„ gold_raw åŒ…å« "æ¨ç†è¿‡ç¨‹ #### ç­”æ¡ˆ"
#             gold_val = extract_last_number(gold_raw)

#             # --- 2. æå– Prediction Answer (é¢„æµ‹æ•°å€¼) ---
#             pred_val = extract_last_number(pred)
            
#             # --- 3. å¯¹æ¯”åˆ¤æ–­ ---
#             is_right = False
#             if gold_val is not None and pred_val is not None:
#                 # æµ®ç‚¹æ•°å¯¹æ¯”ï¼Œå®¹å·® 1e-6
#                 if abs(gold_val - pred_val) < 1e-6:
#                     is_right = True
            
#             if is_right:
#                 correct += 1

#             log_entry = (
#                 f"\n[ID]: {i}\n"
#                 f"[Question]: {question}\n"
#                 f"[Gold Raw]: ...{str(gold_raw)[-50:]} => [Extracted]: {gold_val}\n"
#                 f"[Pred Raw]: ...{str(pred)[-50:].replace(chr(10), ' ')} => [Extracted]: {pred_val}\n"
#                 f"[Result]: {'âœ… Correct' if is_right else 'âŒ Wrong'}\n"
#                 f"{'-'*30}\n"
#             )
#             f.write(log_entry)
#             if i < 10: print(log_entry.strip()) # åªæ‰“å°å‰å‡ ä¸ªé˜²æ­¢åˆ·å±

#         acc = correct / total * 100
#         summary = (
#             f"\nğŸ“Š ç»Ÿè®¡ ({experiment_name}):\n"
#             f"Total: {total}, Correct: {correct}, Accuracy: {acc:.2f}%\n"
#             f"{'='*50}\n"
#         )
#         print(summary)
#         f.write(summary)
#     return acc

# ==========================================
# ğŸ”¥ [é‡æ„ç‰ˆ] è®°å¿†è°ƒç”¨é¢‘æ¬¡åˆ†æ (å«å…¨é‡ç»Ÿè®¡ & å ä½ç¬¦)
# ==========================================
def analyze_memory_usage(rag_results):
    print("\nğŸ” [Analysis] æ­£åœ¨è¿›è¡Œå…¨é‡è®°å¿†çƒ­åº¦ç»Ÿè®¡...")
    
    # -------------------------------------------------------
    # 1. å»ºç«‹å…¨é‡è®°å¿†è´¦æœ¬ (åˆå§‹åŒ–æ‰€æœ‰ ID ä¸º 0)ï¼ŒåŒæ—¶ä¿å­˜å†…å®¹
    # -------------------------------------------------------
    all_memory_ids = set()
    id_to_content = {}  # æ–°å¢ï¼šè®°å½•æ¯æ¡è®°å¿†çš„åŸå§‹æ–‡æœ¬

    try:
        with open(corpus_file, "r", encoding="utf-8") as f:
            for line in f:
                item = json.loads(line)
                mid = str(item['id'])
                all_memory_ids.add(mid)
                # è®°ä½è¿™æ¡è®°å¿†çš„å†…å®¹ï¼Œæ–¹ä¾¿åé¢å†™å…¥ jsonl
                id_to_content[mid] = item.get("contents", "")
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è¯»å–è®°å¿†åº“æ–‡ä»¶ {corpus_file}ï¼Œå°†ä»…ç»Ÿè®¡è¢«æ£€ç´¢åˆ°çš„è®°å¿†ã€‚é”™è¯¯: {e}")
    
    # åˆå§‹åŒ–è®¡æ•°å™¨ï¼Œæ‰€æœ‰å·²çŸ¥ ID é»˜è®¤ä¸º 0
    memory_counter = collections.Counter({mid: 0 for mid in all_memory_ids})
    
    # -------------------------------------------------------
    # 2. ç»Ÿè®¡å®é™…æ£€ç´¢å‘½ä¸­
    # -------------------------------------------------------
    for item in rag_results:
        retrieved_docs = getattr(item, 'retrieval_result', [])
        
        for doc in retrieved_docs:
            if isinstance(doc, dict):
                doc_id = str(doc.get('id'))
            else:
                doc_id = str(getattr(doc, 'id', None))
                
            if doc_id:
                memory_counter[doc_id] += 1

    # -------------------------------------------------------
    # 3. æ’åº (æŒ‰é¢‘æ¬¡é™åº -> ID å‡åº)
    # -------------------------------------------------------
    sorted_memories = sorted(memory_counter.items(), key=lambda x: (-x[1], x[0]))
    
    total_memories = len(sorted_memories)
    used_memories = sum(1 for _, v in sorted_memories if v > 0)
    unused_memories = total_memories - used_memories
    
    print(f"ğŸ“Š è®°å¿†åº“æ€»é‡: {total_memories}")
    print(f"ğŸ”¥ è¢«æ¿€æ´»çš„è®°å¿†: {used_memories} ({(used_memories/total_memories)*100:.2f}%)")
    print(f"ğŸ§Š æ²‰ç¡çš„è®°å¿†(0æ¬¡): {unused_memories}")

    # -------------------------------------------------------
    # 4. âœ… æ–°å¢ï¼šå¯¼å‡ºæŒ‰é¢‘æ¬¡æ’åºçš„ jsonl
    # -------------------------------------------------------
    try:
        print(f"ğŸ’¾ [Save] æ­£åœ¨å¯¼å‡ºè®°å¿†è°ƒç”¨é¢‘æ¬¡æ’åºç»“æœåˆ°: {MEM_FREQ_JSONL_FILE}")
        with open(MEM_FREQ_JSONL_FILE, "w", encoding="utf-8") as f:
            for rank, (mid, freq) in enumerate(sorted_memories, start=1):
                record = {
                    "rank": rank,
                    "memory_id": mid,
                    "freq": int(freq),
                    "contents": id_to_content.get(mid, "")
                }
                f.write(json.dumps(record, ensure_ascii=False) + "\n")
        print("âœ… è°ƒç”¨é¢‘æ¬¡ jsonl å¯¼å‡ºå®Œæˆï¼")
    except Exception as e:
        print(f"âŒ å¯¼å‡º {MEM_FREQ_JSONL_FILE} å¤±è´¥: {e}")

    # -------------------------------------------------------
    # 5. å¯è§†åŒ–é€»è¾‘ (Top 30 ... Bottom 30)
    # -------------------------------------------------------
    if VISUALIZE_MEMORY_DISTRIBUTION:
        print(f"ğŸ¨ [Visual] æ­£åœ¨ç”Ÿæˆé¢‘æ¬¡åˆ†å¸ƒå›¾: {VIS_IMAGE_FILE}")
        try:
            ids = [m[0] for m in sorted_memories]
            counts = [m[1] for m in sorted_memories]
            
            display_limit = 30
            
            if len(ids) > display_limit * 2:
                print(f"â„¹ï¸ å±•ç¤ºç­–ç•¥: Top {display_limit} + å ä½ç¬¦ + Bottom {display_limit}")
                
                head_ids = ids[:display_limit]
                head_counts = counts[:display_limit]
                
                tail_ids = ids[-display_limit:]
                tail_counts = counts[-display_limit:]
                
                plot_ids = head_ids + ["..."] + tail_ids
                plot_counts = head_counts + [0] + tail_counts
                
                colors = ['skyblue'] * len(head_ids) + ['white'] + ['salmon'] * len(tail_ids)
                edge_colors = ['navy'] * len(head_ids) + ['white'] + ['darkred'] * len(tail_ids)
            else:
                plot_ids = ids
                plot_counts = counts
                colors = 'skyblue'
                edge_colors = 'navy'

            plt.figure(figsize=(15, 6))
            bars = plt.bar(plot_ids, plot_counts, color=colors, edgecolor=edge_colors)
            
            plt.title(f'Memory Usage Distribution (Top {display_limit} vs Bottom {display_limit})', fontsize=14)
            plt.xlabel('Memory ID', fontsize=12)
            plt.ylabel('Frequency', fontsize=12)
            
            plt.xticks(rotation=90, fontsize=8) 
            plt.grid(axis='y', linestyle='--', alpha=0.5)
            
            for i, bar in enumerate(bars):
                height = bar.get_height()
                if plot_ids[i] != "...": 
                    plt.text(
                        bar.get_x() + bar.get_width()/2., height,
                        f'{int(height)}',
                        ha='center', va='bottom', fontsize=8
                    )
            
            plt.tight_layout()
            plt.savefig(VIS_IMAGE_FILE, dpi=300)
            print("âœ… å›¾ç‰‡ä¿å­˜æˆåŠŸï¼")
            
        except ImportError:
            print("âŒ ç¼ºå°‘ matplotlib")
            
    else:
        print("\nğŸ† [Top 10 Hot Memories]")
        for mid, count in sorted_memories[:10]:
            print(f"   ID: {mid:<5} | Count: {count}")
            
        print("\nğŸ§Š [Bottom 10 Cold Memories]")
        for mid, count in sorted_memories[-10:]:
             print(f"   ID: {mid:<5} | Count: {count}")


# ==========================================
# 5. è®°å¿†çƒ­åº¦ç»Ÿè®¡
# ==========================================
def analyze_memory_usage(rag_results):
    print("\nğŸ” [Analysis] æ­£åœ¨è¿›è¡Œå…¨é‡è®°å¿†çƒ­åº¦ç»Ÿè®¡...")
    
    all_memory_ids = set()
    id_to_content = {} 

    # è¯»å–è½¬æ¢åçš„ corpus æ–‡ä»¶ä»¥å»ºç«‹åŸºå‡†
    try:
        with open(corpus_file, "r", encoding="utf-8") as f:
            for line in f:
                item = json.loads(line)
                mid = str(item['id'])
                all_memory_ids.add(mid)
                id_to_content[mid] = item.get("contents", "")
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è¯»å–è®°å¿†åº“æ–‡ä»¶ {corpus_file}ï¼Œç»Ÿè®¡å¯èƒ½ä¸å®Œæ•´ã€‚é”™è¯¯: {e}")
    
    memory_counter = collections.Counter({mid: 0 for mid in all_memory_ids})
    
    for item in rag_results:
        retrieved_docs = getattr(item, 'retrieval_result', [])
        for doc in retrieved_docs:
            if isinstance(doc, dict):
                doc_id = str(doc.get('id'))
            else:
                doc_id = str(getattr(doc, 'id', None))
                
            if doc_id:
                memory_counter[doc_id] += 1

    sorted_memories = sorted(memory_counter.items(), key=lambda x: (-x[1], x[0]))
    
    total_memories = len(sorted_memories)
    used_memories = sum(1 for _, v in sorted_memories if v > 0)
    
    print(f"ğŸ“Š è®°å¿†åº“æ€»é‡: {total_memories}")
    print(f"ğŸ”¥ è¢«æ¿€æ´»çš„è®°å¿†: {used_memories} ({(used_memories/total_memories)*100:.2f}%)")

    # å¯¼å‡º Jsonl
    try:
        with open(MEM_FREQ_JSONL_FILE, "w", encoding="utf-8") as f:
            for rank, (mid, freq) in enumerate(sorted_memories, start=1):
                record = {
                    "rank": rank,
                    "memory_id": mid,
                    "freq": int(freq),
                    "contents": id_to_content.get(mid, "")
                }
                f.write(json.dumps(record, ensure_ascii=False) + "\n")
        print(f"ğŸ’¾ é¢‘æ¬¡ç»Ÿè®¡å·²å¯¼å‡º: {MEM_FREQ_JSONL_FILE}")
    except Exception as e:
        print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")

    # ç”»å›¾
    if VISUALIZE_MEMORY_DISTRIBUTION:
        try:
            ids = [m[0] for m in sorted_memories]
            counts = [m[1] for m in sorted_memories]
            
            display_limit = 30
            if len(ids) > display_limit * 2:
                plot_ids = ids[:display_limit] + ["..."] + ids[-display_limit:]
                plot_counts = counts[:display_limit] + [0] + counts[-display_limit:]
                colors = ['skyblue'] * display_limit + ['white'] + ['salmon'] * display_limit
                edge_colors = ['navy'] * display_limit + ['white'] + ['darkred'] * display_limit
            else:
                plot_ids = ids
                plot_counts = counts
                colors = 'skyblue'
                edge_colors = 'navy'

            plt.figure(figsize=(15, 6))
            plt.bar(plot_ids, plot_counts, color=colors, edgecolor=edge_colors)
            plt.title(f'Memory Usage (Source: {MEMORY_SOURCE_FILE})', fontsize=14)
            plt.xticks(rotation=90, fontsize=8) 
            plt.tight_layout()
            plt.savefig(VIS_IMAGE_FILE, dpi=300)
            print(f"âœ… åˆ†å¸ƒå›¾å·²ä¿å­˜: {VIS_IMAGE_FILE}")
        except:
            pass

# ==========================================
# 6. ä¸»ç¨‹åº
# ==========================================
def main():
    if os.path.exists(RESULT_LOG_FILE): os.remove(RESULT_LOG_FILE)
    print(f"ğŸ“ ç»“æœæ—¥å¿—: {RESULT_LOG_FILE}")
    print(f"ğŸ› ï¸ æ¨¡å¼: {EXPERIMENT_MODE} | è®°å¿†æº: {MEMORY_SOURCE_FILE}")

    # å‡†å¤‡æ•°æ® (åŒ…å«è®°å¿†åº“è½¬æ¢)
    if not prepare_data(): return
    
    # æ€»æ˜¯æ„å»º/æ£€æŸ¥ç´¢å¼•ï¼Œå› ä¸º RAG éœ€è¦
    build_index()
    
    generator = None
    config = None
    
    if MODEL_SOURCE == "gemini":
        gemini_config_dict = {
            "device": "cpu",
            "retrieval_method": "bm25",
            "corpus_path": corpus_file,
            "index_path": index_dir,
            "retriever_model_path": index_dir,
            "generator_model": "huggingface", 
            "generator_model_path": "gpt2",
            "generation_method": "custom",  
            "save_dir": "rag_result_cache"
        }
        config = Config(config_dict=gemini_config_dict)
        generator = GeminiGenerator(GEMINI_MODEL_NAME, GEMINI_API_KEY)
        
    elif MODEL_SOURCE == "huggingface":
        try:
            model_path = snapshot_download(repo_id=HF_MODEL_NAME)
        except:
            print("âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥")
            return

        hf_config_dict = {
            "device": "cuda" if torch.cuda.is_available() else "cpu",
            "gpu_num": torch.cuda.device_count(),
            "generator_model": "huggingface",
            "generator_model_path": model_path,
            "generation_method": "huggingface",
            "batch_size": 10,
            "max_input_len": 4096, 
            "max_new_tokens": 1024,
            "save_dir": "rag_result_cache"
        }
        config = Config(config_dict=hf_config_dict)
        generator = get_generator(config)
        
        if hasattr(generator, 'tokenizer'):
            generator.tokenizer.padding_side = 'left' 
            if generator.tokenizer.pad_token is None:
                generator.tokenizer.pad_token = generator.tokenizer.eos_token
                generator.tokenizer.pad_token_id = generator.tokenizer.eos_token_id
            generator.tokenizer.model_max_length = 4096
            
        if hasattr(generator, 'model') and hasattr(generator.model.config, 'pad_token_id') and generator.model.config.pad_token_id is None:
            generator.model.config.pad_token_id = generator.tokenizer.pad_token_id

        generator.max_input_len = 4096

    def format_base_prompt(system_text, user_text):
        if MODEL_SOURCE == "gemini":
            return f"{system_text}\n\n{user_text}" if system_text else user_text
        prompt = ""
        if system_text: prompt += f"{system_text}\n\n"
        prompt += f"### Question:\n{user_text}\n\n### Answer:\nLet's think step by step."
        return prompt

    with open(test_file, "r") as f:
        test_dataset_raw = [json.loads(line) for line in f]

    # --- Baseline ä»»åŠ¡ (ä¿ç•™ä½†é»˜è®¤ä¸è·‘ï¼Œé™¤é EXPERIMENT_MODE æ”¹ä¸º all) ---
    acc_baseline = 0
    if EXPERIMENT_MODE in ['baseline', 'all']:
        print("\nâš”ï¸ [Task A] Baseline (No RAG) ...")
        baseline_inputs = []
        for item in test_dataset_raw:
            # sys_msg = "You are a math expert. Solve the problem in a brief. Don't answer more than 50 words.End your answer with \\boxed{number}."
            sys_msg = "You are a math expert. Solve the problem in a brief. Don't answer more than 50 words.End your answer with #### <number>."
            
            baseline_inputs.append(format_base_prompt(sys_msg, item['question']))
        
        baseline_preds = generator.generate(baseline_inputs)
        baseline_results = []
        for item, pred in zip(test_dataset_raw, baseline_preds):
            baseline_results.append({
                "question": item['question'],
                "golden_answers": item['golden_answers'],
                "pred": pred
            })
        acc_baseline = evaluate_results(baseline_results, "Baseline")

    # --- RAG ä»»åŠ¡ (ä¸»è¦ä»»åŠ¡) ---
    acc_rag = 0
    if EXPERIMENT_MODE in ['rag', 'all']:
        print("\nâš”ï¸ [Task B] FlashRAG (Memory: Optimized K30) ...")
        
        rag_config_dict = config.config_dict.copy() if hasattr(config, 'config_dict') else {}
        if not rag_config_dict:
             rag_config_dict = gemini_config_dict if MODEL_SOURCE == "gemini" else hf_config_dict
             
        rag_config_dict.update({
            "retrieval_method": "bm25",
            "corpus_path": corpus_file, # æŒ‡å‘è½¬æ¢åçš„ corpus
            "index_path": index_dir,
            "retriever_model_path": index_dir,
            "topk": 3 
        })
        
        rag_config = Config(config_dict=rag_config_dict)
        retriever = get_retriever(rag_config)
        
        rag_system_part = (
            "You are a math expert. You can solve math problems in one second to give the correct answer. Below are some similar solved problems. "
            "Refer to the logic in these examples to solve the new question.\n\n"
            "Solve the problem in a very brief. Don't answer more than 80 tokens. If the problem is easy, You can also just give the final answer."
            "Do not perform unit conversion."
            "{reference}\n\n"
            "### Question:\n{question}\n\n"
            "### Answer:\n"
            "Let's think step by step." 
        )
        
        prompt_tpl = PromptTemplate(rag_config, system_prompt=rag_system_part, user_prompt="")
        pipeline = SequentialPipeline(rag_config, prompt_template=prompt_tpl, retriever=retriever, generator=generator)
        dataset_obj = Dataset(rag_config, test_file)
        
        rag_results = pipeline.run(dataset_obj)
        acc_rag = evaluate_results(rag_results, "FlashRAG w/ Optimized Memory")
        analyze_memory_usage(rag_results)

    print("\nâœ… æµ‹è¯•ç»“æŸã€‚")

if __name__ == "__main__":
    main()