import os
import json
import re
import time
import torch
import bm25s
from tqdm import tqdm
from datasets import load_dataset
from huggingface_hub import snapshot_download

# Hydra & OmegaConf
import hydra
from omegaconf import DictConfig, OmegaConf

# FlashRAG
from flashrag.config import Config
from flashrag.pipeline import SequentialPipeline
from flashrag.utils import get_retriever, get_generator, Dataset
from flashrag.prompt import PromptTemplate

# å±è”½ transformers çš„å†—ä½™è­¦å‘Š
import transformers
transformers.logging.set_verbosity_error()

# ==========================================
# 1. æ ¸å¿ƒå·¥å…·ç±» (Generator & Eval)
# ==========================================

# ... (æ­¤å¤„ç›´æ¥ç²˜è´´ä¹‹å‰ SGLangGenerator, GeminiGenerator çš„ä»£ç ) ...
from typing import List
from openai import OpenAI

class SGLangGenerator:
    def __init__(self, base_url, model_name, max_new_tokens=1024, batch_size=8, temperature=0.0):
        self.client = OpenAI(api_key="EMPTY", base_url=base_url.rstrip("/"))
        self.model = model_name
        self.max_new_tokens = max_new_tokens
        self.batch_size = batch_size
        self.temperature = temperature
        self.max_input_len = 4096

    def generate(self, prompts: List[str]) -> List[str]:
        outputs = []
        for i in range(0, len(prompts), self.batch_size):
            batch = prompts[i : i + self.batch_size]
            for p in batch:
                try:
                    resp = self.client.chat.completions.create(
                        model=self.model,
                        messages=[{"role": "user", "content": p}],
                        temperature=self.temperature,
                        max_tokens=self.max_new_tokens,
                    )
                    outputs.append(resp.choices[0].message.content)
                except Exception as e:
                    print(f"âŒ SGLang Error: {e}")
                    outputs.append("")
        return outputs

class GeminiGenerator:
    def __init__(self, model_name, api_key):
        import google.generativeai as genai
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model_name)

    def generate(self, input_list, **kwargs):
        responses = []
        for prompt in input_list:
            try:
                if isinstance(prompt, list): prompt = " ".join(prompt)
                result = self.model.generate_content(str(prompt))
                responses.append(result.text if result.parts else "")
                time.sleep(1) 
            except:
                responses.append("Error")
        return responses

# ğŸ”¥ å¿…é¡»ä½¿ç”¨ä¿®å¤åçš„åˆ¤é¢˜å‡½æ•°
def extract_math_answer(text):
    """
    (å‡çº§ç‰ˆ) ä»æ¨¡å‹è¾“å‡ºä¸­æå–ç­”æ¡ˆ
    é€»è¾‘ä¸ _local_extract ä¿æŒä¸€è‡´ï¼š
    1. ä¼˜å…ˆæ‰¾ \boxed{}
    2. å…œåº•æ‰¾æœ€åä¸€è¡Œ
    3. æ¸…æ´— '=' å’Œ '\approx' ä»¥åŠ LaTeX æ‚è´¨
    """
    if not text: return None
    text = str(text).strip()
    
    # 1. ä¼˜å…ˆæå– \boxed{} å†…å®¹
    idx = text.rfind("\\boxed{")
    if idx != -1:
        content_start = idx + 7 
        balance = 0
        for i in range(content_start, len(text)):
            if text[i] == '{': balance += 1
            elif text[i] == '}':
                if balance == 0: return text[content_start:i] 
                balance -= 1
    
    # 2. å…œåº•ç­–ç•¥ï¼šå–æœ€åä¸€è¡Œå¹¶æ¸…æ´—
    lines = text.strip().split('\n')
    if lines:
        last_line = lines[-1].strip()
        if last_line.endswith('.'): last_line = last_line[:-1]
        
        # æ¸…æ´— LaTeX ç¬¦å·
        last_line = last_line.replace('$', '').replace('`', '')
        
        # å»æ‰ "The Answer is" å‰ç¼€
        last_line = re.sub(r'^(The )?Answer( is)?:?', '', last_line, flags=re.IGNORECASE).strip()
        
        # å¤„ç†ç­‰å¼ (å–ç­‰å·å³è¾¹)
        if '=' in last_line: last_line = last_line.split('=')[-1].strip()
        
        # å¤„ç†è¿‘ä¼¼ç¬¦å·
        if '\\approx' in last_line: last_line = last_line.split('\\approx')[-1].strip()
        
        # é•¿åº¦æ”¾å®½åˆ° 100 (åŸç‰ˆæ˜¯ 50)
        if len(last_line) < 100: return last_line
        
    return None

def normalize_latex(s):
    """
    (å‡çº§ç‰ˆ) æ ‡å‡†åŒ– LaTeX å­—ç¬¦ä¸²
    é€»è¾‘ä¸ _local_norm ä¿æŒä¸€è‡´ï¼š
    1. ç§»é™¤ left/right/mathrm ç­‰ä¿®é¥°ç¬¦
    2. ç»Ÿä¸€åˆ†å·ã€ç™¾åˆ†å·
    3. å†æ¬¡å¤„ç†å¯èƒ½æ®‹ç•™çš„ '=' æˆ– '\in'
    """
    if not s: return ""
    # åŸºç¡€æ¸…æ´—
    s = str(s).replace('$', '').replace('`', '').replace('\\%', '%')
    s = s.replace("\\dfrac", "\\frac").replace("\\text", "")
    
    # ç§»é™¤ä¿®é¥°ç¬¦ (è¿™æ˜¯å…³é”®å·®å¼‚ï¼Œé˜²æ­¢ \left( \right) å¯¼è‡´è¯¯åˆ¤)
    s = s.replace("\\left", "").replace("\\right", "").replace("\\mathrm", "")
    
    # å»é™¤ç©ºç™½
    s = "".join(s.split())
    
    # å†æ¬¡ç¡®ä¿å–ç­‰å·å³è¾¹ (åŒé‡ä¿é™©)
    if '=' in s: s = s.split('=')[-1]
    if '\\in' in s: s = s.split('\\in')[-1]
    
    return s.rstrip('.').strip()

def evaluate_results(results, result_log_file,experiment_name = "å¯¹æ¯”æµ‹è¯•"):
    correct = 0
    total = len(results)
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(result_log_file), exist_ok=True)

    with open(result_log_file, "a", encoding="utf-8") as f:
        header = f"\n{'='*20} {experiment_name} {'='*20}\n"
        print(header.strip())
        f.write(header)
        
        for i, item in enumerate(results):
            # å…¼å®¹ FlashRAG Dataset å¯¹è±¡å’Œ dict
            pred = item.pred if hasattr(item, 'pred') else item['pred']
            gold_raw = item.golden_answers[0] if hasattr(item, 'golden_answers') else item['golden_answers'][0]
            question = item.question if hasattr(item, 'question') else item['question']

            # ä½¿ç”¨å‡çº§åçš„æå–é€»è¾‘
            gold_val = extract_math_answer(gold_raw)
            if gold_val is None: gold_val = str(gold_raw).strip()

            pred_val = extract_math_answer(pred)
            is_right = False
            
            # ä½¿ç”¨å‡çº§åçš„å½’ä¸€åŒ–é€»è¾‘è¿›è¡Œæ¯”å¯¹
            if gold_val and pred_val:
                norm_gold = normalize_latex(gold_val)
                norm_pred = normalize_latex(pred_val)
                if norm_gold == norm_pred:
                    is_right = True

            if is_right: correct += 1

            log_entry = (
                f"\n[ID]: {i}\n"
                f"[Question]: {str(question)[:100]}...\n"
                f"[Gold Raw]: ... => [Extracted]: {gold_val}\n"
                f"[Pred Raw]: ...{str(pred)[-50:].replace(chr(10), ' ')} => [Extracted]: {pred_val}\n"
                f"[Result]: {'âœ… Correct' if is_right else 'âŒ Wrong'}\n"
                f"{'-'*30}\n"
            )
            f.write(log_entry)
            if i < 5: print(log_entry.strip()) # å‡å°‘ä¸€ç‚¹æ§åˆ¶å°è¾“å‡º

        acc = correct / total * 100
        summary = (
            f"\nğŸ“Š ç»Ÿè®¡ ({experiment_name}):\n"
            f"Total: {total}, Correct: {correct}, Accuracy: {acc:.2f}%\n"
            f"{'='*50}\n"
        )
        print(summary)
        f.write(summary)
    return acc

# ==========================================
# 2. è¾…åŠ©æ•°æ®å¤„ç†å‡½æ•°
# ==========================================

def build_index_if_needed(corpus_path, index_path):
    print(f"ğŸ”¨ æ­£åœ¨æ„å»ºç´¢å¼•: {corpus_path} -> {index_path}")
    
    texts = []
    with open(corpus_path, 'r', encoding='utf-8') as f:
        for line in f:
            texts.append(json.loads(line)['contents'])
    
    corpus_tokens = bm25s.tokenize(texts)
    retriever = bm25s.BM25()
    retriever.index(corpus_tokens)
    retriever.save(index_path)
    
    # è¡¥å…… FlashRAG éœ€è¦çš„æ–‡ä»¶
    with open(os.path.join(index_path, "stopwords.tokenizer.json"), "w") as f: json.dump([], f)
    with open(os.path.join(index_path, "vocab.tokenizer.json"), "w") as f:
        v = corpus_tokens.vocab
        json.dump({"word_to_id": v, "stem_to_sid": v, "word_to_stem": {k:k for k in v}}, f)

def convert_optimized_memory_to_corpus(memory_file, corpus_file):
    print(f"ğŸ”¨ æ­£åœ¨è½¬æ¢ä¼˜åŒ–è®°å¿†: {memory_file} -> {corpus_file}")
    if not os.path.exists(memory_file):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°ä¼˜åŒ–è®°å¿†æ–‡ä»¶: {memory_file}")
        
    with open(memory_file, 'r') as fin, open(corpus_file, 'w') as fout:
        for line in fin:
            try:
                item = json.loads(line)
                # ä¼˜å…ˆå–ä¼˜åŒ–è¿‡çš„ question
                content = item.get("question") or item.get("contents", "")
                fout.write(json.dumps({"id": str(item['id']), "contents": content}) + "\n")
            except: continue

# ==========================================
# 3. RAG ä»»åŠ¡æ‰§è¡Œå™¨
# ==========================================

def run_rag_task(task_name, cfg, generator, corpus_path, index_path, test_file):
    print(f"\n{'='*20} æ­£åœ¨æ‰§è¡Œ: {task_name} {'='*20}")
    
    # 1. ç¡®ä¿ç´¢å¼•å­˜åœ¨
    build_index_if_needed(corpus_path, index_path)
    
    # ================= ğŸ”¥ æ ¸å¿ƒä¿®æ”¹å¼€å§‹ =================
    # ç›®çš„ï¼šä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„ç¼“å­˜å­ç›®å½•ï¼Œé˜²æ­¢ç»“æœè¦†å†™
    # ä¾‹å¦‚: .../rag_result_cache/åŸå§‹è®°å¿†_Original
    
    # æ¸…æ´—ä¸€ä¸‹ task_nameï¼Œå»æ‰ç©ºæ ¼å’Œæ‹¬å·ï¼Œåšæˆåˆæ³•çš„æ–‡ä»¶å¤¹å
    safe_name = task_name.replace(" ", "_").replace("(", "").replace(")", "")
    
    # æ‹¼æ¥æ–°çš„ä¿å­˜è·¯å¾„
    task_save_dir = os.path.join(cfg.paths.rag_cache_dir, safe_name)
            
    # ================= ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ç»“æŸ =================

    # 2. æ„é€  Config
    rag_update = {
        "data_dir": cfg.paths.root,
        # ğŸ‘‡ è¿™é‡Œæ”¹æˆäº†æ–°çš„å­ç›®å½• task_save_dir
        "save_dir": task_save_dir, 
        
        "retrieval_method": cfg.experiment.retrieval_method,
        "corpus_path": corpus_path,
        "index_path": index_path,
        "retriever_model_path": index_path,
        "topk": cfg.experiment.retrieval_topk,
        "device": cfg.model.device,
        "generator_model": "openai", # å ä½
        "generator_model_path": "gpt2" # å ä½
    }
    rag_config = Config(config_dict=rag_update)
    
    # 3. åˆå§‹åŒ– Pipeline
    retriever = get_retriever(rag_config)
    prompt_tpl = PromptTemplate(rag_config, system_prompt=cfg.experiment.prompts.rag_system, user_prompt="")
    pipeline = SequentialPipeline(rag_config, prompt_template=prompt_tpl, retriever=retriever, generator=generator)
    
    # 4. åŠ è½½é”™é¢˜é›†
    dataset = Dataset(rag_config, test_file)
    root_dir = cfg.paths.root
    dataset_tag = cfg.experiment.dataset_name.split('/')[-1]
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    result_log_file = os.path.join(root_dir, f"eval_{dataset_tag}_{cfg.model.source}_{cfg.experiment.mode}_{timestamp}.txt")
    # 5. è¿è¡Œ
    results = pipeline.run(dataset)
    acc = evaluate_results(results,result_log_file)
    print(f"ğŸ“Š {task_name} æ­£ç¡®ç‡: {acc:.2f}%")
    
    # é¡ºä¾¿å‘Šè¯‰ä½ ç»“æœå­˜åœ¨å“ªäº†
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {os.path.join(task_save_dir, 'intermediate_data.json')}")
    
    return acc

# ==========================================
# 4. ä¸»ç¨‹åº
# ==========================================

@hydra.main(version_base=None, config_path="conf", config_name="config")
def main(cfg: DictConfig):
    
    mode = cfg.eval_compare.mode
    wrong_file = cfg.eval_compare.wrong_file
    
    print(f"ğŸš€ å¯åŠ¨é”™é¢˜é›†å¯¹æ¯”è¯„æµ‹ | æ¨¡å¼: {mode}")
    
    if not os.path.exists(wrong_file):
        print(f"âŒ é”™é¢˜é›†æ–‡ä»¶ä¸å­˜åœ¨: {wrong_file}\nè¯·å…ˆè¿è¡Œ wrong_filter.py")
        return

    # --- ğŸ”¥ æ–°å¢: Debug åˆ‡ç‰‡é€»è¾‘ ---
    wrong_num = cfg.eval_compare.get("wrong_num")
    if wrong_num:
        try:
            limit = int(wrong_num)
            print(f"ğŸ› [Debug Mode] ä»…æˆªå–å‰ {limit} é“é”™é¢˜è¿›è¡Œæµ‹è¯•...")
            
            # è¯»å–åŸæ–‡ä»¶
            with open(wrong_file, "r", encoding="utf-8") as f:
                lines = f.readlines()
            
            if len(lines) > limit:
                # æ„é€ ä¸´æ—¶æ–‡ä»¶å (ä¾‹å¦‚: wrong_questions_debug_5.jsonl)
                wrong_file_debug = wrong_file.replace(".jsonl", f"_debug_{limit}.jsonl")
                
                # å†™å…¥åˆ‡ç‰‡åçš„æ•°æ®
                with open(wrong_file_debug, "w", encoding="utf-8") as f:
                    f.writelines(lines[:limit])
                
                # æŒ‡é’ˆé‡å®šå‘ï¼šè®©åç»­æµç¨‹è¯»è¿™ä¸ªä¸´æ—¶æ–‡ä»¶
                wrong_file = wrong_file_debug
                print(f"   å·²ç”Ÿæˆä¸´æ—¶åˆ‡ç‰‡æ–‡ä»¶: {wrong_file}")
            else:
                print(f"   é”™é¢˜æ€»æ•° ({len(lines)}) å°‘äº wrong_num ({limit})ï¼Œæ— éœ€åˆ‡ç‰‡ã€‚")
        except Exception as e:
            print(f"âš ï¸ Debug åˆ‡ç‰‡å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨å…¨é‡æ•°æ®ã€‚")
    # -----------------------------

    print(f"ğŸ“‚ å½“å‰ä½¿ç”¨çš„æµ‹è¯•æ–‡ä»¶: {wrong_file}")

    # --- åˆå§‹åŒ– Generator (åªåˆå§‹åŒ–ä¸€æ¬¡ï¼Œå¤ç”¨) ---
    model_source = cfg.model.source
    generator = None
    if model_source == "sglang":
        url = cfg.model.get("sglang_api_url", "http://127.0.0.1:30000/v1")
        name = cfg.model.get("sglang_model_name", "Qwen/Qwen3-4B-Instruct-2507")
        generator = SGLangGenerator(url, name, batch_size=cfg.model.batch_size)
        print("âœ… SGLang Generator Ready")
    elif model_source == "gemini":
        generator = GeminiGenerator(cfg.model.gemini_name, os.environ.get("GEMINI_API_KEY"))
        print("âœ… Gemini Generator Ready")
    elif model_source == "huggingface":
        # å¦‚æœéœ€è¦æ”¯æŒ HFï¼Œå¯ä»¥åœ¨è¿™é‡ŒåŠ ï¼Œä½†å»ºè®®é”™é¢˜æœ¬ç”¨ SGLang è·‘å¾—å¿«
        print("âš ï¸ å»ºè®®ä½¿ç”¨ SGLang è¿›è¡Œé”™é¢˜æœ¬å¿«é€ŸéªŒè¯")
        return

    results_summary = {}

    # --- Task 1: åŸå§‹è®°å¿†åº“ (Original) ---
    if mode in ["original", "both"]:
        orig_cfg = cfg.eval_compare.original
        if not os.path.exists(orig_cfg.corpus_path):
            print(f"âš ï¸ åŸå§‹è¯­æ–™ä¸å­˜åœ¨: {orig_cfg.corpus_path} (è¯·å…ˆè·‘ pre.py)")
        else:
            acc = run_rag_task(
                "åŸå§‹è®°å¿† (Original)", cfg, generator,
                orig_cfg.corpus_path, orig_cfg.index_path, wrong_file
            )
            results_summary["Original"] = acc

    # --- Task 2: ä¼˜åŒ–è®°å¿†åº“ (Optimized) ---
    if mode in ["optimized", "both"]:
        opt_cfg = cfg.eval_compare.optimized
        # å®æ—¶è½¬æ¢ (ä¿è¯æœ€æ–°)
        convert_optimized_memory_to_corpus(opt_cfg.memory_file, opt_cfg.corpus_path)
        
        acc = run_rag_task(
            "ä¼˜åŒ–è®°å¿† (Optimized)", cfg, generator,
            opt_cfg.corpus_path, opt_cfg.index_path, wrong_file
        )
        results_summary["Optimized"] = acc

    # --- æœ€ç»ˆå¯¹æ¯”æŠ¥å‘Š ---
    print("\n" + "="*40)
    print("ğŸ† é”™é¢˜é›†å¤ç›˜å¯¹æ¯”æŠ¥å‘Š")
    if wrong_num: print(f"(Debug: Top {wrong_num})")
    print("="*40)
    print(f"{'ç­–ç•¥':<20} | {'æ­£ç¡®ç‡':<10}")
    print("-" * 35)
    
    base_acc = results_summary.get("Original", 0)
    opt_acc = results_summary.get("Optimized", 0)
    
    if "Original" in results_summary:
        print(f"{'Original RAG':<20} | {base_acc:.2f}%")
    if "Optimized" in results_summary:
        print(f"{'Optimized RAG':<20} | {opt_acc:.2f}%")
    
    if mode == "both":
        diff = opt_acc - base_acc
        icon = "ğŸš€" if diff > 0 else "ğŸ“‰"
        print("-" * 35)
        print(f"æ•ˆæœæå‡: {icon} {diff:+.2f}%")
    print("="*40)

if __name__ == "__main__":
    main()