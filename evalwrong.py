import os
import json
import re
import time
import torch
import bm25s
from tqdm import tqdm
from datasets import load_dataset
from huggingface_hub import snapshot_download

# Hydra & OmegaConf
import hydra
from omegaconf import DictConfig, OmegaConf

# FlashRAG
from flashrag.config import Config
from flashrag.pipeline import SequentialPipeline
from flashrag.utils import get_retriever, get_generator, Dataset
from flashrag.prompt import PromptTemplate

# å±è”½ transformers çš„å†—ä½™è­¦å‘Š
import transformers
transformers.logging.set_verbosity_error()

# ==========================================
# 1. æ ¸å¿ƒå·¥å…·ç±» (Generator & Eval)
# ==========================================

# ... (æ­¤å¤„ç›´æ¥ç²˜è´´ä¹‹å‰ SGLangGenerator, GeminiGenerator çš„ä»£ç ) ...
from typing import List
from openai import OpenAI

class SGLangGenerator:
    def __init__(self, base_url, model_name, max_new_tokens=1024, batch_size=8, temperature=0.0):
        self.client = OpenAI(api_key="EMPTY", base_url=base_url.rstrip("/"))
        self.model = model_name
        self.max_new_tokens = max_new_tokens
        self.batch_size = batch_size
        self.temperature = temperature
        self.max_input_len = 4096

    def generate(self, prompts: List[str]) -> List[str]:
        outputs = []
        for i in range(0, len(prompts), self.batch_size):
            batch = prompts[i : i + self.batch_size]
            for p in batch:
                try:
                    resp = self.client.chat.completions.create(
                        model=self.model,
                        messages=[{"role": "user", "content": p}],
                        temperature=self.temperature,
                        max_tokens=self.max_new_tokens,
                    )
                    outputs.append(resp.choices[0].message.content)
                except Exception as e:
                    print(f"âŒ SGLang Error: {e}")
                    outputs.append("")
        return outputs

class GeminiGenerator:
    def __init__(self, model_name, api_key):
        import google.generativeai as genai
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model_name)

    def generate(self, input_list, **kwargs):
        responses = []
        for prompt in input_list:
            try:
                if isinstance(prompt, list): prompt = " ".join(prompt)
                result = self.model.generate_content(str(prompt))
                responses.append(result.text if result.parts else "")
                time.sleep(1) 
            except:
                responses.append("Error")
        return responses

# ğŸ”¥ å¿…é¡»ä½¿ç”¨ä¿®å¤åçš„åˆ¤é¢˜å‡½æ•°
def extract_math_answer(text):
    if not text: return None
    text = str(text).strip()
    idx = text.rfind("\\boxed{")
    if idx != -1:
        content_start = idx + 7 
        balance = 0
        for i in range(content_start, len(text)):
            if text[i] == '{': balance += 1
            elif text[i] == '}':
                if balance == 0: return text[content_start:i] 
                balance -= 1
    
    lines = text.strip().split('\n')
    if lines:
        last_line = lines[-1].strip()
        if last_line.endswith('.'): last_line = last_line[:-1]
        last_line = last_line.replace('$', '').replace('`', '')
        last_line = re.sub(r'^(The )?Answer( is)?:?', '', last_line, flags=re.IGNORECASE).strip()
        if '=' in last_line: last_line = last_line.split('=')[-1].strip()
        if '\\approx' in last_line: last_line = last_line.split('\\approx')[-1].strip()
        if len(last_line) < 100: return last_line
    return None

def normalize_latex(s):
    if not s: return ""
    s = str(s).replace('$', '').replace('`', '').replace('\\%', '%')
    s = s.replace("\\dfrac", "\\frac").replace("\\text", "")
    s = s.replace("\\left", "").replace("\\right", "").replace("\\mathrm", "")
    s = "".join(s.split())
    if '=' in s: s = s.split('=')[-1]
    if '\\in' in s: s = s.split('\\in')[-1]
    return s.rstrip('.').strip()

def evaluate_results(results):
    correct = 0
    total = len(results)
    for item in results:
        pred = item.pred if hasattr(item, 'pred') else item['pred']
        gold = item.golden_answers[0] if hasattr(item, 'golden_answers') else item['golden_answers'][0]
        
        gold_val = extract_math_answer(gold) or str(gold).strip()
        pred_val = extract_math_answer(pred)
        
        if gold_val and pred_val:
            if normalize_latex(gold_val) == normalize_latex(pred_val):
                correct += 1
    return (correct / total * 100) if total > 0 else 0

# ==========================================
# 2. è¾…åŠ©æ•°æ®å¤„ç†å‡½æ•°
# ==========================================

def build_index_if_needed(corpus_path, index_path):
    if os.path.exists(index_path) and os.path.exists(os.path.join(index_path, "vocab.tokenizer.json")):
        print(f"âœ… ç´¢å¼•å·²å­˜åœ¨: {index_path}")
        return
    print(f"ğŸ”¨ æ­£åœ¨æ„å»ºç´¢å¼•: {corpus_path} -> {index_path}")
    
    texts = []
    with open(corpus_path, 'r', encoding='utf-8') as f:
        for line in f:
            texts.append(json.loads(line)['contents'])
    
    corpus_tokens = bm25s.tokenize(texts)
    retriever = bm25s.BM25()
    retriever.index(corpus_tokens)
    retriever.save(index_path)
    
    # è¡¥å…… FlashRAG éœ€è¦çš„æ–‡ä»¶
    with open(os.path.join(index_path, "stopwords.tokenizer.json"), "w") as f: json.dump([], f)
    with open(os.path.join(index_path, "vocab.tokenizer.json"), "w") as f:
        v = corpus_tokens.vocab
        json.dump({"word_to_id": v, "stem_to_sid": v, "word_to_stem": {k:k for k in v}}, f)

def convert_optimized_memory_to_corpus(memory_file, corpus_file):
    print(f"ğŸ”¨ æ­£åœ¨è½¬æ¢ä¼˜åŒ–è®°å¿†: {memory_file} -> {corpus_file}")
    if not os.path.exists(memory_file):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°ä¼˜åŒ–è®°å¿†æ–‡ä»¶: {memory_file}")
        
    with open(memory_file, 'r') as fin, open(corpus_file, 'w') as fout:
        for line in fin:
            try:
                item = json.loads(line)
                # ä¼˜å…ˆå–ä¼˜åŒ–è¿‡çš„ question
                content = item.get("question") or item.get("contents", "")
                fout.write(json.dumps({"id": str(item['id']), "contents": content}) + "\n")
            except: continue

# ==========================================
# 3. RAG ä»»åŠ¡æ‰§è¡Œå™¨
# ==========================================

def run_rag_task(task_name, cfg, generator, corpus_path, index_path, test_file):
    print(f"\n{'='*20} æ­£åœ¨æ‰§è¡Œ: {task_name} {'='*20}")
    
    # 1. ç¡®ä¿ç´¢å¼•å­˜åœ¨
    build_index_if_needed(corpus_path, index_path)
    
    # 2. æ„é€  Config
    rag_update = {
        "data_dir": cfg.paths.root,
        "save_dir": cfg.paths.rag_cache_dir,
        "retrieval_method": cfg.experiment.retrieval_method,
        "corpus_path": corpus_path,
        "index_path": index_path,
        "retriever_model_path": index_path,
        "topk": cfg.experiment.retrieval_topk,
        "device": cfg.model.device,
        "generator_model": "openai", # å ä½
        "generator_model_path": "gpt2" # å ä½
    }
    rag_config = Config(config_dict=rag_update)
    
    # 3. åˆå§‹åŒ– Pipeline
    retriever = get_retriever(rag_config)
    prompt_tpl = PromptTemplate(rag_config, system_prompt=cfg.experiment.prompts.rag_system, user_prompt="")
    pipeline = SequentialPipeline(rag_config, prompt_template=prompt_tpl, retriever=retriever, generator=generator)
    
    # 4. åŠ è½½é”™é¢˜é›†
    dataset = Dataset(rag_config, test_file)
    
    # 5. è¿è¡Œ
    results = pipeline.run(dataset)
    acc = evaluate_results(results)
    print(f"ğŸ“Š {task_name} æ­£ç¡®ç‡: {acc:.2f}%")
    return acc

# ==========================================
# 4. ä¸»ç¨‹åº
# ==========================================

@hydra.main(version_base=None, config_path="conf", config_name="config")
def main(cfg: DictConfig):
    
    mode = cfg.eval_compare.mode
    wrong_file = cfg.eval_compare.wrong_file
    
    print(f"ğŸš€ å¯åŠ¨é”™é¢˜é›†å¯¹æ¯”è¯„æµ‹ | æ¨¡å¼: {mode}")
    
    if not os.path.exists(wrong_file):
        print(f"âŒ é”™é¢˜é›†æ–‡ä»¶ä¸å­˜åœ¨: {wrong_file}\nè¯·å…ˆè¿è¡Œ wrong_filter.py")
        return

    # --- ğŸ”¥ æ–°å¢: Debug åˆ‡ç‰‡é€»è¾‘ ---
    wrong_num = cfg.experiment.get("wrong_num")
    if wrong_num:
        try:
            limit = int(wrong_num)
            print(f"ğŸ› [Debug Mode] ä»…æˆªå–å‰ {limit} é“é”™é¢˜è¿›è¡Œæµ‹è¯•...")
            
            # è¯»å–åŸæ–‡ä»¶
            with open(wrong_file, "r", encoding="utf-8") as f:
                lines = f.readlines()
            
            if len(lines) > limit:
                # æ„é€ ä¸´æ—¶æ–‡ä»¶å (ä¾‹å¦‚: wrong_questions_debug_5.jsonl)
                wrong_file_debug = wrong_file.replace(".jsonl", f"_debug_{limit}.jsonl")
                
                # å†™å…¥åˆ‡ç‰‡åçš„æ•°æ®
                with open(wrong_file_debug, "w", encoding="utf-8") as f:
                    f.writelines(lines[:limit])
                
                # æŒ‡é’ˆé‡å®šå‘ï¼šè®©åç»­æµç¨‹è¯»è¿™ä¸ªä¸´æ—¶æ–‡ä»¶
                wrong_file = wrong_file_debug
                print(f"   å·²ç”Ÿæˆä¸´æ—¶åˆ‡ç‰‡æ–‡ä»¶: {wrong_file}")
            else:
                print(f"   é”™é¢˜æ€»æ•° ({len(lines)}) å°‘äº wrong_num ({limit})ï¼Œæ— éœ€åˆ‡ç‰‡ã€‚")
        except Exception as e:
            print(f"âš ï¸ Debug åˆ‡ç‰‡å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨å…¨é‡æ•°æ®ã€‚")
    # -----------------------------

    print(f"ğŸ“‚ å½“å‰ä½¿ç”¨çš„æµ‹è¯•æ–‡ä»¶: {wrong_file}")

    # --- åˆå§‹åŒ– Generator (åªåˆå§‹åŒ–ä¸€æ¬¡ï¼Œå¤ç”¨) ---
    model_source = cfg.model.source
    generator = None
    if model_source == "sglang":
        url = cfg.model.get("sglang_api_url", "http://127.0.0.1:30000/v1")
        name = cfg.model.get("sglang_model_name", "Qwen/Qwen3-4B-Instruct-2507")
        generator = SGLangGenerator(url, name, batch_size=cfg.model.batch_size)
        print("âœ… SGLang Generator Ready")
    elif model_source == "gemini":
        generator = GeminiGenerator(cfg.model.gemini_name, os.environ.get("GEMINI_API_KEY"))
        print("âœ… Gemini Generator Ready")
    elif model_source == "huggingface":
        # å¦‚æœéœ€è¦æ”¯æŒ HFï¼Œå¯ä»¥åœ¨è¿™é‡ŒåŠ ï¼Œä½†å»ºè®®é”™é¢˜æœ¬ç”¨ SGLang è·‘å¾—å¿«
        print("âš ï¸ å»ºè®®ä½¿ç”¨ SGLang è¿›è¡Œé”™é¢˜æœ¬å¿«é€ŸéªŒè¯")
        return

    results_summary = {}

    # --- Task 1: åŸå§‹è®°å¿†åº“ (Original) ---
    if mode in ["original", "both"]:
        orig_cfg = cfg.eval_compare.original
        if not os.path.exists(orig_cfg.corpus_path):
            print(f"âš ï¸ åŸå§‹è¯­æ–™ä¸å­˜åœ¨: {orig_cfg.corpus_path} (è¯·å…ˆè·‘ pre.py)")
        else:
            acc = run_rag_task(
                "åŸå§‹è®°å¿† (Original)", cfg, generator,
                orig_cfg.corpus_path, orig_cfg.index_path, wrong_file
            )
            results_summary["Original"] = acc

    # --- Task 2: ä¼˜åŒ–è®°å¿†åº“ (Optimized) ---
    if mode in ["optimized", "both"]:
        opt_cfg = cfg.eval_compare.optimized
        # å®æ—¶è½¬æ¢ (ä¿è¯æœ€æ–°)
        convert_optimized_memory_to_corpus(opt_cfg.memory_file, opt_cfg.corpus_path)
        
        acc = run_rag_task(
            "ä¼˜åŒ–è®°å¿† (Optimized)", cfg, generator,
            opt_cfg.corpus_path, opt_cfg.index_path, wrong_file
        )
        results_summary["Optimized"] = acc

    # --- æœ€ç»ˆå¯¹æ¯”æŠ¥å‘Š ---
    print("\n" + "="*40)
    print("ğŸ† é”™é¢˜é›†å¤ç›˜å¯¹æ¯”æŠ¥å‘Š")
    if wrong_num: print(f"(Debug: Top {wrong_num})")
    print("="*40)
    print(f"{'ç­–ç•¥':<20} | {'æ­£ç¡®ç‡':<10}")
    print("-" * 35)
    
    base_acc = results_summary.get("Original", 0)
    opt_acc = results_summary.get("Optimized", 0)
    
    if "Original" in results_summary:
        print(f"{'Original RAG':<20} | {base_acc:.2f}%")
    if "Optimized" in results_summary:
        print(f"{'Optimized RAG':<20} | {opt_acc:.2f}%")
    
    if mode == "both":
        diff = opt_acc - base_acc
        icon = "ğŸš€" if diff > 0 else "ğŸ“‰"
        print("-" * 35)
        print(f"æ•ˆæœæå‡: {icon} {diff:+.2f}%")
    print("="*40)

if __name__ == "__main__":
    main()