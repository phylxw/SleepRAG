import os
import json
import re
import time
import torch
import bm25s
import logging
import ast
import collections # æ–°å¢: ç”¨äºè®¡æ•°
from datasets import load_dataset
from tqdm import tqdm
from huggingface_hub import snapshot_download
from flashrag.config import Config
from flashrag.pipeline import SequentialPipeline
from flashrag.utils import get_retriever, get_generator, Dataset
from flashrag.prompt import PromptTemplate
import matplotlib.pyplot as plt
import json
# å±è”½ transformers çš„å†—ä½™è­¦å‘Š
import transformers
transformers.logging.set_verbosity_error()

# ==========================================
# ğŸ› ï¸ æ ¸å¿ƒé…ç½®åŒºåŸŸ (å·²ä¿®æ”¹ä¸º GSM8K)
# ==========================================
os.environ["CUDA_VISIBLE_DEVICES"] = "3,4,5,6,7"
# 1. å®éªŒæ§åˆ¶å¼€å…³
# é€‰é¡¹: 'baseline' (åªæµ‹åŸæ¨¡å‹), 'rag' (åªæµ‹FlashRAG), 'all' (å¯¹æ¯”æµ‹è¯•)
EXPERIMENT_MODE = "all" 

# ğŸ”¥ [æ–°å¢] è®°å¿†çƒ­åº¦ç»Ÿè®¡å¼€å…³
# True: ç”»å‡ºè®°å¿†è°ƒç”¨é¢‘æ¬¡åˆ†å¸ƒå›¾ (ä¿å­˜ä¸ºpng)
# False: ä»…åœ¨ç»ˆç«¯è¾“å‡ºé¢‘æ¬¡æœ€é«˜çš„ Top 30 è®°å¿†ID
VISUALIZE_MEMORY_DISTRIBUTION = True

# 2. è°ƒè¯•æ ·æœ¬æ•°
# é€‰é¡¹: 10 (å¿«é€Ÿæµ‹è¯•), None (è·‘å…¨é‡)
# MATH æµ‹è¯•é›†æœ‰ 1319 æ¡ï¼Œå»ºè®®å…ˆè®¾ä¸º 20-50 æ¡è·‘é€šæµç¨‹
DEBUG_NUM = None

# 3. æ¨¡å‹è®¾ç½®
# é€‰é¡¹: 'huggingface' (æœ¬åœ°/HFæ¨¡å‹) æˆ– 'gemini' (Google API)
MODEL_SOURCE = "huggingface" 

# é€‰é¡¹ï¼šQwen/Qwen3-4B-Instruct-2507 Qwen/Qwen2-1.5B-Instruct
# å»ºè®®ä½¿ç”¨ Qwen2.5-7B-Instructï¼Œæ•ˆæœæ›´ç¨³å®š
HF_MODEL_NAME = "Qwen/Qwen3-4B-Instruct-2507" 

# [Gemini é…ç½®]
GEMINI_MODEL_NAME = "gemini-2.5-flash"
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY") 

# ==========================================
# 4. æ•°æ®é›†é…ç½® (MATH / LightEval ä¸“ç”¨é…ç½®)
# ==========================================
# lighteval/MATH å…¶å®é€šå¸¸æŒ‡å‘ hendrycks/competition_math
# å»ºè®®ç›´æ¥ä½¿ç”¨åŸå§‹æºï¼Œæˆ–è€…ç¡®ä¿ä½ å¼•ç”¨çš„åº“å­˜åœ¨
DATASET_NAME = "DigitalLearningGmbH/MATH-lighteval" 
# MATH æ•°æ®é›†éœ€è¦æŒ‡å®šå­é›†ï¼Œ"all" è¡¨ç¤ºåŠ è½½ä»£æ•°ã€å‡ ä½•ç­‰æ‰€æœ‰ç±»åˆ«
DATASET_CONFIG = "algebra" 

SPLIT_TRAIN = "train" 
SPLIT_TEST = "test"   

# ==========================================
# 5. å­—æ®µæ˜ å°„ (âš ï¸ å…³é”®ä¿®æ”¹)
# ==========================================
# GSM8K çš„åˆ—åæ˜¯ 'question' å’Œ 'answer'
# MATH  çš„åˆ—åé€šå¸¸æ˜¯ 'problem'  å’Œ 'solution'
FIELD_MAP = {
    'question': 'problem',        
    'answer': 'solution' 
}

# ==========================================
# âš™ï¸ è‡ªåŠ¨è·¯å¾„ç”Ÿæˆ (å‹¿åŠ¨)
# ==========================================
dataset_tag = DATASET_NAME.split('/')[-1]
corpus_file = f"{dataset_tag}_corpus.jsonl"
test_file = f"{dataset_tag}_test_data.jsonl"
index_dir = f"{dataset_tag}_bm25_index"

timestamp = time.strftime("%Y%m%d_%H%M%S")
RESULT_LOG_FILE = f"{dataset_tag}_{MODEL_SOURCE}_{EXPERIMENT_MODE}_{timestamp}.txt"
VIS_IMAGE_FILE = f"memory_distribution_{timestamp}.png"

# ğŸ”¥ æ–°å¢ï¼šè®°å¿†è°ƒç”¨é¢‘æ¬¡æ’åºç»“æœï¼ˆjsonlï¼‰
MEM_FREQ_JSONL_FILE = f"{dataset_tag}_memory_freq_{timestamp}.jsonl"
# ==========================================
# 1. æ•°æ®å‡†å¤‡æ¨¡å— (é€‚é… GSM8K)
# ==========================================
def prepare_data():
    print(f"ğŸ“¥ [Step 1] æ­£åœ¨åŠ è½½æ•°æ®é›†: {DATASET_NAME} (Config: {DATASET_CONFIG})...")
    try:
        if DATASET_CONFIG:
            dataset = load_dataset(DATASET_NAME, DATASET_CONFIG)
        else:
            dataset = load_dataset(DATASET_NAME)
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return False

    q_col = FIELD_MAP['question']
    a_col = FIELD_MAP['answer']

    # --- A. æ„å»ºè®°å¿†åº“ (ä½¿ç”¨ Train é›†) ---
    if not os.path.exists(corpus_file):
        print(f"ğŸ”¨ [Memory] æ­£åœ¨å°† {SPLIT_TRAIN} é›†è½¬æ¢ä¸ºè®°å¿†åº“: {corpus_file}...")
        with open(corpus_file, "w", encoding="utf-8") as f:
            if SPLIT_TRAIN not in dataset:
                print(f"âš ï¸ è­¦å‘Š: æ•°æ®é›†æ²¡æœ‰ {SPLIT_TRAIN} åˆ’åˆ†ï¼")
                return False
                
            for i, item in enumerate(tqdm(dataset[SPLIT_TRAIN])):
                q_text = item.get(q_col, "")
                a_text = item.get(a_col, "") # GSM8K ç›´æ¥æ˜¯å­—ç¬¦ä¸²ï¼Œä¸éœ€è¦ eval
                
                # æ„å»ºæ£€ç´¢å†…å®¹ï¼šé€šå¸¸æ£€ç´¢ç›¸ä¼¼çš„é—®é¢˜å’Œè§£é¢˜æ€è·¯
                content = f"Question: {q_text}\nAnswer: {a_text}"
                f.write(json.dumps({"id": str(i), "contents": content}) + "\n")
    else:
        print(f"âœ… [Memory] æ£€æµ‹åˆ°ç°æœ‰è®°å¿†åº“: {corpus_file}ï¼Œè·³è¿‡æ„å»ºã€‚")
    
    # --- B. å‡†å¤‡æµ‹è¯•é›† ---
    print(f"ğŸ”¨ [Test] æ­£åœ¨æå–æµ‹è¯•é›† (æ ·æœ¬æ•°: {DEBUG_NUM if DEBUG_NUM else 'ALL'})...")
    with open(test_file, "w", encoding="utf-8") as f:
        if SPLIT_TEST not in dataset:
             print(f"âŒ é”™è¯¯: æ•°æ®é›†æ²¡æœ‰ {SPLIT_TEST} åˆ’åˆ†ï¼")
             return False
             
        test_data = dataset[SPLIT_TEST]
        if DEBUG_NUM:
            limit = min(DEBUG_NUM, len(test_data))
            test_data = test_data.select(range(limit))
            
        for i, item in enumerate(test_data):
            q_text = item.get(q_col, "")
            raw_ans = item.get(a_col, "") # è¿™é‡Œæ˜¯åŒ…å« reasoning + #### result çš„å®Œæ•´ç­”æ¡ˆ
            
            f.write(json.dumps({
                "id": str(i),
                "question": q_text,
                "golden_answers": [str(raw_ans)] # å­˜å…¥åˆ—è¡¨ä¿æŒæ ¼å¼ä¸€è‡´
            }) + "\n")
    return True

# ==========================================
# 2. ç´¢å¼•æ„å»ºæ¨¡å— (BM25)
# ==========================================
def build_index():
    if os.path.exists(index_dir) and os.path.exists(os.path.join(index_dir, "vocab.tokenizer.json")):
        print(f"âœ… [Index] ç´¢å¼•å·²å­˜åœ¨: {index_dir}ï¼Œè·³è¿‡æ„å»ºã€‚")
        return

    print(f"ğŸ”¨ [Index] æ­£åœ¨ä¸º {corpus_file} æ„å»º BM25 ç´¢å¼•...")
    corpus_texts = []
    with open(corpus_file, "r", encoding="utf-8") as f:
        for line in f:
            corpus_texts.append(json.loads(line)['contents'])
    
    corpus_tokens = bm25s.tokenize(corpus_texts)
    retriever_builder = bm25s.BM25()
    retriever_builder.index(corpus_tokens)
    retriever_builder.save(index_dir)
    
    with open(os.path.join(index_dir, "stopwords.tokenizer.json"), "w") as f:
        json.dump([], f)
    with open(os.path.join(index_dir, "vocab.tokenizer.json"), "w") as f:
        vocab = corpus_tokens.vocab
        json.dump({"word_to_id": vocab, "stem_to_sid": vocab, "word_to_stem": {k: k for k in vocab}}, f)
    print("âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼")

# ==========================================
# 3. Gemini ç”Ÿæˆå™¨ç±»
# ==========================================
class GeminiGenerator:
    def __init__(self, model_name, api_key):
        import google.generativeai as genai
        if not api_key:
            raise ValueError("âŒ æœªæ£€æµ‹åˆ° API Keyï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ GEMINI_API_KEY")
        
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model_name)
        print(f"ğŸ¤– Gemini Generator ({model_name}) å·²åŠ è½½")
        
        self.max_input_len = 30000 

    def generate(self, input_list, **kwargs):
        responses = []
        for prompt in input_list:
            try:
                if isinstance(prompt, list): prompt = " ".join(prompt)
                clean_prompt = str(prompt)
                result = self.model.generate_content(clean_prompt)
                if result.parts:
                    responses.append(result.text)
                else:
                    responses.append("Error: Empty Response (Safety Block)")
                import time
                time.sleep(2) # ç¨å¾®å‡å°‘ä¸€ç‚¹ sleepï¼ŒåŠ å¿«é€Ÿåº¦
            except Exception as e:
                print(f"âš ï¸ Gemini API Error: {e}")
                import time
                time.sleep(5)
                responses.append("Error")
        return responses

# ==========================================
# 4. è¯„ä¼°å·¥å…· (ä¸“ä¸º Mathlighteval æ”¹é€ )
# ==========================================

import re

def extract_math_answer(text):
    """
    ä¸“é—¨ç”¨äº MATH/LightEval æ•°æ®é›†çš„ç­”æ¡ˆæå–é€»è¾‘ã€‚
    ç›®æ ‡ï¼šæå– \boxed{...} ä¸­çš„å†…å®¹ã€‚
    """
    if not text:
        return None
    text = str(text)

    # --- ç­–ç•¥ 1: æ ‡å‡† \boxed{...} æå– (æ”¯æŒåµŒå¥—æ‹¬å·) ---
    # ç®€å•çš„æ­£åˆ™ r'\\boxed\{(.*?)\}' æ— æ³•å¤„ç† \boxed{\frac{1}{2}} è¿™ç§åµŒå¥—æƒ…å†µ
    # æ‰€ä»¥æˆ‘ä»¬éœ€è¦ä»åå¾€å‰æ‰¾ \boxed{ï¼Œç„¶åç”¨æ ˆé€»è¾‘åŒ¹é…å³æ‹¬å·
    idx = text.rfind("\\boxed{")
    if idx != -1:
        # ä» "boxed{" åé¢å¼€å§‹æ‰¾
        content_start = idx + 7 
        balance = 0
        for i in range(content_start, len(text)):
            char = text[i]
            if char == '{':
                balance += 1
            elif char == '}':
                if balance == 0:
                    return text[content_start:i] # æ‰¾åˆ°é—­åˆç‚¹ï¼Œè¿”å›å†…å®¹
                balance -= 1
    
    # --- ç­–ç•¥ 2: å¦‚æœæ²¡æ‰¾åˆ° boxedï¼Œå°è¯•æå–æœ€åä¸€è¡Œ (ä¿åº•ç­–ç•¥) ---
    # å¾ˆå¤šæ¨¡å‹å¦‚æœæ²¡æœ‰éµå¾ªæŒ‡ä»¤è¾“å‡º boxedï¼Œç­”æ¡ˆé€šå¸¸åœ¨æœ€å
    lines = text.strip().split('\n')
    if lines:
        last_line = lines[-1].strip()
        # ç®€å•çš„æ¸…ç†ï¼šå»æ‰ "Answer:" "The answer is" ç­‰å‰ç¼€
        last_line = re.sub(r'^(The )?Answer( is)?:?', '', last_line, flags=re.IGNORECASE).strip()
        # å¦‚æœå‰©ä¸‹çš„æ˜¯ä¸ªå¾ˆçŸ­çš„å­—ç¬¦ä¸²ï¼ˆæ¯”å¦‚æ•°å­—æˆ–çŸ­å…¬å¼ï¼‰ï¼Œå°±å½“åšç­”æ¡ˆ
        if len(last_line) < 50: 
            return last_line

    return None

def normalize_latex(s):
    """
    å¯¹ LaTeX ç­”æ¡ˆè¿›è¡Œç®€å•çš„å½’ä¸€åŒ–ï¼Œä»¥ä¾¿è¿›è¡Œå­—ç¬¦ä¸²æ¯”è¾ƒã€‚
    """
    if not s: return ""
    s = str(s)
    # 1. å»é™¤æ‰€æœ‰ç©ºç™½å­—ç¬¦ (ç©ºæ ¼ã€æ¢è¡Œ) -> "x + y" å˜æˆ "x+y"
    s = "".join(s.split())
    # 2. ç»Ÿä¸€éƒ¨åˆ† LaTeX å†™æ³• (å¯é€‰ï¼Œæ ¹æ®éœ€è¦æ‰©å±•)
    # æ¯”å¦‚æŠŠ \dfrac å˜æˆ \frac
    s = s.replace("\\dfrac", "\\frac")
    # 3. å»æ‰æ–‡æœ¬æ¨¡å¼æ ‡è®° (å¯é€‰)
    s = s.replace("\\text", "")
    return s.strip()

def evaluate_results(results, experiment_name):
    correct = 0
    total = len(results)
    
    with open(RESULT_LOG_FILE, "a", encoding="utf-8") as f:
        header = f"\n{'='*20} {experiment_name} {'='*20}\n"
        print(header.strip())
        f.write(header)
        
        for i, item in enumerate(results):
            pred = item.pred if hasattr(item, 'pred') else item['pred']
            gold_raw = item.golden_answers[0] if hasattr(item, 'golden_answers') else item['golden_answers'][0]
            question = item.question if hasattr(item, 'question') else item['question']

            # --- 1. æå– Gold Answer ---
            # MATH æ•°æ®é›†çš„ gold_raw é€šå¸¸æœ¬èº«å°±æ˜¯ solutionï¼Œæœ€åä¸€æ®µå«æœ‰ \boxed{}
            gold_val = extract_math_answer(gold_raw)
            if gold_val is None:
                # å¦‚æœæ ‡å‡†ç­”æ¡ˆé‡Œç«Ÿç„¶æ²¡æœ‰ boxed (æå°‘è§)ï¼Œåˆ™å–æœ€åä¸€éƒ¨åˆ†
                gold_val = str(gold_raw).strip()

            # --- 2. æå– Prediction Answer ---
            pred_val = extract_math_answer(pred)
            
            # --- 3. å¯¹æ¯”åˆ¤æ–­ (æ ¸å¿ƒä¿®æ”¹ï¼šå­—ç¬¦ä¸²å½’ä¸€åŒ–å¯¹æ¯”) ---
            is_right = False
            
            # å¿…é¡»ä¸¤è€…éƒ½æœ‰å€¼æ‰èƒ½æ¯”
            if gold_val and pred_val:
                # å½’ä¸€åŒ–å¤„ç†
                norm_gold = normalize_latex(gold_val)
                norm_pred = normalize_latex(pred_val)
                
                # å­—ç¬¦ä¸²å…¨ç­‰å¯¹æ¯” (Exact Match)
                if norm_gold == norm_pred:
                    is_right = True
                
                # [å¯é€‰] å°è¯•æ•°å€¼å¯¹æ¯” (é˜²æ­¢ 1/2 != 0.5 çš„æƒ…å†µ)
                # åªæœ‰å½“ä¸¤è€…çœ‹èµ·æ¥éƒ½åƒçº¯æ•°å­—æ—¶æ‰å°è¯•
                # try:
                #     if abs(float(norm_gold) - float(norm_pred)) < 1e-6:
                #         is_right = True
                # except:
                #     pass

            if is_right:
                correct += 1

            # æ‰“å°æ—¥å¿—
            log_entry = (
                f"\n[ID]: {i}\n"
                f"[Question]: {str(question)[:100]}...\n" # é¢˜ç›®å¤ªé•¿æˆªæ–­ä¸€ä¸‹
                f"[Gold Raw]: ... => [Extracted]: {gold_val}\n"
                f"[Pred Raw]: ...{str(pred)[-50:].replace(chr(10), ' ')} => [Extracted]: {pred_val}\n"
                f"[Result]: {'âœ… Correct' if is_right else 'âŒ Wrong'}\n"
                f"{'-'*30}\n"
            )
            f.write(log_entry)
            if i < 10: print(log_entry.strip())

        acc = correct / total * 100
        summary = (
            f"\nğŸ“Š ç»Ÿè®¡ ({experiment_name}):\n"
            f"Total: {total}, Correct: {correct}, Accuracy: {acc:.2f}%\n"
            f"{'='*50}\n"
        )
        print(summary)
        f.write(summary)
    return acc
# ==========================================
# ğŸ”¥ [é‡æ„ç‰ˆ] è®°å¿†è°ƒç”¨é¢‘æ¬¡åˆ†æ (å«å…¨é‡ç»Ÿè®¡ & å ä½ç¬¦)
# ==========================================
def analyze_memory_usage(rag_results):
    print("\nğŸ” [Analysis] æ­£åœ¨è¿›è¡Œå…¨é‡è®°å¿†çƒ­åº¦ç»Ÿè®¡...")
    
    # -------------------------------------------------------
    # 1. å»ºç«‹å…¨é‡è®°å¿†è´¦æœ¬ (åˆå§‹åŒ–æ‰€æœ‰ ID ä¸º 0)ï¼ŒåŒæ—¶ä¿å­˜å†…å®¹
    # -------------------------------------------------------
    all_memory_ids = set()
    id_to_content = {}  # æ–°å¢ï¼šè®°å½•æ¯æ¡è®°å¿†çš„åŸå§‹æ–‡æœ¬

    try:
        with open(corpus_file, "r", encoding="utf-8") as f:
            for line in f:
                item = json.loads(line)
                mid = str(item['id'])
                all_memory_ids.add(mid)
                # è®°ä½è¿™æ¡è®°å¿†çš„å†…å®¹ï¼Œæ–¹ä¾¿åé¢å†™å…¥ jsonl
                id_to_content[mid] = item.get("contents", "")
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è¯»å–è®°å¿†åº“æ–‡ä»¶ {corpus_file}ï¼Œå°†ä»…ç»Ÿè®¡è¢«æ£€ç´¢åˆ°çš„è®°å¿†ã€‚é”™è¯¯: {e}")
    
    # åˆå§‹åŒ–è®¡æ•°å™¨ï¼Œæ‰€æœ‰å·²çŸ¥ ID é»˜è®¤ä¸º 0
    memory_counter = collections.Counter({mid: 0 for mid in all_memory_ids})
    
    # -------------------------------------------------------
    # 2. ç»Ÿè®¡å®é™…æ£€ç´¢å‘½ä¸­
    # -------------------------------------------------------
    for item in rag_results:
        retrieved_docs = getattr(item, 'retrieval_result', [])
        
        for doc in retrieved_docs:
            if isinstance(doc, dict):
                doc_id = str(doc.get('id'))
            else:
                doc_id = str(getattr(doc, 'id', None))
                
            if doc_id:
                memory_counter[doc_id] += 1

    # -------------------------------------------------------
    # 3. æ’åº (æŒ‰é¢‘æ¬¡é™åº -> ID å‡åº)
    # -------------------------------------------------------
    sorted_memories = sorted(memory_counter.items(), key=lambda x: (-x[1], x[0]))
    
    total_memories = len(sorted_memories)
    used_memories = sum(1 for _, v in sorted_memories if v > 0)
    unused_memories = total_memories - used_memories
    
    print(f"ğŸ“Š è®°å¿†åº“æ€»é‡: {total_memories}")
    print(f"ğŸ”¥ è¢«æ¿€æ´»çš„è®°å¿†: {used_memories} ({(used_memories/total_memories)*100:.2f}%)")
    print(f"ğŸ§Š æ²‰ç¡çš„è®°å¿†(0æ¬¡): {unused_memories}")

    # -------------------------------------------------------
    # 4. âœ… æ–°å¢ï¼šå¯¼å‡ºæŒ‰é¢‘æ¬¡æ’åºçš„ jsonl
    # -------------------------------------------------------
    try:
        print(f"ğŸ’¾ [Save] æ­£åœ¨å¯¼å‡ºè®°å¿†è°ƒç”¨é¢‘æ¬¡æ’åºç»“æœåˆ°: {MEM_FREQ_JSONL_FILE}")
        with open(MEM_FREQ_JSONL_FILE, "w", encoding="utf-8") as f:
            for rank, (mid, freq) in enumerate(sorted_memories, start=1):
                record = {
                    "rank": rank,
                    "memory_id": mid,
                    "freq": int(freq),
                    "contents": id_to_content.get(mid, "")
                }
                f.write(json.dumps(record, ensure_ascii=False) + "\n")
        print("âœ… è°ƒç”¨é¢‘æ¬¡ jsonl å¯¼å‡ºå®Œæˆï¼")
    except Exception as e:
        print(f"âŒ å¯¼å‡º {MEM_FREQ_JSONL_FILE} å¤±è´¥: {e}")

    # -------------------------------------------------------
    # 5. å¯è§†åŒ–é€»è¾‘ (Top 30 ... Bottom 30)
    # -------------------------------------------------------
    if VISUALIZE_MEMORY_DISTRIBUTION:
        print(f"ğŸ¨ [Visual] æ­£åœ¨ç”Ÿæˆé¢‘æ¬¡åˆ†å¸ƒå›¾: {VIS_IMAGE_FILE}")
        try:
            ids = [m[0] for m in sorted_memories]
            counts = [m[1] for m in sorted_memories]
            
            display_limit = 30
            
            if len(ids) > display_limit * 2:
                print(f"â„¹ï¸ å±•ç¤ºç­–ç•¥: Top {display_limit} + å ä½ç¬¦ + Bottom {display_limit}")
                
                head_ids = ids[:display_limit]
                head_counts = counts[:display_limit]
                
                tail_ids = ids[-display_limit:]
                tail_counts = counts[-display_limit:]
                
                plot_ids = head_ids + ["..."] + tail_ids
                plot_counts = head_counts + [0] + tail_counts
                
                colors = ['skyblue'] * len(head_ids) + ['white'] + ['salmon'] * len(tail_ids)
                edge_colors = ['navy'] * len(head_ids) + ['white'] + ['darkred'] * len(tail_ids)
            else:
                plot_ids = ids
                plot_counts = counts
                colors = 'skyblue'
                edge_colors = 'navy'

            plt.figure(figsize=(15, 6))
            bars = plt.bar(plot_ids, plot_counts, color=colors, edgecolor=edge_colors)
            
            plt.title(f'Memory Usage Distribution (Top {display_limit} vs Bottom {display_limit})', fontsize=14)
            plt.xlabel('Memory ID', fontsize=12)
            plt.ylabel('Frequency', fontsize=12)
            
            plt.xticks(rotation=90, fontsize=8) 
            plt.grid(axis='y', linestyle='--', alpha=0.5)
            
            for i, bar in enumerate(bars):
                height = bar.get_height()
                if plot_ids[i] != "...": 
                    plt.text(
                        bar.get_x() + bar.get_width()/2., height,
                        f'{int(height)}',
                        ha='center', va='bottom', fontsize=8
                    )
            
            plt.tight_layout()
            plt.savefig(VIS_IMAGE_FILE, dpi=300)
            print("âœ… å›¾ç‰‡ä¿å­˜æˆåŠŸï¼")
            
        except ImportError:
            print("âŒ ç¼ºå°‘ matplotlib")
            
    else:
        print("\nğŸ† [Top 10 Hot Memories]")
        for mid, count in sorted_memories[:10]:
            print(f"   ID: {mid:<5} | Count: {count}")
            
        print("\nğŸ§Š [Bottom 10 Cold Memories]")
        for mid, count in sorted_memories[-10:]:
             print(f"   ID: {mid:<5} | Count: {count}")

# ==========================================
# 5. ä¸»ç¨‹åº
# ==========================================
def main():

    print("Visible GPU count:", torch.cuda.device_count())
    for i in range(torch.cuda.device_count()):
        print(f"  GPU {i}:", torch.cuda.get_device_name(i))


    if os.path.exists(RESULT_LOG_FILE): os.remove(RESULT_LOG_FILE)
    print(f"ğŸ“ ç»“æœå°†ä¿å­˜è‡³: {RESULT_LOG_FILE}")
    print(f"ğŸ› ï¸ æ¨¡å¼: {EXPERIMENT_MODE} | æº: {MODEL_SOURCE} | æ•°æ®é›†: {DATASET_NAME}")

    if not prepare_data(): return
    if EXPERIMENT_MODE in ['rag', 'all']: build_index()
    
    generator = None
    config = None
    
    if MODEL_SOURCE == "gemini":
        print(f"ğŸ¤– [Init] åˆå§‹åŒ– Gemini: {GEMINI_MODEL_NAME}...")
        gemini_config_dict = {
            "device": "cpu",
            "retrieval_method": "bm25",
            "corpus_path": corpus_file,
            "index_path": index_dir,
            "retriever_model_path": index_dir,
            "generator_model": "huggingface", 
            "generator_model_path": "gpt2",   
            "generation_method": "custom",  
            "save_dir": "rag_result_cache"
        }
        config = Config(config_dict=gemini_config_dict)
        generator = GeminiGenerator(GEMINI_MODEL_NAME, GEMINI_API_KEY)
        
    elif MODEL_SOURCE == "huggingface":
        print(f"ğŸ“¥ [Init] æ£€æŸ¥/ä¸‹è½½ HF æ¨¡å‹: {HF_MODEL_NAME}...")
        try:
            model_path = snapshot_download(repo_id=HF_MODEL_NAME)
        except:
            print("âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ– HF_ENDPOINT è®¾ç½®")
            return

        hf_config_dict = {
            "device": "cuda" if torch.cuda.is_available() else "cpu",
            "gpu_num": torch.cuda.device_count(),
            "generator_model": "huggingface",
            "generator_model_path": model_path,
            "generation_method": "huggingface",
            "batch_size":10, # ä¿å®ˆä¸€ç‚¹ï¼Œé˜²æ­¢æ˜¾å­˜çˆ†ç‚¸æˆ– padding é—®é¢˜
            "max_input_len": 4096, 
            "max_new_tokens": 1024,
            "save_dir": "rag_result_cache"
        }
        print("ğŸš€ åŠ è½½ HF ç”Ÿæˆå™¨...")
        config = Config(config_dict=hf_config_dict)
        generator = get_generator(config)
        
        # ğŸ”¥ğŸ”¥ğŸ”¥ Critical Fix: Tokenizer & Padding ğŸ”¥ğŸ”¥ğŸ”¥
        # å¾ˆå¤š Instruct æ¨¡å‹åœ¨æ‰¹é‡ç”Ÿæˆæ—¶ï¼Œå¦‚æœ padding è®¾ç½®ä¸å¯¹ï¼Œä¼šç›´æ¥è¾“å‡ºä¹±ç æˆ–åœæ­¢ç¬¦
        if hasattr(generator, 'tokenizer'):
            # ç¡®ä¿ä½¿ç”¨ Left Padding (ç”Ÿæˆä»»åŠ¡å¿…é¡»)
            generator.tokenizer.padding_side = 'left' 
            # ç¡®ä¿ pad_token å­˜åœ¨
            if generator.tokenizer.pad_token is None:
                generator.tokenizer.pad_token = generator.tokenizer.eos_token
                generator.tokenizer.pad_token_id = generator.tokenizer.eos_token_id
            
            # å¼ºåˆ¶æ›´æ–° max length
            generator.tokenizer.model_max_length = 4096
        
        if hasattr(generator, 'model'):
            # ç¡®ä¿æ¨¡å‹ config ä¹Ÿæœ‰ pad token id
            if hasattr(generator.model.config, 'pad_token_id') and generator.model.config.pad_token_id is None:
                generator.model.config.pad_token_id = generator.tokenizer.pad_token_id

        generator.max_input_len = 4096
        print(f"âœ… Tokenizer ä¿®æ­£: padding_side='left', pad_token={generator.tokenizer.pad_token}")
    
    else:
        print(f"âŒ æœªçŸ¥çš„ MODEL_SOURCE: {MODEL_SOURCE}")
        return

    with open(test_file, "r") as f:
        test_dataset_raw = [json.loads(line) for line in f]

    acc_baseline = 0
    acc_rag = 0

    # ==========================================
    # ğŸ”¥ Prompt æ ¼å¼åŒ– (ä¿®å¤: ä½¿ç”¨çº¯æ–‡æœ¬æŒ‡ä»¤æ ¼å¼ï¼Œæ”¾å¼ƒå®¹æ˜“å‡ºé”™çš„ ChatML)
    # ==========================================
    def format_base_prompt(system_text, user_text):
        """
        ä½¿ç”¨æœ€ç¨³å¥çš„ Alpaca é£æ ¼æˆ– Question/Answer é£æ ¼ã€‚
        å¯¹äº Qwen-Instructï¼Œ### Question: ... è¿™ç§æ ¼å¼é€šå¸¸æ¯”é”™è¯¯çš„ ChatML æ ‡ç­¾æ›´å¥½ç”¨ã€‚
        """
        if MODEL_SOURCE == "gemini":
            return f"{system_text}\n\n{user_text}" if system_text else user_text
            
        # HuggingFace æ¨¡å‹é€šç”¨æ ¼å¼
        prompt = ""
        if system_text:
            prompt += f"{system_text}\n\n"
        
        # æ„é€ æ¸…æ™°çš„ Q&A ç»“æ„
        # é‡ç‚¹ï¼šæœ«å°¾åŠ ä¸Š "Let's think step by step." ä½œä¸º Priming (å¯åŠ¨å­)
        prompt += f"### Question:\n{user_text}\n\n### Answer:\nLet's think step by step."
        return prompt

    if EXPERIMENT_MODE in ['baseline', 'all']:
        print("\nâš”ï¸ [Task A] æ­£åœ¨è¿è¡Œ Baseline ...")
        
        baseline_inputs = []
        for item in test_dataset_raw:
            # åœ¨ User Text é‡Œä¸è¦åŠ  instructionï¼Œæ”¾åˆ° format å‡½æ•°é‡Œç»Ÿä¸€åŠ ï¼Œé¿å…æ··ä¹±
            user_content = item['question']
            
            # æ„é€ æç¤ºè¯
            sys_msg = "You are a math expert. Solve the problem in a brief. Don't answer more than 50 words.End your answer with \boxed\{number\}."
            formatted_prompt = format_base_prompt(sys_msg, user_content)
            baseline_inputs.append(formatted_prompt)

        baseline_preds = generator.generate(baseline_inputs)
        
        baseline_results = []
        for item, pred in zip(test_dataset_raw, baseline_preds):
            baseline_results.append({
                "question": item['question'],
                "golden_answers": item['golden_answers'],
                "pred": pred
            })
        acc_baseline = evaluate_results(baseline_results, "Baseline (No RAG)")

    if EXPERIMENT_MODE in ['rag', 'all']:
        print("\nâš”ï¸ [Task B] æ­£åœ¨è¿è¡Œ FlashRAG (Few-shot Retrieval)...")
        
        rag_config_dict = config.config_dict.copy() if hasattr(config, 'config_dict') else {}
        if not rag_config_dict:
             rag_config_dict = gemini_config_dict if MODEL_SOURCE == "gemini" else hf_config_dict
             
        rag_config_dict.update({
            "retrieval_method": "bm25",
            "corpus_path": corpus_file,
            "index_path": index_dir,
            "retriever_model_path": index_dir,
            "topk": 3 
        })
        
        rag_config = Config(config_dict=rag_config_dict)
        retriever = get_retriever(rag_config)
        
        # --- FlashRAG Prompt Template (ä¿®å¤ç‰ˆ) ---
        # 1. æ”¾å¼ƒå¤æ‚çš„ ChatML
        # 2. ç»Ÿä¸€ä½¿ç”¨ ### æ ‡è®°
        # 3. å¼ºåˆ¶ Priming ("Let's think step by step.")
        
        rag_system_part = (
            "You are a math expert. You can solve math problems in one second to give the correct answer. Below are some similar solved problems. "
            "Refer to the logic in these examples to solve the new question.\n\n"
            "Solve the problem in a very brief. Don't answer more than 80 tokens. If the problem is easy, You can also just give the final answer."
            "Do not perform unit conversion."
            "{reference}\n\n"
            "### Question:\n{question}\n\n"
            "### Answer:\n"
            "Let's think step by step." 
        )
        
        # System Prompt åŒ…å«äº†æ‰€æœ‰ç»“æ„ï¼ŒUser Prompt ç•™ç©ºï¼ˆæˆ–è€…ç”± FlashRAG å†…éƒ¨å¤„ç†ï¼‰
        # FlashRAG çš„ Dataset é»˜è®¤æŠŠ query å¡«å…¥ {question}ï¼Œretrieval å¡«å…¥ {reference}
        prompt_tpl = PromptTemplate(rag_config, system_prompt=rag_system_part, user_prompt="")
        
        pipeline = SequentialPipeline(rag_config, prompt_template=prompt_tpl, retriever=retriever, generator=generator)
        
        dataset_obj = Dataset(rag_config, test_file)
        
        # è¿è¡Œ Pipeline
        rag_results = pipeline.run(dataset_obj)
        
        # 1. è¯„ä¼°å‡†ç¡®ç‡
        acc_rag = evaluate_results(rag_results, f"FlashRAG ({dataset_tag} Memory)")
        
        # 2. ğŸ”¥ ç»Ÿè®¡è®°å¿†çƒ­åº¦
        analyze_memory_usage(rag_results)

    if EXPERIMENT_MODE == 'all':
        summary = (
            f"\n{'='*20} æœ€ç»ˆå¯¹æ¯”ç»“æœ {'='*20}\n"
            f"ğŸ“Š æ•°æ®é›†: {DATASET_NAME}\n"
            f"ğŸ¤– æ¨¡å‹: {MODEL_SOURCE} / {GEMINI_MODEL_NAME if MODEL_SOURCE=='gemini' else HF_MODEL_NAME}\n"
            f"ğŸ“‰ Baseline: {acc_baseline:.2f}%\n"
            f"ğŸ“ˆ FlashRAG: {acc_rag:.2f}%\n"
            f"ğŸš€ æå‡: {acc_rag - acc_baseline:+.2f}%\n"
            f"{'='*50}\n"
        )
        print(summary)
        with open(RESULT_LOG_FILE, "a", encoding="utf-8") as f:
            f.write(summary)

if __name__ == "__main__":
    main()